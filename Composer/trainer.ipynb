{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NoteComposeNet\n",
    "from dataset import MidiDataset, VOCABULARY\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {\n",
    "    'notes': [torch.tensor([1 for i in range(0, 255)])],\n",
    "    'velocities': [torch.tensor([1 for i in range(0, 255)])],\n",
    "    'durations': [torch.tensor([1 for i in range(0, 255)])],\n",
    "    'times': [torch.tensor([1 for i in range(0, 255)])],     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NoteComposeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 0.079MB\n"
     ]
    }
   ],
   "source": [
    "# Model Specifications\n",
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m midi \u001b[39m=\u001b[39m MidiDataset(df, context_len \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_context_len)\n\u001b[0;32m      2\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(midi, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_loader))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "midi = MidiDataset(df, context_len = model._context_len)\n",
    "train_loader = DataLoader(midi, batch_size=1)\n",
    "\n",
    "data = next(iter(train_loader))\n",
    "note_tensor = data['notes'].to(\"cuda\")\n",
    "velocity_tensor = data['velocities'].to(\"cuda\")\n",
    "duration_tensor = data['durations'].to(\"cuda\")\n",
    "time_tensor = data['times'].to(\"cuda\")\n",
    "\n",
    "input = {\n",
    "    \"notes\": note_tensor,\n",
    "    \"velocities\": velocity_tensor,\n",
    "    \"durations\": duration_tensor ,\n",
    "    \"times\": time_tensor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joaquin\\Desktop\\Playground\\ML\\Composer\\model.py:114: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  def detokenize(self, inputs):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note =  ['A♯-1', 'D♯8', '<EOS>', 'D♯5', 'A1', 'E9', 'A♯0', 'D♯8', 'B2', 'E1', 'A6', 'C♯5', 'G♯3', 'F9', 'C5', 'A6', 'C9', 'B3', 'E1', 'C1', 'C9', 'F5', 'E1', 'A6', 'C9', 'D♯5', 'A1', 'E1', 'D♯5', 'D♯8', 'D♯8', 'B7', 'C♯0', 'F♯9', 'B7', 'F♯1', 'F2', 'D♯5', 'C♯2', 'D7', 'G2', 'A♯0', 'C♯5', 'G5', 'F0', 'D1', 'G♯0', 'E1', 'G♯3', 'E1', 'A1', 'C7', 'F0', 'G0', 'C5', 'F5', 'B3', 'F5', 'E1', 'G2', 'G3', 'A6', 'F2', 'F0', 'B0', 'A6', 'C♯5', 'F♯6', 'C♯2', 'E1', 'A6', 'A1', 'A6', 'A1', 'A6', 'F5', 'G0', 'C5', 'G0', 'A6', 'F5', 'B3', 'D♯-1', 'A6', 'C♯5', 'B3', 'E3', 'A6', 'G0', 'A♯-1', 'F0', 'E0', 'B2', 'E1', 'D2', 'C♯5', 'C♯-1', 'F5', 'E1', 'A6']\n"
     ]
    }
   ],
   "source": [
    "test_inputs = np.random.randint(0, len(VOCABULARY), size=255, dtype=int)\n",
    "next_notes = model.generate(test_inputs, max_len=100)\n",
    "print(\"Note = \", model.detokenize(next_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NoteComposeNet\n",
    "from dataset import MidiDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SAMPLES_PER_TRACK = 128\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NoteComposeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = r'midi-dataset-mini.csv'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "midi = MidiDataset(df, context_len = model._context_len, samples_per_track=SAMPLES_PER_TRACK)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.001, \n",
    "    weight_decay=0.004,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-07, \n",
    "    amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    midi, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=1,\n",
    "    shuffle=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch(batch):\n",
    "    b, attn, gt = batch\n",
    "    \n",
    "    notes = b['notes'].to(model._device)\n",
    "    notes_gt = gt['notes'].to(model._device)\n",
    "\n",
    "    output_logits = model.forward(notes)\n",
    "\n",
    "    return output_logits, notes_gt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output_logits, notes_gt = unpack_batch(batch)\n",
    "        \n",
    "        loss = loss_fn(output_logits, notes_gt)\n",
    "        loss.backward() \n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Gather data and report\n",
    "        total_loss += loss.item()\n",
    "        last_loss = loss.item()\n",
    "        print('  batch {} loss: {}'.format(i + 1, loss.item()))\n",
    "        tb_x = epoch_index * len(train_loader) + i + 1\n",
    "        tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 4.887590438127518\n",
      "  batch 2 loss: 4.887490317225456\n",
      "  batch 3 loss: 4.887367427349091\n",
      "  batch 4 loss: 4.8870381116867065\n",
      "  batch 5 loss: 4.886938378214836\n",
      "  batch 6 loss: 4.887105941772461\n",
      "  batch 7 loss: 4.8870065957307816\n",
      "  batch 8 loss: 4.886593982577324\n",
      "  batch 9 loss: 4.886748388409615\n",
      "  batch 10 loss: 4.88700906932354\n",
      "  batch 11 loss: 4.886569797992706\n",
      "  batch 12 loss: 4.886604607105255\n",
      "  batch 13 loss: 4.886535182595253\n",
      "  batch 14 loss: 4.886377707123756\n",
      "  batch 15 loss: 4.886149913072586\n",
      "  batch 16 loss: 4.886054769158363\n",
      "  batch 17 loss: 4.885629251599312\n",
      "  batch 18 loss: 4.885914504528046\n",
      "  batch 19 loss: 4.88531257212162\n",
      "  batch 20 loss: 4.885616093873978\n",
      "  batch 21 loss: 4.885588496923447\n",
      "  batch 22 loss: 4.885253548622131\n",
      "  batch 23 loss: 4.885059744119644\n",
      "  batch 24 loss: 4.884831979870796\n",
      "  batch 25 loss: 4.884999170899391\n",
      "  batch 26 loss: 4.884742856025696\n",
      "  batch 27 loss: 4.884558469057083\n",
      "  batch 28 loss: 4.884173110127449\n",
      "  batch 29 loss: 4.884393721818924\n",
      "  batch 30 loss: 4.883594125509262\n",
      "  batch 31 loss: 4.883272722363472\n",
      "  batch 32 loss: 4.883351668715477\n",
      "  batch 33 loss: 4.883107498288155\n",
      "  batch 34 loss: 4.88302181661129\n",
      "  batch 35 loss: 4.882701724767685\n",
      "  batch 36 loss: 4.8823578506708145\n",
      "  batch 37 loss: 4.882535815238953\n",
      "  batch 38 loss: 4.881746664643288\n",
      "  batch 39 loss: 4.882134988903999\n",
      "  batch 40 loss: 4.881359398365021\n",
      "  batch 41 loss: 4.881020352244377\n",
      "  batch 42 loss: 4.880553558468819\n",
      "  batch 43 loss: 4.880068928003311\n",
      "  batch 44 loss: 4.879367604851723\n",
      "  batch 45 loss: 4.880302459001541\n",
      "  batch 46 loss: 4.878912955522537\n",
      "  batch 47 loss: 4.877876907587051\n",
      "  batch 48 loss: 4.8788764625787735\n",
      "  batch 49 loss: 4.877031162381172\n",
      "  batch 50 loss: 4.8781188279390335\n",
      "  batch 51 loss: 4.8765598982572556\n",
      "  batch 52 loss: 4.875231295824051\n",
      "  batch 53 loss: 4.875783249735832\n",
      "  batch 54 loss: 4.874249458312988\n",
      "  batch 55 loss: 4.873860254883766\n",
      "  batch 56 loss: 4.874309942126274\n",
      "  batch 57 loss: 4.872596696019173\n",
      "  batch 58 loss: 4.870228976011276\n",
      "  batch 59 loss: 4.871724098920822\n",
      "  batch 60 loss: 4.870512917637825\n",
      "  batch 61 loss: 4.86980040371418\n",
      "  batch 62 loss: 4.868044272065163\n",
      "  batch 63 loss: 4.865818828344345\n",
      "  batch 64 loss: 4.866270527243614\n",
      "  batch 65 loss: 4.865754500031471\n",
      "  batch 66 loss: 4.864796951413155\n",
      "  batch 67 loss: 4.863028720021248\n",
      "  batch 68 loss: 4.863446742296219\n",
      "  batch 69 loss: 4.859746307134628\n",
      "  batch 70 loss: 4.860568881034851\n",
      "  batch 71 loss: 4.857720166444778\n",
      "  batch 72 loss: 4.854245200753212\n",
      "  batch 73 loss: 4.853142961859703\n",
      "  batch 74 loss: 4.853881508111954\n",
      "  batch 75 loss: 4.849636048078537\n",
      "  batch 76 loss: 4.844454199075699\n",
      "  batch 77 loss: 4.845690071582794\n",
      "  batch 78 loss: 4.845642626285553\n",
      "  batch 79 loss: 4.83940526843071\n",
      "  batch 80 loss: 4.8324242532253265\n",
      "  batch 81 loss: 4.833092376589775\n",
      "  batch 82 loss: 4.837087705731392\n",
      "  batch 83 loss: 4.83520245552063\n",
      "  batch 84 loss: 4.824228882789612\n",
      "  batch 85 loss: 4.822565913200378\n",
      "  batch 86 loss: 4.815817251801491\n",
      "  batch 87 loss: 4.815951436758041\n",
      "  batch 88 loss: 4.801369607448578\n",
      "  batch 89 loss: 4.809443429112434\n",
      "  batch 90 loss: 4.801129132509232\n",
      "  batch 91 loss: 4.800949588418007\n",
      "  batch 92 loss: 4.791674554347992\n",
      "  batch 93 loss: 4.788336515426636\n",
      "  batch 94 loss: 4.784373700618744\n",
      "  batch 95 loss: 4.779638394713402\n",
      "  batch 96 loss: 4.7803739458322525\n",
      "  batch 97 loss: 4.760874032974243\n",
      "  batch 98 loss: 4.76983542740345\n",
      "  batch 99 loss: 4.751947581768036\n",
      "  batch 100 loss: 4.738573402166367\n",
      "  batch 101 loss: 4.740407288074493\n",
      "  batch 102 loss: 4.745541617274284\n",
      "  batch 103 loss: 4.723474085330963\n",
      "  batch 104 loss: 4.710063710808754\n",
      "  batch 105 loss: 4.707056939601898\n",
      "  batch 106 loss: 4.7255569994449615\n",
      "  batch 107 loss: 4.698527231812477\n",
      "  batch 108 loss: 4.656069040298462\n",
      "  batch 109 loss: 4.653210669755936\n",
      "  batch 110 loss: 4.637040317058563\n",
      "  batch 111 loss: 4.644258975982666\n",
      "  batch 112 loss: 4.618750646710396\n",
      "  batch 113 loss: 4.595569759607315\n",
      "  batch 114 loss: 4.5757226049900055\n",
      "  batch 115 loss: 4.563902333378792\n",
      "  batch 116 loss: 4.574235066771507\n",
      "  batch 117 loss: 4.553406536579132\n",
      "  batch 118 loss: 4.542192533612251\n",
      "  batch 119 loss: 4.495091900229454\n",
      "  batch 120 loss: 4.493051111698151\n",
      "  batch 121 loss: 4.477246418595314\n",
      "  batch 122 loss: 4.482146501541138\n",
      "  batch 123 loss: 4.43942566215992\n",
      "  batch 124 loss: 4.458099231123924\n",
      "  batch 125 loss: 4.459104731678963\n",
      "  batch 126 loss: 4.435680031776428\n",
      "  batch 127 loss: 4.405526250600815\n",
      "  batch 128 loss: 4.376648768782616\n",
      "  batch 129 loss: 4.349842622876167\n",
      "  batch 130 loss: 4.295334979891777\n",
      "  batch 131 loss: 4.292480185627937\n",
      "  batch 132 loss: 4.353798970580101\n",
      "  batch 133 loss: 4.316614925861359\n",
      "  batch 134 loss: 4.293029919266701\n",
      "  batch 135 loss: 4.300180181860924\n",
      "  batch 136 loss: 4.244477704167366\n",
      "  batch 137 loss: 4.251299247145653\n",
      "  batch 138 loss: 4.2505409717559814\n",
      "  batch 139 loss: 4.237795770168304\n",
      "  batch 140 loss: 4.197350099682808\n",
      "  batch 141 loss: 4.230075985193253\n",
      "  batch 142 loss: 4.207425028085709\n",
      "  batch 143 loss: 4.170556217432022\n",
      "  batch 144 loss: 4.1907939016819\n",
      "  batch 145 loss: 4.163109391927719\n",
      "  batch 146 loss: 4.169495344161987\n",
      "  batch 147 loss: 4.136802777647972\n",
      "  batch 148 loss: 4.137130945920944\n",
      "  batch 149 loss: 4.126928925514221\n",
      "  batch 150 loss: 4.1358053386211395\n",
      "  batch 151 loss: 4.107169196009636\n",
      "  batch 152 loss: 4.137793779373169\n",
      "  batch 153 loss: 4.1013020277023315\n",
      "  batch 154 loss: 4.073216736316681\n",
      "  batch 155 loss: 4.076183915138245\n",
      "  batch 156 loss: 4.09752244502306\n",
      "  batch 157 loss: 4.0839696899056435\n",
      "  batch 158 loss: 4.074147418141365\n",
      "  batch 159 loss: 4.058983340859413\n",
      "  batch 160 loss: 4.067613892257214\n",
      "  batch 161 loss: 4.041255094110966\n",
      "  batch 162 loss: 4.064448691904545\n",
      "  batch 163 loss: 4.0525955483317375\n",
      "  batch 164 loss: 4.057414308190346\n",
      "  batch 165 loss: 4.02470950782299\n",
      "  batch 166 loss: 4.032746896147728\n",
      "  batch 167 loss: 4.023573458194733\n",
      "  batch 168 loss: 4.010107539594173\n",
      "  batch 169 loss: 4.026129528880119\n",
      "  batch 170 loss: 4.007532179355621\n",
      "  batch 171 loss: 4.003685779869556\n",
      "  batch 172 loss: 4.000194288790226\n",
      "  batch 173 loss: 4.000785760581493\n",
      "  batch 174 loss: 4.003076009452343\n",
      "  batch 175 loss: 3.995797485113144\n",
      "  batch 176 loss: 4.020920716226101\n",
      "  batch 177 loss: 3.985922686755657\n",
      "  batch 178 loss: 4.000081866979599\n",
      "  batch 179 loss: 3.9850165024399757\n",
      "  batch 180 loss: 3.986782990396023\n",
      "  batch 181 loss: 3.9740805998444557\n",
      "  batch 182 loss: 3.9904848858714104\n",
      "  batch 183 loss: 3.966330900788307\n",
      "  batch 184 loss: 3.981812961399555\n",
      "  batch 185 loss: 3.974462352693081\n",
      "  batch 186 loss: 3.9872932508587837\n",
      "  batch 187 loss: 3.9563162699341774\n",
      "  batch 188 loss: 3.96030193567276\n",
      "  batch 189 loss: 3.9728089347481728\n",
      "  batch 190 loss: 3.985251732170582\n",
      "  batch 191 loss: 3.9741049706935883\n",
      "  batch 192 loss: 3.971150115132332\n",
      "  batch 193 loss: 3.961322598159313\n",
      "  batch 194 loss: 3.9585151001811028\n",
      "  batch 195 loss: 3.964472994208336\n",
      "  batch 196 loss: 3.959089957177639\n",
      "  batch 197 loss: 3.957821860909462\n",
      "  batch 198 loss: 3.9584661722183228\n",
      "  batch 199 loss: 3.964377850294113\n",
      "  batch 200 loss: 3.9573497623205185\n",
      "  batch 201 loss: 3.955943390727043\n",
      "  batch 202 loss: 3.9699530974030495\n",
      "  batch 203 loss: 3.962903194129467\n",
      "  batch 204 loss: 3.9580606147646904\n",
      "  batch 205 loss: 3.9505182951688766\n",
      "  batch 206 loss: 3.9417758509516716\n",
      "  batch 207 loss: 3.9662854373455048\n",
      "  batch 208 loss: 3.9518565759062767\n",
      "  batch 209 loss: 3.93769683688879\n",
      "  batch 210 loss: 3.955529786646366\n",
      "  batch 211 loss: 3.9491559341549873\n",
      "  batch 212 loss: 3.962911695241928\n",
      "  batch 213 loss: 3.9384199753403664\n",
      "  batch 214 loss: 3.948902666568756\n",
      "  batch 215 loss: 3.9596662372350693\n",
      "  batch 216 loss: 3.943312332034111\n",
      "  batch 217 loss: 3.944766543805599\n",
      "  batch 218 loss: 3.947884738445282\n",
      "  batch 219 loss: 3.9421438723802567\n",
      "  batch 220 loss: 3.948357529938221\n",
      "  batch 221 loss: 3.936300553381443\n",
      "  batch 222 loss: 3.963048316538334\n",
      "  batch 223 loss: 3.9377720654010773\n",
      "  batch 224 loss: 3.942666083574295\n",
      "  batch 225 loss: 3.938035912811756\n",
      "  batch 226 loss: 3.939341574907303\n",
      "  batch 227 loss: 3.9320139810442924\n",
      "  batch 228 loss: 3.935163512825966\n",
      "  batch 229 loss: 3.936224728822708\n",
      "  batch 230 loss: 3.934067152440548\n",
      "  batch 231 loss: 3.9332405179739\n",
      "  batch 232 loss: 3.9320638328790665\n",
      "  batch 233 loss: 3.9292068481445312\n",
      "  batch 234 loss: 3.938002236187458\n",
      "  batch 235 loss: 3.927060693502426\n",
      "  batch 236 loss: 3.9253581389784813\n",
      "  batch 237 loss: 3.929943822324276\n",
      "  batch 238 loss: 3.9277730882167816\n",
      "  batch 239 loss: 3.9332531541585922\n",
      "  batch 240 loss: 3.931494988501072\n",
      "  batch 241 loss: 3.933472789824009\n",
      "  batch 242 loss: 3.9297069758176804\n",
      "  batch 243 loss: 3.939362920820713\n",
      "  batch 244 loss: 3.934978373348713\n",
      "  batch 245 loss: 3.9307031109929085\n",
      "  batch 246 loss: 3.933964490890503\n",
      "  batch 247 loss: 3.938405156135559\n",
      "  batch 248 loss: 3.9316938295960426\n",
      "  batch 249 loss: 3.9345750734210014\n",
      "  batch 250 loss: 3.9419777393341064\n",
      "  batch 251 loss: 3.92730400711298\n",
      "  batch 252 loss: 3.927879437804222\n",
      "  batch 253 loss: 3.927506260573864\n",
      "  batch 254 loss: 3.9305289909243584\n",
      "  batch 255 loss: 3.9270108118653297\n",
      "  batch 256 loss: 3.9233040139079094\n",
      "  batch 257 loss: 3.925111308693886\n",
      "  batch 258 loss: 3.9252934008836746\n",
      "  batch 259 loss: 3.927766554057598\n",
      "  batch 260 loss: 3.9226469472050667\n",
      "  batch 261 loss: 3.9246156811714172\n",
      "  batch 262 loss: 3.9232294484972954\n",
      "  batch 263 loss: 3.930724285542965\n",
      "  batch 264 loss: 3.9270109236240387\n",
      "  batch 265 loss: 3.9349787309765816\n",
      "  batch 266 loss: 3.9230432137846947\n",
      "  batch 267 loss: 3.9223243966698647\n",
      "  batch 268 loss: 3.9189993664622307\n",
      "  batch 269 loss: 3.924678511917591\n",
      "  batch 270 loss: 3.9273362159729004\n",
      "  batch 271 loss: 3.919755019247532\n",
      "  batch 272 loss: 3.916781522333622\n",
      "  batch 273 loss: 3.937234438955784\n",
      "  batch 274 loss: 3.9154646918177605\n",
      "  batch 275 loss: 3.920542858541012\n",
      "  batch 276 loss: 3.929810866713524\n",
      "  batch 277 loss: 3.919281803071499\n",
      "  batch 278 loss: 3.9229871705174446\n",
      "  batch 279 loss: 3.91992723941803\n",
      "  batch 280 loss: 3.9225369840860367\n",
      "  batch 281 loss: 3.922193303704262\n",
      "  batch 282 loss: 3.924010917544365\n",
      "  batch 283 loss: 3.9197316765785217\n",
      "  batch 284 loss: 3.9203720316290855\n",
      "  batch 285 loss: 3.9245351627469063\n",
      "  batch 286 loss: 3.91974551230669\n",
      "  batch 287 loss: 3.918201059103012\n",
      "  batch 288 loss: 3.919109880924225\n",
      "  batch 289 loss: 3.9186277762055397\n",
      "  batch 290 loss: 3.921762101352215\n",
      "  batch 291 loss: 3.9180894568562508\n",
      "  batch 292 loss: 3.919298119843006\n",
      "  batch 293 loss: 3.9192109927535057\n",
      "  batch 294 loss: 3.9177898913621902\n",
      "  batch 295 loss: 3.9160542339086533\n",
      "  batch 296 loss: 3.919089935719967\n",
      "  batch 297 loss: 3.9151499941945076\n",
      "  batch 298 loss: 3.9148992970585823\n",
      "  batch 299 loss: 3.9165979102253914\n",
      "  batch 300 loss: 3.917602777481079\n",
      "  batch 301 loss: 3.916215516626835\n",
      "  batch 302 loss: 3.913453124463558\n",
      "  batch 303 loss: 3.9156145453453064\n",
      "  batch 304 loss: 3.9183106422424316\n",
      "  batch 305 loss: 3.9184324219822884\n",
      "  batch 306 loss: 3.917766183614731\n",
      "  batch 307 loss: 3.9160589277744293\n",
      "  batch 308 loss: 3.9147912189364433\n",
      "  batch 309 loss: 3.915126122534275\n",
      "  batch 310 loss: 3.9156765043735504\n",
      "  batch 311 loss: 3.9151383712887764\n",
      "  batch 312 loss: 3.915723964571953\n",
      "  batch 313 loss: 3.9185099825263023\n",
      "  batch 314 loss: 3.91234839707613\n",
      "  batch 315 loss: 3.9121567010879517\n",
      "  batch 316 loss: 3.9135124906897545\n",
      "  batch 317 loss: 3.9158287420868874\n",
      "  batch 318 loss: 3.919688194990158\n",
      "  batch 319 loss: 3.9131335243582726\n",
      "  batch 320 loss: 3.9119046330451965\n",
      "  batch 321 loss: 3.913703814148903\n",
      "  batch 322 loss: 3.9163336008787155\n",
      "  batch 323 loss: 3.916102983057499\n",
      "  batch 324 loss: 3.9155293107032776\n",
      "  batch 325 loss: 3.9219358414411545\n",
      "  batch 326 loss: 3.9150901660323143\n",
      "  batch 327 loss: 3.918486349284649\n",
      "  batch 328 loss: 3.9129309579730034\n",
      "  batch 329 loss: 3.909814938902855\n",
      "  batch 330 loss: 3.9134590178728104\n",
      "  batch 331 loss: 3.9110374078154564\n",
      "  batch 332 loss: 3.9133008420467377\n",
      "  batch 333 loss: 3.914074309170246\n",
      "  batch 334 loss: 3.911645568907261\n",
      "  batch 335 loss: 3.9106432646512985\n",
      "  batch 336 loss: 3.9158309251070023\n",
      "  batch 337 loss: 3.915960982441902\n",
      "  batch 338 loss: 3.9107681661844254\n",
      "  batch 339 loss: 3.90834878385067\n",
      "  batch 340 loss: 3.9122319146990776\n",
      "  batch 341 loss: 3.911771297454834\n",
      "  batch 342 loss: 3.9125839695334435\n",
      "  batch 343 loss: 3.9183337911963463\n",
      "  batch 344 loss: 3.9094011783599854\n",
      "  batch 345 loss: 3.91521929949522\n",
      "  batch 346 loss: 3.9110077619552612\n",
      "  batch 347 loss: 3.9084601774811745\n",
      "  batch 348 loss: 3.910383850336075\n",
      "  batch 349 loss: 3.9118719026446342\n",
      "  batch 350 loss: 3.9113663658499718\n",
      "  batch 351 loss: 3.9088209196925163\n",
      "  batch 352 loss: 3.9120120108127594\n",
      "  batch 353 loss: 3.908105693757534\n",
      "  batch 354 loss: 3.9101108089089394\n",
      "  batch 355 loss: 3.908143073320389\n",
      "  batch 356 loss: 3.913506269454956\n",
      "  batch 357 loss: 3.912732593715191\n",
      "  batch 358 loss: 3.9118200093507767\n",
      "  batch 359 loss: 3.908479630947113\n",
      "  batch 360 loss: 3.9078399166464806\n",
      "  batch 361 loss: 3.9084446504712105\n",
      "  batch 362 loss: 3.9119857251644135\n",
      "  batch 363 loss: 3.906345710158348\n",
      "  batch 364 loss: 3.912832871079445\n",
      "  batch 365 loss: 3.9075867608189583\n",
      "  batch 366 loss: 3.908639073371887\n",
      "  batch 367 loss: 3.9164803102612495\n",
      "  batch 368 loss: 3.910221092402935\n",
      "  batch 369 loss: 3.908859208226204\n",
      "  batch 370 loss: 3.906081035733223\n",
      "  batch 371 loss: 3.906971499323845\n",
      "  batch 372 loss: 3.9130723029375076\n",
      "  batch 373 loss: 3.9160887226462364\n",
      "  batch 374 loss: 3.913138836622238\n",
      "  batch 375 loss: 3.907352067530155\n",
      "  batch 376 loss: 3.9115730971097946\n",
      "  batch 377 loss: 3.908191218972206\n",
      "  batch 378 loss: 3.9118629470467567\n",
      "  batch 379 loss: 3.9089664816856384\n",
      "  batch 380 loss: 3.9075093865394592\n",
      "  batch 381 loss: 3.9084287881851196\n",
      "  batch 382 loss: 3.909807965159416\n",
      "  batch 383 loss: 3.908317446708679\n",
      "  batch 384 loss: 3.906943492591381\n",
      "  batch 385 loss: 3.912371553480625\n",
      "  batch 386 loss: 3.906681075692177\n",
      "  batch 387 loss: 3.9071668535470963\n",
      "  batch 388 loss: 3.9097904413938522\n",
      "  batch 389 loss: 3.9122655764222145\n",
      "  batch 390 loss: 3.9133965224027634\n",
      "  batch 391 loss: 3.911109283566475\n",
      "  batch 392 loss: 3.9080241844058037\n",
      "  batch 393 loss: 3.908203646540642\n",
      "  batch 394 loss: 3.90798332542181\n",
      "  batch 395 loss: 3.9064070731401443\n",
      "  batch 396 loss: 3.9081141650676727\n",
      "  batch 397 loss: 3.9106945618987083\n",
      "  batch 398 loss: 3.9092970564961433\n",
      "  batch 399 loss: 3.906318187713623\n",
      "  batch 400 loss: 3.9080413058400154\n",
      "  batch 401 loss: 3.9098247811198235\n",
      "  batch 402 loss: 3.906834177672863\n",
      "  batch 403 loss: 3.905813090503216\n",
      "  batch 404 loss: 3.904806099832058\n",
      "  batch 405 loss: 3.905893124639988\n",
      "  batch 406 loss: 3.906229369342327\n",
      "  batch 407 loss: 3.9075858741998672\n",
      "  batch 408 loss: 3.912100166082382\n",
      "  batch 409 loss: 3.9071985855698586\n",
      "  batch 410 loss: 3.9084321185946465\n",
      "  batch 411 loss: 3.91069258749485\n",
      "  batch 412 loss: 3.906843200325966\n",
      "  batch 413 loss: 3.905209109187126\n",
      "  batch 414 loss: 3.908946566283703\n",
      "  batch 415 loss: 3.904048830270767\n",
      "  batch 416 loss: 3.9082143381237984\n",
      "  batch 417 loss: 3.9068305641412735\n",
      "  batch 418 loss: 3.9088661670684814\n",
      "  batch 419 loss: 3.909742347896099\n",
      "  batch 420 loss: 3.908356264233589\n",
      "  batch 421 loss: 3.9046113714575768\n",
      "  batch 422 loss: 3.907176800072193\n",
      "  batch 423 loss: 3.903682269155979\n",
      "  batch 424 loss: 3.9091218188405037\n",
      "LOSS train 3.9091218188405037 valid 3.902763851654698\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.906037151813507\n",
      "  batch 2 loss: 3.907883904874325\n",
      "  batch 3 loss: 3.9046738520264626\n",
      "  batch 4 loss: 3.9082328975200653\n",
      "  batch 5 loss: 3.9074023067951202\n",
      "  batch 6 loss: 3.902953438460827\n",
      "  batch 7 loss: 3.9072080925107002\n",
      "  batch 8 loss: 3.907552421092987\n",
      "  batch 9 loss: 3.9048937484622\n",
      "  batch 10 loss: 3.9043136462569237\n",
      "  batch 11 loss: 3.905558653175831\n",
      "  batch 12 loss: 3.9058932065963745\n",
      "  batch 13 loss: 3.9067609533667564\n",
      "  batch 14 loss: 3.905255824327469\n",
      "  batch 15 loss: 3.9032616168260574\n",
      "  batch 16 loss: 3.9046067222952843\n",
      "  batch 17 loss: 3.90633687376976\n",
      "  batch 18 loss: 3.9087348207831383\n",
      "  batch 19 loss: 3.9051458835601807\n",
      "  batch 20 loss: 3.9056194573640823\n",
      "  batch 21 loss: 3.9030712097883224\n",
      "  batch 22 loss: 3.906906098127365\n",
      "  batch 23 loss: 3.9051181450486183\n",
      "  batch 24 loss: 3.908289797604084\n",
      "  batch 25 loss: 3.9064467325806618\n",
      "  batch 26 loss: 3.9049081057310104\n",
      "  batch 27 loss: 3.90707129240036\n",
      "  batch 28 loss: 3.9050589129328728\n",
      "  batch 29 loss: 3.9039095416665077\n",
      "  batch 30 loss: 3.906877636909485\n",
      "  batch 31 loss: 3.90372284501791\n",
      "  batch 32 loss: 3.904620498418808\n",
      "  batch 33 loss: 3.905285023152828\n",
      "  batch 34 loss: 3.904471978545189\n",
      "  batch 35 loss: 3.9046241864562035\n",
      "  batch 36 loss: 3.9067943543195724\n",
      "  batch 37 loss: 3.9077044278383255\n",
      "  batch 38 loss: 3.9053145423531532\n",
      "  batch 39 loss: 3.907902806997299\n",
      "  batch 40 loss: 3.9103949144482613\n",
      "  batch 41 loss: 3.904113858938217\n",
      "  batch 42 loss: 3.9048454612493515\n",
      "  batch 43 loss: 3.9033700972795486\n",
      "  batch 44 loss: 3.907391034066677\n",
      "  batch 45 loss: 3.9086278900504112\n",
      "  batch 46 loss: 3.9048459455370903\n",
      "  batch 47 loss: 3.9042048528790474\n",
      "  batch 48 loss: 3.9030300304293633\n",
      "  batch 49 loss: 3.9037560001015663\n",
      "  batch 50 loss: 3.9045505821704865\n",
      "  batch 51 loss: 3.9099887385964394\n",
      "  batch 52 loss: 3.9041326716542244\n",
      "  batch 53 loss: 3.904939077794552\n",
      "  batch 54 loss: 3.905791573226452\n",
      "  batch 55 loss: 3.906251147389412\n",
      "  batch 56 loss: 3.904301553964615\n",
      "  batch 57 loss: 3.902196265757084\n",
      "  batch 58 loss: 3.9042848497629166\n",
      "  batch 59 loss: 3.9046757891774178\n",
      "  batch 60 loss: 3.9034275710582733\n",
      "  batch 61 loss: 3.9035029485821724\n",
      "  batch 62 loss: 3.906225748360157\n",
      "  batch 63 loss: 3.9046778306365013\n",
      "  batch 64 loss: 3.904665730893612\n",
      "  batch 65 loss: 3.902662642300129\n",
      "  batch 66 loss: 3.9028948470950127\n",
      "  batch 67 loss: 3.905585527420044\n",
      "  batch 68 loss: 3.902391590178013\n",
      "  batch 69 loss: 3.9030545875430107\n",
      "  batch 70 loss: 3.9038984924554825\n",
      "  batch 71 loss: 3.903263568878174\n",
      "  batch 72 loss: 3.906415656208992\n",
      "  batch 73 loss: 3.9021697714924812\n",
      "  batch 74 loss: 3.9044812098145485\n",
      "  batch 75 loss: 3.903128206729889\n",
      "  batch 76 loss: 3.9035694673657417\n",
      "  batch 77 loss: 3.9038009718060493\n",
      "  batch 78 loss: 3.902786187827587\n",
      "  batch 79 loss: 3.904077932238579\n",
      "  batch 80 loss: 3.905842572450638\n",
      "  batch 81 loss: 3.9017536714673042\n",
      "  batch 82 loss: 3.902694582939148\n",
      "  batch 83 loss: 3.901540845632553\n",
      "  batch 84 loss: 3.9022848159074783\n",
      "  batch 85 loss: 3.904576398432255\n",
      "  batch 86 loss: 3.9046468883752823\n",
      "  batch 87 loss: 3.9046602696180344\n",
      "  batch 88 loss: 3.9028710424900055\n",
      "  batch 89 loss: 3.9026340171694756\n",
      "  batch 90 loss: 3.902115620672703\n",
      "  batch 91 loss: 3.9032255709171295\n",
      "  batch 92 loss: 3.9032847210764885\n",
      "  batch 93 loss: 3.9029942005872726\n",
      "  batch 94 loss: 3.9025866463780403\n",
      "  batch 95 loss: 3.9013042226433754\n",
      "  batch 96 loss: 3.9035689905285835\n",
      "  batch 97 loss: 3.905620254576206\n",
      "  batch 98 loss: 3.9035690128803253\n",
      "  batch 99 loss: 3.9012132436037064\n",
      "  batch 100 loss: 3.9037712067365646\n",
      "  batch 101 loss: 3.901263825595379\n",
      "  batch 102 loss: 3.9027378037571907\n",
      "  batch 103 loss: 3.902591995894909\n",
      "  batch 104 loss: 3.903915673494339\n",
      "  batch 105 loss: 3.9038675874471664\n",
      "  batch 106 loss: 3.9051104858517647\n",
      "  batch 107 loss: 3.903257191181183\n",
      "  batch 108 loss: 3.9014942422509193\n",
      "  batch 109 loss: 3.9027409702539444\n",
      "  batch 110 loss: 3.9022254794836044\n",
      "  batch 111 loss: 3.9045087322592735\n",
      "  batch 112 loss: 3.903605654835701\n",
      "  batch 113 loss: 3.9017131105065346\n",
      "  batch 114 loss: 3.903187520802021\n",
      "  batch 115 loss: 3.9014698192477226\n",
      "  batch 116 loss: 3.902593784034252\n",
      "  batch 117 loss: 3.9020551294088364\n",
      "  batch 118 loss: 3.9044730737805367\n",
      "  batch 119 loss: 3.9027476087212563\n",
      "  batch 120 loss: 3.901587426662445\n",
      "  batch 121 loss: 3.9033247902989388\n",
      "  batch 122 loss: 3.905783638358116\n",
      "  batch 123 loss: 3.9054311513900757\n",
      "  batch 124 loss: 3.9023957401514053\n",
      "  batch 125 loss: 3.9027158468961716\n",
      "  batch 126 loss: 3.901576980948448\n",
      "  batch 127 loss: 3.902391642332077\n",
      "  batch 128 loss: 3.9015458524227142\n",
      "  batch 129 loss: 3.9026910588145256\n",
      "  batch 130 loss: 3.9011997804045677\n",
      "  batch 131 loss: 3.9027148708701134\n",
      "  batch 132 loss: 3.90230081230402\n",
      "  batch 133 loss: 3.9085547029972076\n",
      "  batch 134 loss: 3.9008975625038147\n",
      "  batch 135 loss: 3.900800921022892\n",
      "  batch 136 loss: 3.9038569554686546\n",
      "  batch 137 loss: 3.9020263329148293\n",
      "  batch 138 loss: 3.904842682182789\n",
      "  batch 139 loss: 3.901749074459076\n",
      "  batch 140 loss: 3.9050476402044296\n",
      "  batch 141 loss: 3.9007728323340416\n",
      "  batch 142 loss: 3.9025199860334396\n",
      "  batch 143 loss: 3.902289852499962\n",
      "  batch 144 loss: 3.9015017673373222\n",
      "  batch 145 loss: 3.901860184967518\n",
      "  batch 146 loss: 3.9038100615143776\n",
      "  batch 147 loss: 3.904247924685478\n",
      "  batch 148 loss: 3.9013486802577972\n",
      "  batch 149 loss: 3.9025497809052467\n",
      "  batch 150 loss: 3.900877572596073\n",
      "  batch 151 loss: 3.9014121741056442\n",
      "  batch 152 loss: 3.9014546126127243\n",
      "  batch 153 loss: 3.90276487916708\n",
      "  batch 154 loss: 3.9027386009693146\n",
      "  batch 155 loss: 3.902164675295353\n",
      "  batch 156 loss: 3.9021873995661736\n",
      "  batch 157 loss: 3.9019738510251045\n",
      "  batch 158 loss: 3.9021239429712296\n",
      "  batch 159 loss: 3.900196723639965\n",
      "  batch 160 loss: 3.902540884912014\n",
      "  batch 161 loss: 3.9013590440154076\n",
      "  batch 162 loss: 3.9037397354841232\n",
      "  batch 163 loss: 3.901191659271717\n",
      "  batch 164 loss: 3.9008841812610626\n",
      "  batch 165 loss: 3.9015824422240257\n",
      "  batch 166 loss: 3.900302030146122\n",
      "  batch 167 loss: 3.9007751494646072\n",
      "  batch 168 loss: 3.9013812616467476\n",
      "  batch 169 loss: 3.901960425078869\n",
      "  batch 170 loss: 3.9015493914484978\n",
      "  batch 171 loss: 3.9017175659537315\n",
      "  batch 172 loss: 3.901264563202858\n",
      "  batch 173 loss: 3.902300924062729\n",
      "  batch 174 loss: 3.9039804488420486\n",
      "  batch 175 loss: 3.900437332689762\n",
      "  batch 176 loss: 3.902167536318302\n",
      "  batch 177 loss: 3.901594892144203\n",
      "  batch 178 loss: 3.9008054807782173\n",
      "  batch 179 loss: 3.9009214714169502\n",
      "  batch 180 loss: 3.9012196138501167\n",
      "  batch 181 loss: 3.9024705216288567\n",
      "  batch 182 loss: 3.9008482471108437\n",
      "  batch 183 loss: 3.900681793689728\n",
      "  batch 184 loss: 3.9005429446697235\n",
      "  batch 185 loss: 3.9021624699234962\n",
      "  batch 186 loss: 3.9003281593322754\n",
      "  batch 187 loss: 3.9007374048233032\n",
      "  batch 188 loss: 3.9008080437779427\n",
      "  batch 189 loss: 3.9008044600486755\n",
      "  batch 190 loss: 3.900516889989376\n",
      "  batch 191 loss: 3.9027146473526955\n",
      "  batch 192 loss: 3.90070703625679\n",
      "  batch 193 loss: 3.9015703350305557\n",
      "  batch 194 loss: 3.9050678461790085\n",
      "  batch 195 loss: 3.900010585784912\n",
      "  batch 196 loss: 3.9015683233737946\n",
      "  batch 197 loss: 3.9026419520378113\n",
      "  batch 198 loss: 3.9009069353342056\n",
      "  batch 199 loss: 3.905831664800644\n",
      "  batch 200 loss: 3.9010062143206596\n",
      "  batch 201 loss: 3.9019686803221703\n",
      "  batch 202 loss: 3.9028193056583405\n",
      "  batch 203 loss: 3.90280831605196\n",
      "  batch 204 loss: 3.901058003306389\n",
      "  batch 205 loss: 3.9004224836826324\n",
      "  batch 206 loss: 3.900873437523842\n",
      "  batch 207 loss: 3.9011399000883102\n",
      "  batch 208 loss: 3.9008732736110687\n",
      "  batch 209 loss: 3.900223769247532\n",
      "  batch 210 loss: 3.9010872319340706\n",
      "  batch 211 loss: 3.901548311114311\n",
      "  batch 212 loss: 3.900912255048752\n",
      "  batch 213 loss: 3.8999074399471283\n",
      "  batch 214 loss: 3.9030967354774475\n",
      "  batch 215 loss: 3.901269994676113\n",
      "  batch 216 loss: 3.902793288230896\n",
      "  batch 217 loss: 3.8995173946022987\n",
      "  batch 218 loss: 3.9000744596123695\n",
      "  batch 219 loss: 3.90046489238739\n",
      "  batch 220 loss: 3.9007942602038383\n",
      "  batch 221 loss: 3.9000461399555206\n",
      "  batch 222 loss: 3.899788059294224\n",
      "  batch 223 loss: 3.9021881446242332\n",
      "  batch 224 loss: 3.900277756154537\n",
      "  batch 225 loss: 3.900474615395069\n",
      "  batch 226 loss: 3.8996362760663033\n",
      "  batch 227 loss: 3.900331422686577\n",
      "  batch 228 loss: 3.9002587497234344\n",
      "  batch 229 loss: 3.900529220700264\n",
      "  batch 230 loss: 3.9024573862552643\n",
      "  batch 231 loss: 3.9001691043376923\n",
      "  batch 232 loss: 3.901861861348152\n",
      "  batch 233 loss: 3.9012619331479073\n",
      "  batch 234 loss: 3.899648256599903\n",
      "  batch 235 loss: 3.900486893951893\n",
      "  batch 236 loss: 3.9005076736211777\n",
      "  batch 237 loss: 3.9015722572803497\n",
      "  batch 238 loss: 3.90035417675972\n",
      "  batch 239 loss: 3.90158049762249\n",
      "  batch 240 loss: 3.9007470682263374\n",
      "  batch 241 loss: 3.899973548948765\n",
      "  batch 242 loss: 3.9006454795598984\n",
      "  batch 243 loss: 3.9006422385573387\n",
      "  batch 244 loss: 3.900760270655155\n",
      "  batch 245 loss: 3.9002702459692955\n",
      "  batch 246 loss: 3.9010871052742004\n",
      "  batch 247 loss: 3.899538964033127\n",
      "  batch 248 loss: 3.8996931612491608\n",
      "  batch 249 loss: 3.900567851960659\n",
      "  batch 250 loss: 3.901562415063381\n",
      "  batch 251 loss: 3.900652691721916\n",
      "  batch 252 loss: 3.899715304374695\n",
      "  batch 253 loss: 3.900247760117054\n",
      "  batch 254 loss: 3.901364028453827\n",
      "  batch 255 loss: 3.9021084010601044\n",
      "  batch 256 loss: 3.901870511472225\n",
      "  batch 257 loss: 3.9019935727119446\n",
      "  batch 258 loss: 3.9004867374897003\n",
      "  batch 259 loss: 3.9009882882237434\n",
      "  batch 260 loss: 3.8998270332813263\n",
      "  batch 261 loss: 3.901306666433811\n",
      "  batch 262 loss: 3.8994770497083664\n",
      "  batch 263 loss: 3.900382421910763\n",
      "  batch 264 loss: 3.8995752036571503\n",
      "  batch 265 loss: 3.900911830365658\n",
      "  batch 266 loss: 3.901921086013317\n",
      "  batch 267 loss: 3.9001953080296516\n",
      "  batch 268 loss: 3.899977073073387\n",
      "  batch 269 loss: 3.898736760020256\n",
      "  batch 270 loss: 3.9000849947333336\n",
      "  batch 271 loss: 3.9001620039343834\n",
      "  batch 272 loss: 3.8994037732481956\n",
      "  batch 273 loss: 3.8997953310608864\n",
      "  batch 274 loss: 3.9001316279172897\n",
      "  batch 275 loss: 3.8996179327368736\n",
      "  batch 276 loss: 3.898832894861698\n",
      "  batch 277 loss: 3.8995651230216026\n",
      "  batch 278 loss: 3.9000940695405006\n",
      "  batch 279 loss: 3.899947017431259\n",
      "  batch 280 loss: 3.899593971669674\n",
      "  batch 281 loss: 3.902124620974064\n",
      "  batch 282 loss: 3.8998150676488876\n",
      "  batch 283 loss: 3.899180628359318\n",
      "  batch 284 loss: 3.8997887894511223\n",
      "  batch 285 loss: 3.9004243463277817\n",
      "  batch 286 loss: 3.9000605642795563\n",
      "  batch 287 loss: 3.9000224843621254\n",
      "  batch 288 loss: 3.8999729827046394\n",
      "  batch 289 loss: 3.898931197822094\n",
      "  batch 290 loss: 3.9000615775585175\n",
      "  batch 291 loss: 3.9015763327479362\n",
      "  batch 292 loss: 3.8998016864061356\n",
      "  batch 293 loss: 3.9005546048283577\n",
      "  batch 294 loss: 3.9001020565629005\n",
      "  batch 295 loss: 3.899746797978878\n",
      "  batch 296 loss: 3.8991899713873863\n",
      "  batch 297 loss: 3.899517595767975\n",
      "  batch 298 loss: 3.8999348804354668\n",
      "  batch 299 loss: 3.8992708697915077\n",
      "  batch 300 loss: 3.8991844430565834\n",
      "  batch 301 loss: 3.899910017848015\n",
      "  batch 302 loss: 3.9003807604312897\n",
      "  batch 303 loss: 3.900117725133896\n",
      "  batch 304 loss: 3.8989772275090218\n",
      "  batch 305 loss: 3.8997953608632088\n",
      "  batch 306 loss: 3.8991093412041664\n",
      "  batch 307 loss: 3.899418294429779\n",
      "  batch 308 loss: 3.900987096130848\n",
      "  batch 309 loss: 3.8995115235447884\n",
      "  batch 310 loss: 3.901676505804062\n",
      "  batch 311 loss: 3.8993000835180283\n",
      "  batch 312 loss: 3.899150349199772\n",
      "  batch 313 loss: 3.8990384489297867\n",
      "  batch 314 loss: 3.9015665277838707\n",
      "  batch 315 loss: 3.900920383632183\n",
      "  batch 316 loss: 3.8985883593559265\n",
      "  batch 317 loss: 3.9005128741264343\n",
      "  batch 318 loss: 3.901577301323414\n",
      "  batch 319 loss: 3.898611471056938\n",
      "  batch 320 loss: 3.899131439626217\n",
      "  batch 321 loss: 3.9001424461603165\n",
      "  batch 322 loss: 3.9006369933485985\n",
      "  batch 323 loss: 3.90023735165596\n",
      "  batch 324 loss: 3.900513842701912\n",
      "  batch 325 loss: 3.8992853090167046\n",
      "  batch 326 loss: 3.898906648159027\n",
      "  batch 327 loss: 3.899087928235531\n",
      "  batch 328 loss: 3.900805465877056\n",
      "  batch 329 loss: 3.8996352776885033\n",
      "  batch 330 loss: 3.8988299444317818\n",
      "  batch 331 loss: 3.899619944393635\n",
      "  batch 332 loss: 3.8993185609579086\n",
      "  batch 333 loss: 3.898756206035614\n",
      "  batch 334 loss: 3.8988190069794655\n",
      "  batch 335 loss: 3.8997301012277603\n",
      "  batch 336 loss: 3.9002351984381676\n",
      "  batch 337 loss: 3.8987627550959587\n",
      "  batch 338 loss: 3.901801809668541\n",
      "  batch 339 loss: 3.8992338851094246\n",
      "  batch 340 loss: 3.8991133272647858\n",
      "  batch 341 loss: 3.901463471353054\n",
      "  batch 342 loss: 3.901377722620964\n",
      "  batch 343 loss: 3.899456962943077\n",
      "  batch 344 loss: 3.8993698805570602\n",
      "  batch 345 loss: 3.899423271417618\n",
      "  batch 346 loss: 3.8992802426218987\n",
      "  batch 347 loss: 3.8988410010933876\n",
      "  batch 348 loss: 3.8986830562353134\n",
      "  batch 349 loss: 3.9007847234606743\n",
      "  batch 350 loss: 3.9004367664456367\n",
      "  batch 351 loss: 3.9000915214419365\n",
      "  batch 352 loss: 3.8983652517199516\n",
      "  batch 353 loss: 3.8998122587800026\n",
      "  batch 354 loss: 3.899557016789913\n",
      "  batch 355 loss: 3.8992174342274666\n",
      "  batch 356 loss: 3.899424448609352\n",
      "  batch 357 loss: 3.898768723011017\n",
      "  batch 358 loss: 3.8985330685973167\n",
      "  batch 359 loss: 3.8987746462225914\n",
      "  batch 360 loss: 3.899299055337906\n",
      "  batch 361 loss: 3.900317668914795\n",
      "  batch 362 loss: 3.900171637535095\n",
      "  batch 363 loss: 3.898989975452423\n",
      "  batch 364 loss: 3.899534910917282\n",
      "  batch 365 loss: 3.90012476593256\n",
      "  batch 366 loss: 3.8990311548113823\n",
      "  batch 367 loss: 3.898603454232216\n",
      "  batch 368 loss: 3.8990961238741875\n",
      "  batch 369 loss: 3.8993785679340363\n",
      "  batch 370 loss: 3.8989101499319077\n",
      "  batch 371 loss: 3.899529978632927\n",
      "  batch 372 loss: 3.9003979191184044\n",
      "  batch 373 loss: 3.898498460650444\n",
      "  batch 374 loss: 3.898832619190216\n",
      "  batch 375 loss: 3.898330368101597\n",
      "  batch 376 loss: 3.89812358468771\n",
      "  batch 377 loss: 3.8992290943861008\n",
      "  batch 378 loss: 3.8994975090026855\n",
      "  batch 379 loss: 3.898497261106968\n",
      "  batch 380 loss: 3.8987064212560654\n",
      "  batch 381 loss: 3.899253748357296\n",
      "  batch 382 loss: 3.9007592871785164\n",
      "  batch 383 loss: 3.8993181735277176\n",
      "  batch 384 loss: 3.8985412642359734\n",
      "  batch 385 loss: 3.899347148835659\n",
      "  batch 386 loss: 3.8985985666513443\n",
      "  batch 387 loss: 3.8986774682998657\n",
      "  batch 388 loss: 3.898768648505211\n",
      "  batch 389 loss: 3.8990127742290497\n",
      "  batch 390 loss: 3.9133932143449783\n",
      "  batch 391 loss: 3.8982644081115723\n",
      "  batch 392 loss: 3.8991594091057777\n",
      "  batch 393 loss: 3.898587182164192\n",
      "  batch 394 loss: 3.8989697843790054\n",
      "  batch 395 loss: 3.8997028470039368\n",
      "  batch 396 loss: 3.899065837264061\n",
      "  batch 397 loss: 3.8990968093276024\n",
      "  batch 398 loss: 3.898323215544224\n",
      "  batch 399 loss: 3.8984294682741165\n",
      "  batch 400 loss: 3.8986828848719597\n",
      "  batch 401 loss: 3.8992778584361076\n",
      "  batch 402 loss: 3.89894687384367\n",
      "  batch 403 loss: 3.8988768830895424\n",
      "  batch 404 loss: 3.8987986519932747\n",
      "  batch 405 loss: 3.8981776237487793\n",
      "  batch 406 loss: 3.8983638286590576\n",
      "  batch 407 loss: 3.8988848477602005\n",
      "  batch 408 loss: 3.898132748901844\n",
      "  batch 409 loss: 3.898958809673786\n",
      "  batch 410 loss: 3.899270735681057\n",
      "  batch 411 loss: 3.8996399343013763\n",
      "  batch 412 loss: 3.898297280073166\n",
      "  batch 413 loss: 3.8996783196926117\n",
      "  batch 414 loss: 3.9007911905646324\n",
      "  batch 415 loss: 3.9002955183386803\n",
      "  batch 416 loss: 3.898172579705715\n",
      "  batch 417 loss: 3.8990249037742615\n",
      "  batch 418 loss: 3.8986534997820854\n",
      "  batch 419 loss: 3.898359127342701\n",
      "  batch 420 loss: 3.8998229429125786\n",
      "  batch 421 loss: 3.898050770163536\n",
      "  batch 422 loss: 3.8990934416651726\n",
      "  batch 423 loss: 3.8995595946907997\n",
      "  batch 424 loss: 3.8993051573634148\n",
      "LOSS train 3.8993051573634148 valid 3.8976378173760646\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.898391269147396\n",
      "  batch 2 loss: 3.8984517008066177\n",
      "  batch 3 loss: 3.899396523833275\n",
      "  batch 4 loss: 3.8983254432678223\n",
      "  batch 5 loss: 3.8990851268172264\n",
      "  batch 6 loss: 3.8982485458254814\n",
      "  batch 7 loss: 3.8984076902270317\n",
      "  batch 8 loss: 3.899978958070278\n",
      "  batch 9 loss: 3.8984988033771515\n",
      "  batch 10 loss: 3.898130252957344\n",
      "  batch 11 loss: 3.8987559154629707\n",
      "  batch 12 loss: 3.8979577124118805\n",
      "  batch 13 loss: 3.8985298573970795\n",
      "  batch 14 loss: 3.898593381047249\n",
      "  batch 15 loss: 3.898411840200424\n",
      "  batch 16 loss: 3.8986220732331276\n",
      "  batch 17 loss: 3.898385725915432\n",
      "  batch 18 loss: 3.8989795073866844\n",
      "  batch 19 loss: 3.8979242220520973\n",
      "  batch 20 loss: 3.8985179886221886\n",
      "  batch 21 loss: 3.8992640301585197\n",
      "  batch 22 loss: 3.8998020589351654\n",
      "  batch 23 loss: 3.8983703553676605\n",
      "  batch 24 loss: 3.8989933282136917\n",
      "  batch 25 loss: 3.8983497247099876\n",
      "  batch 26 loss: 3.8979707583785057\n",
      "  batch 27 loss: 3.8980506286025047\n",
      "  batch 28 loss: 3.898626148700714\n",
      "  batch 29 loss: 3.898548498749733\n",
      "  batch 30 loss: 3.8986360654234886\n",
      "  batch 31 loss: 3.898486629128456\n",
      "  batch 32 loss: 3.8992781788110733\n",
      "  batch 33 loss: 3.8983921706676483\n",
      "  batch 34 loss: 3.898420974612236\n",
      "  batch 35 loss: 3.89899942278862\n",
      "  batch 36 loss: 3.8992188423871994\n",
      "  batch 37 loss: 3.8992328867316246\n",
      "  batch 38 loss: 3.8981201723217964\n",
      "  batch 39 loss: 3.899033986032009\n",
      "  batch 40 loss: 3.899137906730175\n",
      "  batch 41 loss: 3.8986922577023506\n",
      "  batch 42 loss: 3.898246370255947\n",
      "  batch 43 loss: 3.901438221335411\n",
      "  batch 44 loss: 3.898923210799694\n",
      "  batch 45 loss: 3.8981232568621635\n",
      "  batch 46 loss: 3.898629382252693\n",
      "  batch 47 loss: 3.8978480771183968\n",
      "  batch 48 loss: 3.8989755660295486\n",
      "  batch 49 loss: 3.8987030163407326\n",
      "  batch 50 loss: 3.898872047662735\n",
      "  batch 51 loss: 3.900056444108486\n",
      "  batch 52 loss: 3.8986704126000404\n",
      "  batch 53 loss: 3.898256577551365\n",
      "  batch 54 loss: 3.8986448273062706\n",
      "  batch 55 loss: 3.898004524409771\n",
      "  batch 56 loss: 3.8979136869311333\n",
      "  batch 57 loss: 3.8976703137159348\n",
      "  batch 58 loss: 3.8980948850512505\n",
      "  batch 59 loss: 3.8979847878217697\n",
      "  batch 60 loss: 3.8980576246976852\n",
      "  batch 61 loss: 3.8983967006206512\n",
      "  batch 62 loss: 3.898946776986122\n",
      "  batch 63 loss: 3.898071713745594\n",
      "  batch 64 loss: 3.8993686735630035\n",
      "  batch 65 loss: 3.8983112424612045\n",
      "  batch 66 loss: 3.8989318683743477\n",
      "  batch 67 loss: 3.897998221218586\n",
      "  batch 68 loss: 3.8985493034124374\n",
      "  batch 69 loss: 3.898476891219616\n",
      "  batch 70 loss: 3.898300401866436\n",
      "  batch 71 loss: 3.8980260640382767\n",
      "  batch 72 loss: 3.8981547728180885\n",
      "  batch 73 loss: 3.897993192076683\n",
      "  batch 74 loss: 3.8982083424925804\n",
      "  batch 75 loss: 3.8991468474268913\n",
      "  batch 76 loss: 3.8982637524604797\n",
      "  batch 77 loss: 3.9004109129309654\n",
      "  batch 78 loss: 3.8985527977347374\n",
      "  batch 79 loss: 3.898283876478672\n",
      "  batch 80 loss: 3.8981816321611404\n",
      "  batch 81 loss: 3.8983055874705315\n",
      "  batch 82 loss: 3.898501142859459\n",
      "  batch 83 loss: 3.8990484923124313\n",
      "  batch 84 loss: 3.898542858660221\n",
      "  batch 85 loss: 3.898816652595997\n",
      "  batch 86 loss: 3.898058108985424\n",
      "  batch 87 loss: 3.8981748074293137\n",
      "  batch 88 loss: 3.8986502960324287\n",
      "  batch 89 loss: 3.8985875248908997\n",
      "  batch 90 loss: 3.898978427052498\n",
      "  batch 91 loss: 3.8980217650532722\n",
      "  batch 92 loss: 3.8978219479322433\n",
      "  batch 93 loss: 3.898042671382427\n",
      "  batch 94 loss: 3.8981075659394264\n",
      "  batch 95 loss: 3.898843824863434\n",
      "  batch 96 loss: 3.8982947170734406\n",
      "  batch 97 loss: 3.9000772684812546\n",
      "  batch 98 loss: 3.898086301982403\n",
      "  batch 99 loss: 3.8979322984814644\n",
      "  batch 100 loss: 3.8978511840105057\n",
      "  batch 101 loss: 3.8982039242982864\n",
      "  batch 102 loss: 3.8978516459465027\n",
      "  batch 103 loss: 3.8979744762182236\n",
      "  batch 104 loss: 3.899075210094452\n",
      "  batch 105 loss: 3.8986880630254745\n",
      "  batch 106 loss: 3.8978568017482758\n",
      "  batch 107 loss: 3.8979946225881577\n",
      "  batch 108 loss: 3.8982720971107483\n",
      "  batch 109 loss: 3.8979246094822884\n",
      "  batch 110 loss: 3.8977107033133507\n",
      "  batch 111 loss: 3.8980255275964737\n",
      "  batch 112 loss: 3.898446962237358\n",
      "  batch 113 loss: 3.8977305814623833\n",
      "  batch 114 loss: 3.898612327873707\n",
      "  batch 115 loss: 3.898236356675625\n",
      "  batch 116 loss: 3.8979072496294975\n",
      "  batch 117 loss: 3.8984021320939064\n",
      "  batch 118 loss: 3.898176908493042\n",
      "  batch 119 loss: 3.89801412075758\n",
      "  batch 120 loss: 3.8990416675806046\n",
      "  batch 121 loss: 3.898168623447418\n",
      "  batch 122 loss: 3.898226983845234\n",
      "  batch 123 loss: 3.898586891591549\n",
      "  batch 124 loss: 3.89813195168972\n",
      "  batch 125 loss: 3.897804282605648\n",
      "  batch 126 loss: 3.898168556392193\n",
      "  batch 127 loss: 3.8978396132588387\n",
      "  batch 128 loss: 3.897402249276638\n",
      "  batch 129 loss: 3.897961027920246\n",
      "  batch 130 loss: 3.898678570985794\n",
      "  batch 131 loss: 3.898284427821636\n",
      "  batch 132 loss: 3.8985225409269333\n",
      "  batch 133 loss: 3.898659475147724\n",
      "  batch 134 loss: 3.898944966495037\n",
      "  batch 135 loss: 3.8977860137820244\n",
      "  batch 136 loss: 3.898477204144001\n",
      "  batch 137 loss: 3.8982298225164413\n",
      "  batch 138 loss: 3.8976935744285583\n",
      "  batch 139 loss: 3.8974715545773506\n",
      "  batch 140 loss: 3.897930771112442\n",
      "  batch 141 loss: 3.898043856024742\n",
      "  batch 142 loss: 3.897722467780113\n",
      "  batch 143 loss: 3.898139163851738\n",
      "  batch 144 loss: 3.8978074938058853\n",
      "  batch 145 loss: 3.897454634308815\n",
      "  batch 146 loss: 3.898316264152527\n",
      "  batch 147 loss: 3.897994317114353\n",
      "  batch 148 loss: 3.898609898984432\n",
      "  batch 149 loss: 3.8976586014032364\n",
      "  batch 150 loss: 3.897745333611965\n",
      "  batch 151 loss: 3.899337016046047\n",
      "  batch 152 loss: 3.897743009030819\n",
      "  batch 153 loss: 3.8977524489164352\n",
      "  batch 154 loss: 3.897587336599827\n",
      "  batch 155 loss: 3.8977011665701866\n",
      "  batch 156 loss: 3.899394541978836\n",
      "  batch 157 loss: 3.898142822086811\n",
      "  batch 158 loss: 3.898257441818714\n",
      "  batch 159 loss: 3.897726759314537\n",
      "  batch 160 loss: 3.8994300067424774\n",
      "  batch 161 loss: 3.897628463804722\n",
      "  batch 162 loss: 3.898443304002285\n",
      "  batch 163 loss: 3.8978827446699142\n",
      "  batch 164 loss: 3.89952290058136\n",
      "  batch 165 loss: 3.897652141749859\n",
      "  batch 166 loss: 3.898623324930668\n",
      "  batch 167 loss: 3.8976138159632683\n",
      "  batch 168 loss: 3.8983502611517906\n",
      "  batch 169 loss: 3.8978675454854965\n",
      "  batch 170 loss: 3.8987385854125023\n",
      "  batch 171 loss: 3.8978140726685524\n",
      "  batch 172 loss: 3.898444913327694\n",
      "  batch 173 loss: 3.8982048258185387\n",
      "  batch 174 loss: 3.897405229508877\n",
      "  batch 175 loss: 3.899851769208908\n",
      "  batch 176 loss: 3.8979277908802032\n",
      "  batch 177 loss: 3.898252956569195\n",
      "  batch 178 loss: 3.897402487695217\n",
      "  batch 179 loss: 3.897704616189003\n",
      "  batch 180 loss: 3.897951625287533\n",
      "  batch 181 loss: 3.8999858051538467\n",
      "  batch 182 loss: 3.897537387907505\n",
      "  batch 183 loss: 3.897852636873722\n",
      "  batch 184 loss: 3.8983342200517654\n",
      "  batch 185 loss: 3.897942915558815\n",
      "  batch 186 loss: 3.8978018909692764\n",
      "  batch 187 loss: 3.8973845466971397\n",
      "  batch 188 loss: 3.897672913968563\n",
      "  batch 189 loss: 3.8985686004161835\n",
      "  batch 190 loss: 3.8979752734303474\n",
      "  batch 191 loss: 3.8975277692079544\n",
      "  batch 192 loss: 3.8980960845947266\n",
      "  batch 193 loss: 3.8975444734096527\n",
      "  batch 194 loss: 3.8978208750486374\n",
      "  batch 195 loss: 3.8978129774332047\n",
      "  batch 196 loss: 3.8981022238731384\n",
      "  batch 197 loss: 3.8983186706900597\n",
      "  batch 198 loss: 3.897527292370796\n",
      "  batch 199 loss: 3.897642768919468\n",
      "  batch 200 loss: 3.8986188173294067\n",
      "  batch 201 loss: 3.8976982086896896\n",
      "  batch 202 loss: 3.900078222155571\n",
      "  batch 203 loss: 3.898447960615158\n",
      "  batch 204 loss: 3.8978496342897415\n",
      "  batch 205 loss: 3.8986151665449142\n",
      "  batch 206 loss: 3.897575303912163\n",
      "  batch 207 loss: 3.89774189889431\n",
      "  batch 208 loss: 3.8980130329728127\n",
      "  batch 209 loss: 3.897587925195694\n",
      "  batch 210 loss: 3.8984109312295914\n",
      "  batch 211 loss: 3.8984497785568237\n",
      "  batch 212 loss: 3.897752046585083\n",
      "  batch 213 loss: 3.8978087455034256\n",
      "  batch 214 loss: 3.897588089108467\n",
      "  batch 215 loss: 3.897514060139656\n",
      "  batch 216 loss: 3.897931456565857\n",
      "  batch 217 loss: 3.897597908973694\n",
      "  batch 218 loss: 3.8982662856578827\n",
      "  batch 219 loss: 3.897454112768173\n",
      "  batch 220 loss: 3.8977870047092438\n",
      "  batch 221 loss: 3.897836811840534\n",
      "  batch 222 loss: 3.898485243320465\n",
      "  batch 223 loss: 3.89778383821249\n",
      "  batch 224 loss: 3.8977347314357758\n",
      "  batch 225 loss: 3.8980851992964745\n",
      "  batch 226 loss: 3.8982750698924065\n",
      "  batch 227 loss: 3.8975925594568253\n",
      "  batch 228 loss: 3.8978430554270744\n",
      "  batch 229 loss: 3.897546596825123\n",
      "  batch 230 loss: 3.8975609838962555\n",
      "  batch 231 loss: 3.897305876016617\n",
      "  batch 232 loss: 3.8971205353736877\n",
      "  batch 233 loss: 3.8981467485427856\n",
      "  batch 234 loss: 3.89790291339159\n",
      "  batch 235 loss: 3.8975374028086662\n",
      "  batch 236 loss: 3.8983632400631905\n",
      "  batch 237 loss: 3.898216724395752\n",
      "  batch 238 loss: 3.8973193168640137\n",
      "  batch 239 loss: 3.897507756948471\n",
      "  batch 240 loss: 3.897215574979782\n",
      "  batch 241 loss: 3.898823857307434\n",
      "  batch 242 loss: 3.898963026702404\n",
      "  batch 243 loss: 3.8975667506456375\n",
      "  batch 244 loss: 3.8972507640719414\n",
      "  batch 245 loss: 3.8979309126734734\n",
      "  batch 246 loss: 3.897345647215843\n",
      "  batch 247 loss: 3.8976722434163094\n",
      "  batch 248 loss: 3.8976185023784637\n",
      "  batch 249 loss: 3.897978760302067\n",
      "  batch 250 loss: 3.8977967649698257\n",
      "  batch 251 loss: 3.8973887637257576\n",
      "  batch 252 loss: 3.898080438375473\n",
      "  batch 253 loss: 3.8976386412978172\n",
      "  batch 254 loss: 3.897423319518566\n",
      "  batch 255 loss: 3.897345259785652\n",
      "  batch 256 loss: 3.8984140679240227\n",
      "  batch 257 loss: 3.897580198943615\n",
      "  batch 258 loss: 3.8973339945077896\n",
      "  batch 259 loss: 3.8989442959427834\n",
      "  batch 260 loss: 3.8972811102867126\n",
      "  batch 261 loss: 3.8976526260375977\n",
      "  batch 262 loss: 3.89768423140049\n",
      "  batch 263 loss: 3.8978521898388863\n",
      "  batch 264 loss: 3.8976762145757675\n",
      "  batch 265 loss: 3.897296130657196\n",
      "  batch 266 loss: 3.8974859043955803\n",
      "  batch 267 loss: 3.8975665345788\n",
      "  batch 268 loss: 3.8981529399752617\n",
      "  batch 269 loss: 3.897452414035797\n",
      "  batch 270 loss: 3.897466279566288\n",
      "  batch 271 loss: 3.8974612951278687\n",
      "  batch 272 loss: 3.900985822081566\n",
      "  batch 273 loss: 3.8979449421167374\n",
      "  batch 274 loss: 3.897022731602192\n",
      "  batch 275 loss: 3.8977311849594116\n",
      "  batch 276 loss: 3.8978268429636955\n",
      "  batch 277 loss: 3.897727556526661\n",
      "  batch 278 loss: 3.8972839787602425\n",
      "  batch 279 loss: 3.8975480049848557\n",
      "  batch 280 loss: 3.8972655460238457\n",
      "  batch 281 loss: 3.8974074348807335\n",
      "  batch 282 loss: 3.898351825773716\n",
      "  batch 283 loss: 3.8975194469094276\n",
      "  batch 284 loss: 3.8972659558057785\n",
      "  batch 285 loss: 3.8973279669880867\n",
      "  batch 286 loss: 3.8976051285862923\n",
      "  batch 287 loss: 3.8969216495752335\n",
      "  batch 288 loss: 3.896995909512043\n",
      "  batch 289 loss: 3.8982659056782722\n",
      "  batch 290 loss: 3.8977839574217796\n",
      "  batch 291 loss: 3.8975288420915604\n",
      "  batch 292 loss: 3.8973720595240593\n",
      "  batch 293 loss: 3.897537410259247\n",
      "  batch 294 loss: 3.8973532542586327\n",
      "  batch 295 loss: 3.897519513964653\n",
      "  batch 296 loss: 3.8971828669309616\n",
      "  batch 297 loss: 3.8970879167318344\n",
      "  batch 298 loss: 3.897880904376507\n",
      "  batch 299 loss: 3.897665113210678\n",
      "  batch 300 loss: 3.897081106901169\n",
      "  batch 301 loss: 3.897369585931301\n",
      "  batch 302 loss: 3.898322142660618\n",
      "  batch 303 loss: 3.897908851504326\n",
      "  batch 304 loss: 3.8977300077676773\n",
      "  batch 305 loss: 3.897385463118553\n",
      "  batch 306 loss: 3.8975063115358353\n",
      "  batch 307 loss: 3.8970756381750107\n",
      "  batch 308 loss: 3.8975399807095528\n",
      "  batch 309 loss: 3.8973876908421516\n",
      "  batch 310 loss: 3.89851663261652\n",
      "  batch 311 loss: 3.897037237882614\n",
      "  batch 312 loss: 3.897114746272564\n",
      "  batch 313 loss: 3.897554136812687\n",
      "  batch 314 loss: 3.8972412273287773\n",
      "  batch 315 loss: 3.8979733884334564\n",
      "  batch 316 loss: 3.8975590020418167\n",
      "  batch 317 loss: 3.8979485780000687\n",
      "  batch 318 loss: 3.897314667701721\n",
      "  batch 319 loss: 3.8971912041306496\n",
      "  batch 320 loss: 3.8974949568510056\n",
      "  batch 321 loss: 3.8972690254449844\n",
      "  batch 322 loss: 3.897357665002346\n",
      "  batch 323 loss: 3.8972584009170532\n",
      "  batch 324 loss: 3.897587090730667\n",
      "  batch 325 loss: 3.897899329662323\n",
      "  batch 326 loss: 3.8973536491394043\n",
      "  batch 327 loss: 3.897359348833561\n",
      "  batch 328 loss: 3.897168554365635\n",
      "  batch 329 loss: 3.897476591169834\n",
      "  batch 330 loss: 3.897345647215843\n",
      "  batch 331 loss: 3.8975761085748672\n",
      "  batch 332 loss: 3.897205203771591\n",
      "  batch 333 loss: 3.897679664194584\n",
      "  batch 334 loss: 3.8975945711135864\n",
      "  batch 335 loss: 3.8975002244114876\n",
      "  batch 336 loss: 3.897372044622898\n",
      "  batch 337 loss: 3.8976616710424423\n",
      "  batch 338 loss: 3.8976315557956696\n",
      "  batch 339 loss: 3.8980503529310226\n",
      "  batch 340 loss: 3.897545389831066\n",
      "  batch 341 loss: 3.8975978791713715\n",
      "  batch 342 loss: 3.8973144963383675\n",
      "  batch 343 loss: 3.897491827607155\n",
      "  batch 344 loss: 3.897204667329788\n",
      "  batch 345 loss: 3.8972921147942543\n",
      "  batch 346 loss: 3.8970847129821777\n",
      "  batch 347 loss: 3.896823897957802\n",
      "  batch 348 loss: 3.8970005065202713\n",
      "  batch 349 loss: 3.8975002840161324\n",
      "  batch 350 loss: 3.897179886698723\n",
      "  batch 351 loss: 3.8973518013954163\n",
      "  batch 352 loss: 3.897229239344597\n",
      "  batch 353 loss: 3.8971003741025925\n",
      "  batch 354 loss: 3.897315666079521\n",
      "  batch 355 loss: 3.897946819663048\n",
      "  batch 356 loss: 3.8972334414720535\n",
      "  batch 357 loss: 3.897600881755352\n",
      "  batch 358 loss: 3.897198438644409\n",
      "  batch 359 loss: 3.896972320973873\n",
      "  batch 360 loss: 3.8970013186335564\n",
      "  batch 361 loss: 3.8972427174448967\n",
      "  batch 362 loss: 3.8972271233797073\n",
      "  batch 363 loss: 3.897759310901165\n",
      "  batch 364 loss: 3.8974596112966537\n",
      "  batch 365 loss: 3.8976616710424423\n",
      "  batch 366 loss: 3.8973460346460342\n",
      "  batch 367 loss: 3.897895097732544\n",
      "  batch 368 loss: 3.897916443645954\n",
      "  batch 369 loss: 3.897200882434845\n",
      "  batch 370 loss: 3.896906591951847\n",
      "  batch 371 loss: 3.897327981889248\n",
      "  batch 372 loss: 3.897030271589756\n",
      "  batch 373 loss: 3.89717947691679\n",
      "  batch 374 loss: 3.897645354270935\n",
      "  batch 375 loss: 3.896826282143593\n",
      "  batch 376 loss: 3.8971883431077003\n",
      "  batch 377 loss: 3.897676520049572\n",
      "  batch 378 loss: 3.8972900733351707\n",
      "  batch 379 loss: 3.896940268576145\n",
      "  batch 380 loss: 3.8969413340091705\n",
      "  batch 381 loss: 3.897069990634918\n",
      "  batch 382 loss: 3.897536590695381\n",
      "  batch 383 loss: 3.897157333791256\n",
      "  batch 384 loss: 3.897197663784027\n",
      "  batch 385 loss: 3.8972483798861504\n",
      "  batch 386 loss: 3.897124759852886\n",
      "  batch 387 loss: 3.897738993167877\n",
      "  batch 388 loss: 3.8970483168959618\n",
      "  batch 389 loss: 3.8969485759735107\n",
      "  batch 390 loss: 3.8970764353871346\n",
      "  batch 391 loss: 3.8973953053355217\n",
      "  batch 392 loss: 3.89715389162302\n",
      "  batch 393 loss: 3.8979846239089966\n",
      "  batch 394 loss: 3.8973559513688087\n",
      "  batch 395 loss: 3.897322289645672\n",
      "  batch 396 loss: 3.8981423005461693\n",
      "  batch 397 loss: 3.897289790213108\n",
      "  batch 398 loss: 3.8974189907312393\n",
      "  batch 399 loss: 3.897068239748478\n",
      "  batch 400 loss: 3.8970250710844994\n",
      "  batch 401 loss: 3.897730514407158\n",
      "  batch 402 loss: 3.897060178220272\n",
      "  batch 403 loss: 3.8972591385245323\n",
      "  batch 404 loss: 3.8971825316548347\n",
      "  batch 405 loss: 3.897553399205208\n",
      "  batch 406 loss: 3.8971815034747124\n",
      "  batch 407 loss: 3.896969974040985\n",
      "  batch 408 loss: 3.8974930197000504\n",
      "  batch 409 loss: 3.8970382511615753\n",
      "  batch 410 loss: 3.8977128863334656\n",
      "  batch 411 loss: 3.897378131747246\n",
      "  batch 412 loss: 3.896893784403801\n",
      "  batch 413 loss: 3.8971184715628624\n",
      "  batch 414 loss: 3.8973532542586327\n",
      "  batch 415 loss: 3.8972572907805443\n",
      "  batch 416 loss: 3.896917447447777\n",
      "  batch 417 loss: 3.8972247913479805\n",
      "  batch 418 loss: 3.897477440536022\n",
      "  batch 419 loss: 3.8970011696219444\n",
      "  batch 420 loss: 3.8973048254847527\n",
      "  batch 421 loss: 3.896921269595623\n",
      "  batch 422 loss: 3.8971961811184883\n",
      "  batch 423 loss: 3.8978089541196823\n",
      "  batch 424 loss: 3.8974201008677483\n",
      "LOSS train 3.8974201008677483 valid 3.8966315811113366\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.8971416652202606\n",
      "  batch 2 loss: 3.8974033147096634\n",
      "  batch 3 loss: 3.897398851811886\n",
      "  batch 4 loss: 3.896872952580452\n",
      "  batch 5 loss: 3.8971274122595787\n",
      "  batch 6 loss: 3.8972486332058907\n",
      "  batch 7 loss: 3.8973648846149445\n",
      "  batch 8 loss: 3.897305905818939\n",
      "  batch 9 loss: 3.8973884135484695\n",
      "  batch 10 loss: 3.8971818536520004\n",
      "  batch 11 loss: 3.8968502059578896\n",
      "  batch 12 loss: 3.8969216123223305\n",
      "  batch 13 loss: 3.8970198035240173\n",
      "  batch 14 loss: 3.897024028003216\n",
      "  batch 15 loss: 3.897307686507702\n",
      "  batch 16 loss: 3.897533133625984\n",
      "  batch 17 loss: 3.8972716629505157\n",
      "  batch 18 loss: 3.8970976397395134\n",
      "  batch 19 loss: 3.8968378454446793\n",
      "  batch 20 loss: 3.897109739482403\n",
      "  batch 21 loss: 3.897147335112095\n",
      "  batch 22 loss: 3.8971515148878098\n",
      "  batch 23 loss: 3.8974700570106506\n",
      "  batch 24 loss: 3.897572085261345\n",
      "  batch 25 loss: 3.897198908030987\n",
      "  batch 26 loss: 3.897808738052845\n",
      "  batch 27 loss: 3.8970819264650345\n",
      "  batch 28 loss: 3.897212006151676\n",
      "  batch 29 loss: 3.8975184857845306\n",
      "  batch 30 loss: 3.8970948308706284\n",
      "  batch 31 loss: 3.897005893290043\n",
      "  batch 32 loss: 3.897194504737854\n",
      "  batch 33 loss: 3.8971789702773094\n",
      "  batch 34 loss: 3.897128328680992\n",
      "  batch 35 loss: 3.8984590992331505\n",
      "  batch 36 loss: 3.8969121500849724\n",
      "  batch 37 loss: 3.897693745791912\n",
      "  batch 38 loss: 3.897405318915844\n",
      "  batch 39 loss: 3.897305481135845\n",
      "  batch 40 loss: 3.897364765405655\n",
      "  batch 41 loss: 3.8974083811044693\n",
      "  batch 42 loss: 3.8976194709539413\n",
      "  batch 43 loss: 3.8968286365270615\n",
      "  batch 44 loss: 3.897343657910824\n",
      "  batch 45 loss: 3.8974086865782738\n",
      "  batch 46 loss: 3.8972228839993477\n",
      "  batch 47 loss: 3.8971340358257294\n",
      "  batch 48 loss: 3.8969997838139534\n",
      "  batch 49 loss: 3.8976583257317543\n",
      "  batch 50 loss: 3.897204704582691\n",
      "  batch 51 loss: 3.896883472800255\n",
      "  batch 52 loss: 3.8968506306409836\n",
      "  batch 53 loss: 3.8976424261927605\n",
      "  batch 54 loss: 3.896781675517559\n",
      "  batch 55 loss: 3.8976647779345512\n",
      "  batch 56 loss: 3.897220067679882\n",
      "  batch 57 loss: 3.8967826515436172\n",
      "  batch 58 loss: 3.897155210375786\n",
      "  batch 59 loss: 3.8970581218600273\n",
      "  batch 60 loss: 3.8972294107079506\n",
      "  batch 61 loss: 3.8970280811190605\n",
      "  batch 62 loss: 3.8972898945212364\n",
      "  batch 63 loss: 3.896999403834343\n",
      "  batch 64 loss: 3.8976133540272713\n",
      "  batch 65 loss: 3.8967747315764427\n",
      "  batch 66 loss: 3.896834947168827\n",
      "  batch 67 loss: 3.8969131261110306\n",
      "  batch 68 loss: 3.897180490195751\n",
      "  batch 69 loss: 3.896873213350773\n",
      "  batch 70 loss: 3.89677707105875\n",
      "  batch 71 loss: 3.8971634805202484\n",
      "  batch 72 loss: 3.8975125029683113\n",
      "  batch 73 loss: 3.8970594704151154\n",
      "  batch 74 loss: 3.897173836827278\n",
      "  batch 75 loss: 3.8969564735889435\n",
      "  batch 76 loss: 3.8968574851751328\n",
      "  batch 77 loss: 3.8969411849975586\n",
      "  batch 78 loss: 3.8976737782359123\n",
      "  batch 79 loss: 3.8970023542642593\n",
      "  batch 80 loss: 3.8970103710889816\n",
      "  batch 81 loss: 3.8970653489232063\n",
      "  batch 82 loss: 3.8968012258410454\n",
      "  batch 83 loss: 3.8967543095350266\n",
      "  batch 84 loss: 3.8973417058587074\n",
      "  batch 85 loss: 3.898121513426304\n",
      "  batch 86 loss: 3.897856906056404\n",
      "  batch 87 loss: 3.896846741437912\n",
      "  batch 88 loss: 3.89806866645813\n",
      "  batch 89 loss: 3.897379405796528\n",
      "  batch 90 loss: 3.8970610052347183\n",
      "  batch 91 loss: 3.897162139415741\n",
      "  batch 92 loss: 3.897489070892334\n",
      "  batch 93 loss: 3.8968077301979065\n",
      "  batch 94 loss: 3.896829143166542\n",
      "  batch 95 loss: 3.896915093064308\n",
      "  batch 96 loss: 3.898085817694664\n",
      "  batch 97 loss: 3.897810973227024\n",
      "  batch 98 loss: 3.8969318494200706\n",
      "  batch 99 loss: 3.8975394517183304\n",
      "  batch 100 loss: 3.8969500064849854\n",
      "  batch 101 loss: 3.897058881819248\n",
      "  batch 102 loss: 3.8970556929707527\n",
      "  batch 103 loss: 3.896829418838024\n",
      "  batch 104 loss: 3.8969978243112564\n",
      "  batch 105 loss: 3.8967758268117905\n",
      "  batch 106 loss: 3.89679629355669\n",
      "  batch 107 loss: 3.896742396056652\n",
      "  batch 108 loss: 3.896948531270027\n",
      "  batch 109 loss: 3.896881863474846\n",
      "  batch 110 loss: 3.8969466388225555\n",
      "  batch 111 loss: 3.8977873474359512\n",
      "  batch 112 loss: 3.896903522312641\n",
      "  batch 113 loss: 3.8966950848698616\n",
      "  batch 114 loss: 3.8968335911631584\n",
      "  batch 115 loss: 3.8968943431973457\n",
      "  batch 116 loss: 3.8969027549028397\n",
      "  batch 117 loss: 3.8972171768546104\n",
      "  batch 118 loss: 3.8969402462244034\n",
      "  batch 119 loss: 3.896933600306511\n",
      "  batch 120 loss: 3.897046834230423\n",
      "  batch 121 loss: 3.897396169602871\n",
      "  batch 122 loss: 3.8972118720412254\n",
      "  batch 123 loss: 3.8970397785305977\n",
      "  batch 124 loss: 3.8969658985733986\n",
      "  batch 125 loss: 3.8969445526599884\n",
      "  batch 126 loss: 3.8968718871474266\n",
      "  batch 127 loss: 3.8968628495931625\n",
      "  batch 128 loss: 3.8976036086678505\n",
      "  batch 129 loss: 3.8969117775559425\n",
      "  batch 130 loss: 3.8967290818691254\n",
      "  batch 131 loss: 3.89706040173769\n",
      "  batch 132 loss: 3.8972817808389664\n",
      "  batch 133 loss: 3.8970705941319466\n",
      "  batch 134 loss: 3.8967633098363876\n",
      "  batch 135 loss: 3.8970791921019554\n",
      "  batch 136 loss: 3.8970421701669693\n",
      "  batch 137 loss: 3.896970972418785\n",
      "  batch 138 loss: 3.8969071581959724\n",
      "  batch 139 loss: 3.8968069031834602\n",
      "  batch 140 loss: 3.896974600851536\n",
      "  batch 141 loss: 3.8971601873636246\n",
      "  batch 142 loss: 3.89687966555357\n",
      "  batch 143 loss: 3.8968579471111298\n",
      "  batch 144 loss: 3.8968530148267746\n",
      "  batch 145 loss: 3.897002376616001\n",
      "  batch 146 loss: 3.8971907570958138\n",
      "  batch 147 loss: 3.89688478410244\n",
      "  batch 148 loss: 3.8966826796531677\n",
      "  batch 149 loss: 3.8975769728422165\n",
      "  batch 150 loss: 3.897859774529934\n",
      "  batch 151 loss: 3.8966355696320534\n",
      "  batch 152 loss: 3.896594502031803\n",
      "  batch 153 loss: 3.8968580961227417\n",
      "  batch 154 loss: 3.8970920592546463\n",
      "  batch 155 loss: 3.89666561037302\n",
      "  batch 156 loss: 3.896598905324936\n",
      "  batch 157 loss: 3.8967624455690384\n",
      "  batch 158 loss: 3.8970928713679314\n",
      "  batch 159 loss: 3.8967882469296455\n",
      "  batch 160 loss: 3.897244058549404\n",
      "  batch 161 loss: 3.897097796201706\n",
      "  batch 162 loss: 3.8967964872717857\n",
      "  batch 163 loss: 3.897014781832695\n",
      "  batch 164 loss: 3.8969449177384377\n",
      "  batch 165 loss: 3.8972308188676834\n",
      "  batch 166 loss: 3.8969023525714874\n",
      "  batch 167 loss: 3.896893188357353\n",
      "  batch 168 loss: 3.8967599123716354\n",
      "  batch 169 loss: 3.8969779312610626\n",
      "  batch 170 loss: 3.8969859778881073\n",
      "  batch 171 loss: 3.89666336029768\n",
      "  batch 172 loss: 3.8969322368502617\n",
      "  batch 173 loss: 3.896886110305786\n",
      "  batch 174 loss: 3.8968081697821617\n",
      "  batch 175 loss: 3.8966405764222145\n",
      "  batch 176 loss: 3.8967956602573395\n",
      "  batch 177 loss: 3.896848328411579\n",
      "  batch 178 loss: 3.8966770321130753\n",
      "  batch 179 loss: 3.896994426846504\n",
      "  batch 180 loss: 3.89700598269701\n",
      "  batch 181 loss: 3.8967543691396713\n",
      "  batch 182 loss: 3.897082142531872\n",
      "  batch 183 loss: 3.8971076160669327\n",
      "  batch 184 loss: 3.8970983624458313\n",
      "  batch 185 loss: 3.8970883935689926\n",
      "  batch 186 loss: 3.8965815752744675\n",
      "  batch 187 loss: 3.897548131644726\n",
      "  batch 188 loss: 3.8970016464591026\n",
      "  batch 189 loss: 3.8968198001384735\n",
      "  batch 190 loss: 3.897133357822895\n",
      "  batch 191 loss: 3.8967996910214424\n",
      "  batch 192 loss: 3.8968356400728226\n",
      "  batch 193 loss: 3.896868906915188\n",
      "  batch 194 loss: 3.8969068825244904\n",
      "  batch 195 loss: 3.8967217952013016\n",
      "  batch 196 loss: 3.8967369347810745\n",
      "  batch 197 loss: 3.8967145681381226\n",
      "  batch 198 loss: 3.8967067822813988\n",
      "  batch 199 loss: 3.8967271745204926\n",
      "  batch 200 loss: 3.8970076218247414\n",
      "  batch 201 loss: 3.8971781134605408\n",
      "  batch 202 loss: 3.8972242176532745\n",
      "  batch 203 loss: 3.8966973945498466\n",
      "  batch 204 loss: 3.896623879671097\n",
      "  batch 205 loss: 3.896548815071583\n",
      "  batch 206 loss: 3.896568685770035\n",
      "  batch 207 loss: 3.896644189953804\n",
      "  batch 208 loss: 3.8966730386018753\n",
      "  batch 209 loss: 3.8971504494547844\n",
      "  batch 210 loss: 3.8965989276766777\n",
      "  batch 211 loss: 3.8970878049731255\n",
      "  batch 212 loss: 3.8967597484588623\n",
      "  batch 213 loss: 3.896793730556965\n",
      "  batch 214 loss: 3.8972141817212105\n",
      "  batch 215 loss: 3.8976265862584114\n",
      "  batch 216 loss: 3.8965588435530663\n",
      "  batch 217 loss: 3.8970010578632355\n",
      "  batch 218 loss: 3.897117495536804\n",
      "  batch 219 loss: 3.8970833122730255\n",
      "  batch 220 loss: 3.8967461064457893\n",
      "  batch 221 loss: 3.8969320729374886\n",
      "  batch 222 loss: 3.8969649598002434\n",
      "  batch 223 loss: 3.8969229087233543\n",
      "  batch 224 loss: 3.8966060280799866\n",
      "  batch 225 loss: 3.8965905755758286\n",
      "  batch 226 loss: 3.8968110606074333\n",
      "  batch 227 loss: 3.8968221843242645\n",
      "  batch 228 loss: 3.8967936858534813\n",
      "  batch 229 loss: 3.896878555417061\n",
      "  batch 230 loss: 3.896513670682907\n",
      "  batch 231 loss: 3.8970500454306602\n",
      "  batch 232 loss: 3.896916501224041\n",
      "  batch 233 loss: 3.8965549916028976\n",
      "  batch 234 loss: 3.8967839926481247\n",
      "  batch 235 loss: 3.8968288600444794\n",
      "  batch 236 loss: 3.89717610925436\n",
      "  batch 237 loss: 3.896868348121643\n",
      "  batch 238 loss: 3.89694993942976\n",
      "  batch 239 loss: 3.896864391863346\n",
      "  batch 240 loss: 3.8968834429979324\n",
      "  batch 241 loss: 3.896584376692772\n",
      "  batch 242 loss: 3.8966670259833336\n",
      "  batch 243 loss: 3.8968440294265747\n",
      "  batch 244 loss: 3.896695040166378\n",
      "  batch 245 loss: 3.896763652563095\n",
      "  batch 246 loss: 3.8968356922268867\n",
      "  batch 247 loss: 3.896741069853306\n",
      "  batch 248 loss: 3.8973195552825928\n",
      "  batch 249 loss: 3.8973230347037315\n",
      "  batch 250 loss: 3.896937347948551\n",
      "  batch 251 loss: 3.896892860531807\n",
      "  batch 252 loss: 3.8968527391552925\n",
      "  batch 253 loss: 3.8967882469296455\n",
      "  batch 254 loss: 3.896884001791477\n",
      "  batch 255 loss: 3.8968062922358513\n",
      "  batch 256 loss: 3.8966336771845818\n",
      "  batch 257 loss: 3.8964770436286926\n",
      "  batch 258 loss: 3.8969143480062485\n",
      "  batch 259 loss: 3.8967415913939476\n",
      "  batch 260 loss: 3.896541714668274\n",
      "  batch 261 loss: 3.896665245294571\n",
      "  batch 262 loss: 3.896728277206421\n",
      "  batch 263 loss: 3.896820232272148\n",
      "  batch 264 loss: 3.8965717628598213\n",
      "  batch 265 loss: 3.8968876898288727\n",
      "  batch 266 loss: 3.896945536136627\n",
      "  batch 267 loss: 3.8968900218605995\n",
      "  batch 268 loss: 3.8970850482583046\n",
      "  batch 269 loss: 3.897732339799404\n",
      "  batch 270 loss: 3.8969783633947372\n",
      "  batch 271 loss: 3.8966983929276466\n",
      "  batch 272 loss: 3.8965823650360107\n",
      "  batch 273 loss: 3.896911531686783\n",
      "  batch 274 loss: 3.8968456760048866\n",
      "  batch 275 loss: 3.8968652337789536\n",
      "  batch 276 loss: 3.8966433256864548\n",
      "  batch 277 loss: 3.8965591341257095\n",
      "  batch 278 loss: 3.8967692106962204\n",
      "  batch 279 loss: 3.897319108247757\n",
      "  batch 280 loss: 3.8967233449220657\n",
      "  batch 281 loss: 3.89680428057909\n",
      "  batch 282 loss: 3.8982012942433357\n",
      "  batch 283 loss: 3.8968301117420197\n",
      "  batch 284 loss: 3.8967513889074326\n",
      "  batch 285 loss: 3.8966560661792755\n",
      "  batch 286 loss: 3.8970978260040283\n",
      "  batch 287 loss: 3.896968588232994\n",
      "  batch 288 loss: 3.896699756383896\n",
      "  batch 289 loss: 3.8966924473643303\n",
      "  batch 290 loss: 3.896463468670845\n",
      "  batch 291 loss: 3.8966992646455765\n",
      "  batch 292 loss: 3.8973258584737778\n",
      "  batch 293 loss: 3.896866537630558\n",
      "  batch 294 loss: 3.8966930955648422\n",
      "  batch 295 loss: 3.8965159729123116\n",
      "  batch 296 loss: 3.897193171083927\n",
      "  batch 297 loss: 3.896993674337864\n",
      "  batch 298 loss: 3.896575503051281\n",
      "  batch 299 loss: 3.896560348570347\n",
      "  batch 300 loss: 3.8968492820858955\n",
      "  batch 301 loss: 3.8968572542071342\n",
      "  batch 302 loss: 3.8965506851673126\n",
      "  batch 303 loss: 3.8969767466187477\n",
      "  batch 304 loss: 3.8967436999082565\n",
      "  batch 305 loss: 3.8966445475816727\n",
      "  batch 306 loss: 3.896561585366726\n",
      "  batch 307 loss: 3.8965462148189545\n",
      "  batch 308 loss: 3.8966813534498215\n",
      "  batch 309 loss: 3.8967091366648674\n",
      "  batch 310 loss: 3.8970576971769333\n",
      "  batch 311 loss: 3.8967362493276596\n",
      "  batch 312 loss: 3.89657711237669\n",
      "  batch 313 loss: 3.896503783762455\n",
      "  batch 314 loss: 3.8967802971601486\n",
      "  batch 315 loss: 3.8967670649290085\n",
      "  batch 316 loss: 3.8964329212903976\n",
      "  batch 317 loss: 3.8967875987291336\n",
      "  batch 318 loss: 3.897097520530224\n",
      "  batch 319 loss: 3.8966692239046097\n",
      "  batch 320 loss: 3.8967745900154114\n",
      "  batch 321 loss: 3.896721452474594\n",
      "  batch 322 loss: 3.8966577872633934\n",
      "  batch 323 loss: 3.8967717438936234\n",
      "  batch 324 loss: 3.89661318808794\n",
      "  batch 325 loss: 3.896569050848484\n",
      "  batch 326 loss: 3.896472878754139\n",
      "  batch 327 loss: 3.896514415740967\n",
      "  batch 328 loss: 3.8966949582099915\n",
      "  batch 329 loss: 3.897202730178833\n",
      "  batch 330 loss: 3.896468237042427\n",
      "  batch 331 loss: 3.896588534116745\n",
      "  batch 332 loss: 3.8964689821004868\n",
      "  batch 333 loss: 3.8968394473195076\n",
      "  batch 334 loss: 3.8966869115829468\n",
      "  batch 335 loss: 3.8968713581562042\n",
      "  batch 336 loss: 3.8967277631163597\n",
      "  batch 337 loss: 3.896573022007942\n",
      "  batch 338 loss: 3.896701231598854\n",
      "  batch 339 loss: 3.896800845861435\n",
      "  batch 340 loss: 3.8968518376350403\n",
      "  batch 341 loss: 3.8965676575899124\n",
      "  batch 342 loss: 3.8965592309832573\n",
      "  batch 343 loss: 3.897049382328987\n",
      "  batch 344 loss: 3.897308401763439\n",
      "  batch 345 loss: 3.8969255536794662\n",
      "  batch 346 loss: 3.896578572690487\n",
      "  batch 347 loss: 3.8963791355490685\n",
      "  batch 348 loss: 3.8966362327337265\n",
      "  batch 349 loss: 3.8966337889432907\n",
      "  batch 350 loss: 3.896597221493721\n",
      "  batch 351 loss: 3.8964639231562614\n",
      "  batch 352 loss: 3.8964900001883507\n",
      "  batch 353 loss: 3.89679616689682\n",
      "  batch 354 loss: 3.896578788757324\n",
      "  batch 355 loss: 3.8969591483473778\n",
      "  batch 356 loss: 3.896456263959408\n",
      "  batch 357 loss: 3.897231787443161\n",
      "  batch 358 loss: 3.8964387625455856\n",
      "  batch 359 loss: 3.8967637941241264\n",
      "  batch 360 loss: 3.8966028690338135\n",
      "  batch 361 loss: 3.8969406336545944\n",
      "  batch 362 loss: 3.896579548716545\n",
      "  batch 363 loss: 3.896935023367405\n",
      "  batch 364 loss: 3.896596945822239\n",
      "  batch 365 loss: 3.896686740219593\n",
      "  batch 366 loss: 3.8968782797455788\n",
      "  batch 367 loss: 3.8966292440891266\n",
      "  batch 368 loss: 3.8967112228274345\n",
      "  batch 369 loss: 3.8971550688147545\n",
      "  batch 370 loss: 3.896808259189129\n",
      "  batch 371 loss: 3.896757237613201\n",
      "  batch 372 loss: 3.8968439251184464\n",
      "  batch 373 loss: 3.896668516099453\n",
      "  batch 374 loss: 3.8966046422719955\n",
      "  batch 375 loss: 3.8967058956623077\n",
      "  batch 376 loss: 3.896918646991253\n",
      "  batch 377 loss: 3.8964190036058426\n",
      "  batch 378 loss: 3.8965158239006996\n",
      "  batch 379 loss: 3.8966952189803123\n",
      "  batch 380 loss: 3.8965252563357353\n",
      "  batch 381 loss: 3.8966031298041344\n",
      "  batch 382 loss: 3.8966096490621567\n",
      "  batch 383 loss: 3.896637052297592\n",
      "  batch 384 loss: 3.8964435681700706\n",
      "  batch 385 loss: 3.8967297673225403\n",
      "  batch 386 loss: 3.8965092077851295\n",
      "  batch 387 loss: 3.89640524238348\n",
      "  batch 388 loss: 3.896681569516659\n",
      "  batch 389 loss: 3.896550014615059\n",
      "  batch 390 loss: 3.896598741412163\n",
      "  batch 391 loss: 3.896572172641754\n",
      "  batch 392 loss: 3.896677665412426\n",
      "  batch 393 loss: 3.8966235145926476\n",
      "  batch 394 loss: 3.8965820372104645\n",
      "  batch 395 loss: 3.896556906402111\n",
      "  batch 396 loss: 3.896653637290001\n",
      "  batch 397 loss: 3.89649198949337\n",
      "  batch 398 loss: 3.8965561389923096\n",
      "  batch 399 loss: 3.8969006463885307\n",
      "  batch 400 loss: 3.896470881998539\n",
      "  batch 401 loss: 3.8964381366968155\n",
      "  batch 402 loss: 3.8966961950063705\n",
      "  batch 403 loss: 3.89665125310421\n",
      "  batch 404 loss: 3.896463617682457\n",
      "  batch 405 loss: 3.8965844362974167\n",
      "  batch 406 loss: 3.896461360156536\n",
      "  batch 407 loss: 3.896617293357849\n",
      "  batch 408 loss: 3.896658308804035\n",
      "  batch 409 loss: 3.8965025544166565\n",
      "  batch 410 loss: 3.8965211510658264\n",
      "  batch 411 loss: 3.8970336616039276\n",
      "  batch 412 loss: 3.898096427321434\n",
      "  batch 413 loss: 3.8966163769364357\n",
      "  batch 414 loss: 3.8964396715164185\n",
      "  batch 415 loss: 3.896410584449768\n",
      "  batch 416 loss: 3.896452486515045\n",
      "  batch 417 loss: 3.8964001908898354\n",
      "  batch 418 loss: 3.8963869214057922\n",
      "  batch 419 loss: 3.897162489593029\n",
      "  batch 420 loss: 3.8967857137322426\n",
      "  batch 421 loss: 3.8966802209615707\n",
      "  batch 422 loss: 3.896592617034912\n",
      "  batch 423 loss: 3.896645687520504\n",
      "  batch 424 loss: 3.89653392881155\n",
      "LOSS train 3.89653392881155 valid 3.896259419901191\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.896415375173092\n",
      "  batch 2 loss: 3.8972642049193382\n",
      "  batch 3 loss: 3.8966521248221397\n",
      "  batch 4 loss: 3.89641135931015\n",
      "  batch 5 loss: 3.8966106548905373\n",
      "  batch 6 loss: 3.8965655490756035\n",
      "  batch 7 loss: 3.896909885108471\n",
      "  batch 8 loss: 3.8965167328715324\n",
      "  batch 9 loss: 3.8969183936715126\n",
      "  batch 10 loss: 3.8965230211615562\n",
      "  batch 11 loss: 3.896375834941864\n",
      "  batch 12 loss: 3.8969056606292725\n",
      "  batch 13 loss: 3.8965892642736435\n",
      "  batch 14 loss: 3.8964973837137222\n",
      "  batch 15 loss: 3.896678753197193\n",
      "  batch 16 loss: 3.8966654911637306\n",
      "  batch 17 loss: 3.896418869495392\n",
      "  batch 18 loss: 3.896677426993847\n",
      "  batch 19 loss: 3.89645054936409\n",
      "  batch 20 loss: 3.8966266959905624\n",
      "  batch 21 loss: 3.896888554096222\n",
      "  batch 22 loss: 3.896703191101551\n",
      "  batch 23 loss: 3.896499127149582\n",
      "  batch 24 loss: 3.896649017930031\n",
      "  batch 25 loss: 3.8967489302158356\n",
      "  batch 26 loss: 3.8965879008173943\n",
      "  batch 27 loss: 3.896634116768837\n",
      "  batch 28 loss: 3.8965714499354362\n",
      "  batch 29 loss: 3.8966851755976677\n",
      "  batch 30 loss: 3.896413616836071\n",
      "  batch 31 loss: 3.8965466395020485\n",
      "  batch 32 loss: 3.8965424969792366\n",
      "  batch 33 loss: 3.8965441584587097\n",
      "  batch 34 loss: 3.8970982655882835\n",
      "  batch 35 loss: 3.8965582102537155\n",
      "  batch 36 loss: 3.89638002961874\n",
      "  batch 37 loss: 3.8965706825256348\n",
      "  batch 38 loss: 3.8965416327118874\n",
      "  batch 39 loss: 3.8964942768216133\n",
      "  batch 40 loss: 3.896590992808342\n",
      "  batch 41 loss: 3.8965398222208023\n",
      "  batch 42 loss: 3.896638922393322\n",
      "  batch 43 loss: 3.8969212621450424\n",
      "  batch 44 loss: 3.896594427525997\n",
      "  batch 45 loss: 3.8963961750268936\n",
      "  batch 46 loss: 3.8964508771896362\n",
      "  batch 47 loss: 3.89665100723505\n",
      "  batch 48 loss: 3.8965417593717575\n",
      "  batch 49 loss: 3.8965621292591095\n",
      "  batch 50 loss: 3.89646727591753\n",
      "  batch 51 loss: 3.896718330681324\n",
      "  batch 52 loss: 3.896466441452503\n",
      "  batch 53 loss: 3.8963724076747894\n",
      "  batch 54 loss: 3.8966637924313545\n",
      "  batch 55 loss: 3.896476946771145\n",
      "  batch 56 loss: 3.8969643265008926\n",
      "  batch 57 loss: 3.896541103720665\n",
      "  batch 58 loss: 3.8963568806648254\n",
      "  batch 59 loss: 3.8965659588575363\n",
      "  batch 60 loss: 3.8968025371432304\n",
      "  batch 61 loss: 3.896691270172596\n",
      "  batch 62 loss: 3.8964136987924576\n",
      "  batch 63 loss: 3.896526999771595\n",
      "  batch 64 loss: 3.8966683819890022\n",
      "  batch 65 loss: 3.8967029750347137\n",
      "  batch 66 loss: 3.896385170519352\n",
      "  batch 67 loss: 3.896520435810089\n",
      "  batch 68 loss: 3.8967250511050224\n",
      "  batch 69 loss: 3.8966400399804115\n",
      "  batch 70 loss: 3.8964656442403793\n",
      "  batch 71 loss: 3.8964221104979515\n",
      "  batch 72 loss: 3.896372340619564\n",
      "  batch 73 loss: 3.8966619968414307\n",
      "  batch 74 loss: 3.8965157568454742\n",
      "  batch 75 loss: 3.896497182548046\n",
      "  batch 76 loss: 3.8962806165218353\n",
      "  batch 77 loss: 3.896443985402584\n",
      "  batch 78 loss: 3.896428421139717\n",
      "  batch 79 loss: 3.8963274136185646\n",
      "  batch 80 loss: 3.8967210277915\n",
      "  batch 81 loss: 3.8964163064956665\n",
      "  batch 82 loss: 3.8966993540525436\n",
      "  batch 83 loss: 3.8967518284916878\n",
      "  batch 84 loss: 3.896605685353279\n",
      "  batch 85 loss: 3.896383799612522\n",
      "  batch 86 loss: 3.8965414986014366\n",
      "  batch 87 loss: 3.8964741304516792\n",
      "  batch 88 loss: 3.8967465311288834\n",
      "  batch 89 loss: 3.896670922636986\n",
      "  batch 90 loss: 3.8963440507650375\n",
      "  batch 91 loss: 3.8965921849012375\n",
      "  batch 92 loss: 3.896338365972042\n",
      "  batch 93 loss: 3.8965535536408424\n",
      "  batch 94 loss: 3.8966108560562134\n",
      "  batch 95 loss: 3.8966427221894264\n",
      "  batch 96 loss: 3.896388739347458\n",
      "  batch 97 loss: 3.8965573012828827\n",
      "  batch 98 loss: 3.896598994731903\n",
      "  batch 99 loss: 3.8965432047843933\n",
      "  batch 100 loss: 3.8963796347379684\n",
      "  batch 101 loss: 3.8965287506580353\n",
      "  batch 102 loss: 3.896459236741066\n",
      "  batch 103 loss: 3.896685376763344\n",
      "  batch 104 loss: 3.897089295089245\n",
      "  batch 105 loss: 3.8965853080153465\n",
      "  batch 106 loss: 3.8966317400336266\n",
      "  batch 107 loss: 3.8963767290115356\n",
      "  batch 108 loss: 3.8964889720082283\n",
      "  batch 109 loss: 3.8965444937348366\n",
      "  batch 110 loss: 3.8970932289958\n",
      "  batch 111 loss: 3.896366707980633\n",
      "  batch 112 loss: 3.8963062912225723\n",
      "  batch 113 loss: 3.896648570895195\n",
      "  batch 114 loss: 3.896313078701496\n",
      "  batch 115 loss: 3.8964569568634033\n",
      "  batch 116 loss: 3.896608293056488\n",
      "  batch 117 loss: 3.89641971886158\n",
      "  batch 118 loss: 3.8967142552137375\n",
      "  batch 119 loss: 3.8964512199163437\n",
      "  batch 120 loss: 3.896509513258934\n",
      "  batch 121 loss: 3.8964205607771873\n",
      "  batch 122 loss: 3.8966002017259598\n",
      "  batch 123 loss: 3.896346852183342\n",
      "  batch 124 loss: 3.8963411822915077\n",
      "  batch 125 loss: 3.896316073834896\n",
      "  batch 126 loss: 3.896477408707142\n",
      "  batch 127 loss: 3.896373823285103\n",
      "  batch 128 loss: 3.8970851451158524\n",
      "  batch 129 loss: 3.896505817770958\n",
      "  batch 130 loss: 3.8964803218841553\n",
      "  batch 131 loss: 3.8964780420064926\n",
      "  batch 132 loss: 3.896406300365925\n",
      "  batch 133 loss: 3.896648645401001\n",
      "  batch 134 loss: 3.896243616938591\n",
      "  batch 135 loss: 3.8968324586749077\n",
      "  batch 136 loss: 3.8966140300035477\n",
      "  batch 137 loss: 3.8966552764177322\n",
      "  batch 138 loss: 3.896414138376713\n",
      "  batch 139 loss: 3.8965079709887505\n",
      "  batch 140 loss: 3.8963700383901596\n",
      "  batch 141 loss: 3.8965966552495956\n",
      "  batch 142 loss: 3.8970258235931396\n",
      "  batch 143 loss: 3.89681725949049\n",
      "  batch 144 loss: 3.896704137325287\n",
      "  batch 145 loss: 3.8963520601391792\n",
      "  batch 146 loss: 3.896910682320595\n",
      "  batch 147 loss: 3.896488204598427\n",
      "  batch 148 loss: 3.896238699555397\n",
      "  batch 149 loss: 3.896271161735058\n",
      "  batch 150 loss: 3.8964544013142586\n",
      "  batch 151 loss: 3.8963769748806953\n",
      "  batch 152 loss: 3.896527111530304\n",
      "  batch 153 loss: 3.896285004913807\n",
      "  batch 154 loss: 3.8963557332754135\n",
      "  batch 155 loss: 3.896539032459259\n",
      "  batch 156 loss: 3.896333761513233\n",
      "  batch 157 loss: 3.896397165954113\n",
      "  batch 158 loss: 3.896962895989418\n",
      "  batch 159 loss: 3.8964712619781494\n",
      "  batch 160 loss: 3.8963071554899216\n",
      "  batch 161 loss: 3.8965449929237366\n",
      "  batch 162 loss: 3.8964851275086403\n",
      "  batch 163 loss: 3.896701365709305\n",
      "  batch 164 loss: 3.8963432759046555\n",
      "  batch 165 loss: 3.896679997444153\n",
      "  batch 166 loss: 3.896576352417469\n",
      "  batch 167 loss: 3.896252989768982\n",
      "  batch 168 loss: 3.896419756114483\n",
      "  batch 169 loss: 3.89699724316597\n",
      "  batch 170 loss: 3.896661937236786\n",
      "  batch 171 loss: 3.8964128121733665\n",
      "  batch 172 loss: 3.8962657675147057\n",
      "  batch 173 loss: 3.896368019282818\n",
      "  batch 174 loss: 3.896557465195656\n",
      "  batch 175 loss: 3.8964504674077034\n",
      "  batch 176 loss: 3.8965368419885635\n",
      "  batch 177 loss: 3.89626681804657\n",
      "  batch 178 loss: 3.896377243101597\n",
      "  batch 179 loss: 3.896546021103859\n",
      "  batch 180 loss: 3.896392099559307\n",
      "  batch 181 loss: 3.8965284675359726\n",
      "  batch 182 loss: 3.8964868411421776\n",
      "  batch 183 loss: 3.896377608180046\n",
      "  batch 184 loss: 3.8966151103377342\n",
      "  batch 185 loss: 3.896552361547947\n",
      "  batch 186 loss: 3.896384283900261\n",
      "  batch 187 loss: 3.8964852318167686\n",
      "  batch 188 loss: 3.8967354968190193\n",
      "  batch 189 loss: 3.8965282291173935\n",
      "  batch 190 loss: 3.896431691944599\n",
      "  batch 191 loss: 3.8963781520724297\n",
      "  batch 192 loss: 3.896289214491844\n",
      "  batch 193 loss: 3.8963405713438988\n",
      "  batch 194 loss: 3.896500341594219\n",
      "  batch 195 loss: 3.896447405219078\n",
      "  batch 196 loss: 3.8965934664011\n",
      "  batch 197 loss: 3.8964657932519913\n",
      "  batch 198 loss: 3.8964602649211884\n",
      "  batch 199 loss: 3.896586239337921\n",
      "  batch 200 loss: 3.896316647529602\n",
      "  batch 201 loss: 3.8964891582727432\n",
      "  batch 202 loss: 3.8964471593499184\n",
      "  batch 203 loss: 3.8964953571558\n",
      "  batch 204 loss: 3.8963721245527267\n",
      "  batch 205 loss: 3.8963565453886986\n",
      "  batch 206 loss: 3.896896243095398\n",
      "  batch 207 loss: 3.8963463082909584\n",
      "  batch 208 loss: 3.8962947726249695\n",
      "  batch 209 loss: 3.8963577523827553\n",
      "  batch 210 loss: 3.8964851945638657\n",
      "  batch 211 loss: 3.8965803757309914\n",
      "  batch 212 loss: 3.896374210715294\n",
      "  batch 213 loss: 3.896534577012062\n",
      "  batch 214 loss: 3.8967011123895645\n",
      "  batch 215 loss: 3.8962094634771347\n",
      "  batch 216 loss: 3.896415762603283\n",
      "  batch 217 loss: 3.8964318335056305\n",
      "  batch 218 loss: 3.8962956815958023\n",
      "  batch 219 loss: 3.8963834792375565\n",
      "  batch 220 loss: 3.8964063227176666\n",
      "  batch 221 loss: 3.896368682384491\n",
      "  batch 222 loss: 3.896409772336483\n",
      "  batch 223 loss: 3.896583117544651\n",
      "  batch 224 loss: 3.8964604064822197\n",
      "  batch 225 loss: 3.8964832350611687\n",
      "  batch 226 loss: 3.8964961171150208\n",
      "  batch 227 loss: 3.8962457105517387\n",
      "  batch 228 loss: 3.896513134241104\n",
      "  batch 229 loss: 3.8962714225053787\n",
      "  batch 230 loss: 3.8964498713612556\n",
      "  batch 231 loss: 3.8963059782981873\n",
      "  batch 232 loss: 3.896653436124325\n",
      "  batch 233 loss: 3.896322324872017\n",
      "  batch 234 loss: 3.8964713141322136\n",
      "  batch 235 loss: 3.8962655290961266\n",
      "  batch 236 loss: 3.8965648263692856\n",
      "  batch 237 loss: 3.8963490650057793\n",
      "  batch 238 loss: 3.896327793598175\n",
      "  batch 239 loss: 3.8964812383055687\n",
      "  batch 240 loss: 3.896424822509289\n",
      "  batch 241 loss: 3.8962741792201996\n",
      "  batch 242 loss: 3.8963130339980125\n",
      "  batch 243 loss: 3.8962495252490044\n",
      "  batch 244 loss: 3.896463356912136\n",
      "  batch 245 loss: 3.896464876830578\n",
      "  batch 246 loss: 3.8964376896619797\n",
      "  batch 247 loss: 3.8967100977897644\n",
      "  batch 248 loss: 3.8970722258090973\n",
      "  batch 249 loss: 3.896360822021961\n",
      "  batch 250 loss: 3.896626852452755\n",
      "  batch 251 loss: 3.89660457521677\n",
      "  batch 252 loss: 3.8963672071695328\n",
      "  batch 253 loss: 3.8964207991957664\n",
      "  batch 254 loss: 3.8962202593684196\n",
      "  batch 255 loss: 3.896336555480957\n",
      "  batch 256 loss: 3.8965117558836937\n",
      "  batch 257 loss: 3.896572470664978\n",
      "  batch 258 loss: 3.8962580636143684\n",
      "  batch 259 loss: 3.8961997628211975\n",
      "  batch 260 loss: 3.8962181210517883\n",
      "  batch 261 loss: 3.8968488574028015\n",
      "  batch 262 loss: 3.8962848260998726\n",
      "  batch 263 loss: 3.8962169885635376\n",
      "  batch 264 loss: 3.896359346807003\n",
      "  batch 265 loss: 3.8963798731565475\n",
      "  batch 266 loss: 3.896371118724346\n",
      "  batch 267 loss: 3.8964094072580338\n",
      "  batch 268 loss: 3.896429166197777\n",
      "  batch 269 loss: 3.8964216113090515\n",
      "  batch 270 loss: 3.8965372666716576\n",
      "  batch 271 loss: 3.896393470466137\n",
      "  batch 272 loss: 3.8963361755013466\n",
      "  batch 273 loss: 3.896471783518791\n",
      "  batch 274 loss: 3.896307624876499\n",
      "  batch 275 loss: 3.896288141608238\n",
      "  batch 276 loss: 3.8963358625769615\n",
      "  batch 277 loss: 3.8962769731879234\n",
      "  batch 278 loss: 3.8966948986053467\n",
      "  batch 279 loss: 3.8963692635297775\n",
      "  batch 280 loss: 3.8963631242513657\n",
      "  batch 281 loss: 3.89631187915802\n",
      "  batch 282 loss: 3.8963575959205627\n",
      "  batch 283 loss: 3.8972969874739647\n",
      "  batch 284 loss: 3.896356701850891\n",
      "  batch 285 loss: 3.896373249590397\n",
      "  batch 286 loss: 3.8963820710778236\n",
      "  batch 287 loss: 3.896469347178936\n",
      "  batch 288 loss: 3.896236427128315\n",
      "  batch 289 loss: 3.8961925506591797\n",
      "  batch 290 loss: 3.896573320031166\n",
      "  batch 291 loss: 3.8963002637028694\n",
      "  batch 292 loss: 3.8979778811335564\n",
      "  batch 293 loss: 3.8965231627225876\n",
      "  batch 294 loss: 3.8963867127895355\n",
      "  batch 295 loss: 3.8962175846099854\n",
      "  batch 296 loss: 3.8962547108531\n",
      "  batch 297 loss: 3.8962428346276283\n",
      "  batch 298 loss: 3.896329417824745\n",
      "  batch 299 loss: 3.8962884545326233\n",
      "  batch 300 loss: 3.8963956460356712\n",
      "  batch 301 loss: 3.896243952214718\n",
      "  batch 302 loss: 3.896372102200985\n",
      "  batch 303 loss: 3.896973542869091\n",
      "  batch 304 loss: 3.8962904885411263\n",
      "  batch 305 loss: 3.8964276015758514\n",
      "  batch 306 loss: 3.896389827132225\n",
      "  batch 307 loss: 3.896149218082428\n",
      "  batch 308 loss: 3.8963403925299644\n",
      "  batch 309 loss: 3.8964079916477203\n",
      "  batch 310 loss: 3.896509923040867\n",
      "  batch 311 loss: 3.896338552236557\n",
      "  batch 312 loss: 3.8962182477116585\n",
      "  batch 313 loss: 3.8963787108659744\n",
      "  batch 314 loss: 3.8965393230319023\n",
      "  batch 315 loss: 3.8964447677135468\n",
      "  batch 316 loss: 3.896403454244137\n",
      "  batch 317 loss: 3.8963126465678215\n",
      "  batch 318 loss: 3.8962825015187263\n",
      "  batch 319 loss: 3.897182174026966\n",
      "  batch 320 loss: 3.89634208381176\n",
      "  batch 321 loss: 3.8966167345643044\n",
      "  batch 322 loss: 3.8963270485401154\n",
      "  batch 323 loss: 3.896400459110737\n",
      "  batch 324 loss: 3.8964374661445618\n",
      "  batch 325 loss: 3.8961657136678696\n",
      "  batch 326 loss: 3.8963580057024956\n",
      "  batch 327 loss: 3.8962518870830536\n",
      "  batch 328 loss: 3.896360360085964\n",
      "  batch 329 loss: 3.896242581307888\n",
      "  batch 330 loss: 3.8962587267160416\n",
      "  batch 331 loss: 3.89630126953125\n",
      "  batch 332 loss: 3.896418660879135\n",
      "  batch 333 loss: 3.8962616845965385\n",
      "  batch 334 loss: 3.896263226866722\n",
      "  batch 335 loss: 3.896193243563175\n",
      "  batch 336 loss: 3.897068828344345\n",
      "  batch 337 loss: 3.896511673927307\n",
      "  batch 338 loss: 3.896586149930954\n",
      "  batch 339 loss: 3.8964992240071297\n",
      "  batch 340 loss: 3.896335281431675\n",
      "  batch 341 loss: 3.8962734416127205\n",
      "  batch 342 loss: 3.8962979465723038\n",
      "  batch 343 loss: 3.896323673427105\n",
      "  batch 344 loss: 3.896293096244335\n",
      "  batch 345 loss: 3.896365776658058\n",
      "  batch 346 loss: 3.8964215517044067\n",
      "  batch 347 loss: 3.8967228531837463\n",
      "  batch 348 loss: 3.8964404240250587\n",
      "  batch 349 loss: 3.8963448256254196\n",
      "  batch 350 loss: 3.896333858370781\n",
      "  batch 351 loss: 3.896298035979271\n",
      "  batch 352 loss: 3.8963946625590324\n",
      "  batch 353 loss: 3.896353952586651\n",
      "  batch 354 loss: 3.8964485079050064\n",
      "  batch 355 loss: 3.896537646651268\n",
      "  batch 356 loss: 3.8965654149651527\n",
      "  batch 357 loss: 3.896440528333187\n",
      "  batch 358 loss: 3.89617982506752\n",
      "  batch 359 loss: 3.89636293053627\n",
      "  batch 360 loss: 3.8963702023029327\n",
      "  batch 361 loss: 3.89612103253603\n",
      "  batch 362 loss: 3.8963595554232597\n",
      "  batch 363 loss: 3.8964204639196396\n",
      "  batch 364 loss: 3.8963020890951157\n",
      "  batch 365 loss: 3.896644189953804\n",
      "  batch 366 loss: 3.896236941218376\n",
      "  batch 367 loss: 3.896357052028179\n",
      "  batch 368 loss: 3.8963705375790596\n",
      "  batch 369 loss: 3.8963007032871246\n",
      "  batch 370 loss: 3.8962435871362686\n",
      "  batch 371 loss: 3.8962229415774345\n",
      "  batch 372 loss: 3.8963088169693947\n",
      "  batch 373 loss: 3.896329641342163\n",
      "  batch 374 loss: 3.896535262465477\n",
      "  batch 375 loss: 3.8961482867598534\n",
      "  batch 376 loss: 3.896255701780319\n",
      "  batch 377 loss: 3.896371766924858\n",
      "  batch 378 loss: 3.8962348997592926\n",
      "  batch 379 loss: 3.896391950547695\n",
      "  batch 380 loss: 3.896421991288662\n",
      "  batch 381 loss: 3.896322712302208\n",
      "  batch 382 loss: 3.896563231945038\n",
      "  batch 383 loss: 3.8962362483143806\n",
      "  batch 384 loss: 3.896469362080097\n",
      "  batch 385 loss: 3.8964794501662254\n",
      "  batch 386 loss: 3.8965661451220512\n",
      "  batch 387 loss: 3.896280638873577\n",
      "  batch 388 loss: 3.8962671756744385\n",
      "  batch 389 loss: 3.8961272090673447\n",
      "  batch 390 loss: 3.896165631711483\n",
      "  batch 391 loss: 3.896223723888397\n",
      "  batch 392 loss: 3.896312788128853\n",
      "  batch 393 loss: 3.8962307050824165\n",
      "  batch 394 loss: 3.896363489329815\n",
      "  batch 395 loss: 3.8962200582027435\n",
      "  batch 396 loss: 3.896374858915806\n",
      "  batch 397 loss: 3.8962177112698555\n",
      "  batch 398 loss: 3.8965102657675743\n",
      "  batch 399 loss: 3.896349824965\n",
      "  batch 400 loss: 3.8962832018733025\n",
      "  batch 401 loss: 3.8962831795215607\n",
      "  batch 402 loss: 3.8962336629629135\n",
      "  batch 403 loss: 3.896220050752163\n",
      "  batch 404 loss: 3.89621452242136\n",
      "  batch 405 loss: 3.8962169364094734\n",
      "  batch 406 loss: 3.896397866308689\n",
      "  batch 407 loss: 3.896435797214508\n",
      "  batch 408 loss: 3.89665699750185\n",
      "  batch 409 loss: 3.896346054971218\n",
      "  batch 410 loss: 3.89666298776865\n",
      "  batch 411 loss: 3.8962342739105225\n",
      "  batch 412 loss: 3.896536245942116\n",
      "  batch 413 loss: 3.8962118476629257\n",
      "  batch 414 loss: 3.896221235394478\n",
      "  batch 415 loss: 3.8961765989661217\n",
      "  batch 416 loss: 3.896260991692543\n",
      "  batch 417 loss: 3.89631899446249\n",
      "  batch 418 loss: 3.896218292415142\n",
      "  batch 419 loss: 3.896201215684414\n",
      "  batch 420 loss: 3.896463595330715\n",
      "  batch 421 loss: 3.896305799484253\n",
      "  batch 422 loss: 3.8961723893880844\n",
      "  batch 423 loss: 3.896173693239689\n",
      "  batch 424 loss: 3.8963785469532013\n",
      "LOSS train 3.8963785469532013 valid 3.8960703206863605\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.8963151574134827\n",
      "  batch 2 loss: 3.8961977139115334\n",
      "  batch 3 loss: 3.8966317027807236\n",
      "  batch 4 loss: 3.896205060184002\n",
      "  batch 5 loss: 3.896311178803444\n",
      "  batch 6 loss: 3.8965694829821587\n",
      "  batch 7 loss: 3.8962326124310493\n",
      "  batch 8 loss: 3.8962322026491165\n",
      "  batch 9 loss: 3.8962713479995728\n",
      "  batch 10 loss: 3.8964945897459984\n",
      "  batch 11 loss: 3.8962480947375298\n",
      "  batch 12 loss: 3.896256923675537\n",
      "  batch 13 loss: 3.8962628096342087\n",
      "  batch 14 loss: 3.896590143442154\n",
      "  batch 15 loss: 3.896270662546158\n",
      "  batch 16 loss: 3.8962944969534874\n",
      "  batch 17 loss: 3.8962885662913322\n",
      "  batch 18 loss: 3.8962537944316864\n",
      "  batch 19 loss: 3.8962774500250816\n",
      "  batch 20 loss: 3.8961900621652603\n",
      "  batch 21 loss: 3.896337643265724\n",
      "  batch 22 loss: 3.896217294037342\n",
      "  batch 23 loss: 3.8963515534996986\n",
      "  batch 24 loss: 3.8961628451943398\n",
      "  batch 25 loss: 3.8962295204401016\n",
      "  batch 26 loss: 3.896175689995289\n",
      "  batch 27 loss: 3.8963698148727417\n",
      "  batch 28 loss: 3.897850975394249\n",
      "  batch 29 loss: 3.896412968635559\n",
      "  batch 30 loss: 3.8962671980261803\n",
      "  batch 31 loss: 3.8962650522589684\n",
      "  batch 32 loss: 3.896224118769169\n",
      "  batch 33 loss: 3.896389201283455\n",
      "  batch 34 loss: 3.896419547498226\n",
      "  batch 35 loss: 3.896203726530075\n",
      "  batch 36 loss: 3.896078072488308\n",
      "  batch 37 loss: 3.8962797224521637\n",
      "  batch 38 loss: 3.8962255865335464\n",
      "  batch 39 loss: 3.8963443264365196\n",
      "  batch 40 loss: 3.8963077515363693\n",
      "  batch 41 loss: 3.896253891289234\n",
      "  batch 42 loss: 3.896381191909313\n",
      "  batch 43 loss: 3.896166540682316\n",
      "  batch 44 loss: 3.896698161959648\n",
      "  batch 45 loss: 3.8963492065668106\n",
      "  batch 46 loss: 3.8963746577501297\n",
      "  batch 47 loss: 3.8963460326194763\n",
      "  batch 48 loss: 3.896181143820286\n",
      "  batch 49 loss: 3.8963726311922073\n",
      "  batch 50 loss: 3.8961536213755608\n",
      "  batch 51 loss: 3.8964920341968536\n",
      "  batch 52 loss: 3.896219253540039\n",
      "  batch 53 loss: 3.896315410733223\n",
      "  batch 54 loss: 3.8962822183966637\n",
      "  batch 55 loss: 3.8963003009557724\n",
      "  batch 56 loss: 3.8963961601257324\n",
      "  batch 57 loss: 3.896500952541828\n",
      "  batch 58 loss: 3.8961302116513252\n",
      "  batch 59 loss: 3.8962538093328476\n",
      "  batch 60 loss: 3.8963054940104485\n",
      "  batch 61 loss: 3.896204188466072\n",
      "  batch 62 loss: 3.8962527215480804\n",
      "  batch 63 loss: 3.8962728157639503\n",
      "  batch 64 loss: 3.896154537796974\n",
      "  batch 65 loss: 3.896175868809223\n",
      "  batch 66 loss: 3.8961892277002335\n",
      "  batch 67 loss: 3.8962979316711426\n",
      "  batch 68 loss: 3.8961181566119194\n",
      "  batch 69 loss: 3.89617882668972\n",
      "  batch 70 loss: 3.8963260650634766\n",
      "  batch 71 loss: 3.896111398935318\n",
      "  batch 72 loss: 3.8962238505482674\n",
      "  batch 73 loss: 3.8962514474987984\n",
      "  batch 74 loss: 3.8962674662470818\n",
      "  batch 75 loss: 3.896245814859867\n",
      "  batch 76 loss: 3.8961694687604904\n",
      "  batch 77 loss: 3.896167606115341\n",
      "  batch 78 loss: 3.89618206769228\n",
      "  batch 79 loss: 3.896153151988983\n",
      "  batch 80 loss: 3.8964099809527397\n",
      "  batch 81 loss: 3.896244138479233\n",
      "  batch 82 loss: 3.8961874917149544\n",
      "  batch 83 loss: 3.896177500486374\n",
      "  batch 84 loss: 3.896347649395466\n",
      "  batch 85 loss: 3.896338574588299\n",
      "  batch 86 loss: 3.8961600065231323\n",
      "  batch 87 loss: 3.896314673125744\n",
      "  batch 88 loss: 3.896250270307064\n",
      "  batch 89 loss: 3.8963642194867134\n",
      "  batch 90 loss: 3.8962131440639496\n",
      "  batch 91 loss: 3.896274834871292\n",
      "  batch 92 loss: 3.8962464705109596\n",
      "  batch 93 loss: 3.896235316991806\n",
      "  batch 94 loss: 3.896168775856495\n",
      "  batch 95 loss: 3.8961577266454697\n",
      "  batch 96 loss: 3.896167576313019\n",
      "  batch 97 loss: 3.8963198363780975\n",
      "  batch 98 loss: 3.8961868435144424\n",
      "  batch 99 loss: 3.8962969332933426\n",
      "  batch 100 loss: 3.896444469690323\n",
      "  batch 101 loss: 3.896185226738453\n",
      "  batch 102 loss: 3.8962818682193756\n",
      "  batch 103 loss: 3.8962955102324486\n",
      "  batch 104 loss: 3.896487519145012\n",
      "  batch 105 loss: 3.896126851439476\n",
      "  batch 106 loss: 3.8962349593639374\n",
      "  batch 107 loss: 3.896494783461094\n",
      "  batch 108 loss: 3.8962255343794823\n",
      "  batch 109 loss: 3.8962715938687325\n",
      "  batch 110 loss: 3.896241970360279\n",
      "  batch 111 loss: 3.896117866039276\n",
      "  batch 112 loss: 3.896115429699421\n",
      "  batch 113 loss: 3.8962851241230965\n",
      "  batch 114 loss: 3.8963479325175285\n",
      "  batch 115 loss: 3.8962835744023323\n",
      "  batch 116 loss: 3.8961932361125946\n",
      "  batch 117 loss: 3.896288901567459\n",
      "  batch 118 loss: 3.8963231295347214\n",
      "  batch 119 loss: 3.896383911371231\n",
      "  batch 120 loss: 3.896250121295452\n",
      "  batch 121 loss: 3.8963355645537376\n",
      "  batch 122 loss: 3.896306574344635\n",
      "  batch 123 loss: 3.8963283970952034\n",
      "  batch 124 loss: 3.896393232047558\n",
      "  batch 125 loss: 3.8963335379958153\n",
      "  batch 126 loss: 3.896446816623211\n",
      "  batch 127 loss: 3.896269530057907\n",
      "  batch 128 loss: 3.8962336108088493\n",
      "  batch 129 loss: 3.896205797791481\n",
      "  batch 130 loss: 3.8969318717718124\n",
      "  batch 131 loss: 3.8960991352796555\n",
      "  batch 132 loss: 3.8962358459830284\n",
      "  batch 133 loss: 3.8961650282144547\n",
      "  batch 134 loss: 3.8962017968297005\n",
      "  batch 135 loss: 3.8963057696819305\n",
      "  batch 136 loss: 3.896121419966221\n",
      "  batch 137 loss: 3.8961759135127068\n",
      "  batch 138 loss: 3.8962121680378914\n",
      "  batch 139 loss: 3.896094888448715\n",
      "  batch 140 loss: 3.8963280469179153\n",
      "  batch 141 loss: 3.8963979110121727\n",
      "  batch 142 loss: 3.8964447006583214\n",
      "  batch 143 loss: 3.896219551563263\n",
      "  batch 144 loss: 3.8963485658168793\n",
      "  batch 145 loss: 3.8961035311222076\n",
      "  batch 146 loss: 3.8963049575686455\n",
      "  batch 147 loss: 3.896099641919136\n",
      "  batch 148 loss: 3.8961497098207474\n",
      "  batch 149 loss: 3.8962808176875114\n",
      "  batch 150 loss: 3.8961757123470306\n",
      "  batch 151 loss: 3.896311767399311\n",
      "  batch 152 loss: 3.896271161735058\n",
      "  batch 153 loss: 3.8962310776114464\n",
      "  batch 154 loss: 3.8962669521570206\n",
      "  batch 155 loss: 3.896179012954235\n",
      "  batch 156 loss: 3.896197535097599\n",
      "  batch 157 loss: 3.8961926624178886\n",
      "  batch 158 loss: 3.8962031453847885\n",
      "  batch 159 loss: 3.896269716322422\n",
      "  batch 160 loss: 3.8961844071745872\n",
      "  batch 161 loss: 3.89616908878088\n",
      "  batch 162 loss: 3.896209754049778\n",
      "  batch 163 loss: 3.8961381167173386\n",
      "  batch 164 loss: 3.8961328268051147\n",
      "  batch 165 loss: 3.8961828649044037\n",
      "  batch 166 loss: 3.8962240889668465\n",
      "  batch 167 loss: 3.8962357863783836\n",
      "  batch 168 loss: 3.896600730717182\n",
      "  batch 169 loss: 3.896150231361389\n",
      "  batch 170 loss: 3.896123491227627\n",
      "  batch 171 loss: 3.896251618862152\n",
      "  batch 172 loss: 3.8985444977879524\n",
      "  batch 173 loss: 3.896103151142597\n",
      "  batch 174 loss: 3.8961757346987724\n",
      "  batch 175 loss: 3.8965408504009247\n",
      "  batch 176 loss: 3.8962476029992104\n",
      "  batch 177 loss: 3.8961839750409126\n",
      "  batch 178 loss: 3.8962832912802696\n",
      "  batch 179 loss: 3.896249659359455\n",
      "  batch 180 loss: 3.8964632004499435\n",
      "  batch 181 loss: 3.8963538184762\n",
      "  batch 182 loss: 3.89625446498394\n",
      "  batch 183 loss: 3.896297074854374\n",
      "  batch 184 loss: 3.8961234986782074\n",
      "  batch 185 loss: 3.8961438313126564\n",
      "  batch 186 loss: 3.896065689623356\n",
      "  batch 187 loss: 3.896311543881893\n",
      "  batch 188 loss: 3.896165482699871\n",
      "  batch 189 loss: 3.8963118195533752\n",
      "  batch 190 loss: 3.89625234156847\n",
      "  batch 191 loss: 3.8967027068138123\n",
      "  batch 192 loss: 3.896171987056732\n",
      "  batch 193 loss: 3.896255947649479\n",
      "  batch 194 loss: 3.8961897790431976\n",
      "  batch 195 loss: 3.896362565457821\n",
      "  batch 196 loss: 3.8965491354465485\n",
      "  batch 197 loss: 3.896165832877159\n",
      "  batch 198 loss: 3.896266721189022\n",
      "  batch 199 loss: 3.896518163383007\n",
      "  batch 200 loss: 3.8962582647800446\n",
      "  batch 201 loss: 3.8961915895342827\n",
      "  batch 202 loss: 3.896179534494877\n",
      "  batch 203 loss: 3.8962660133838654\n",
      "  batch 204 loss: 3.8961597457528114\n",
      "  batch 205 loss: 3.8961340934038162\n",
      "  batch 206 loss: 3.896222658455372\n",
      "  batch 207 loss: 3.8962233513593674\n",
      "  batch 208 loss: 3.896058388054371\n",
      "  batch 209 loss: 3.8961419835686684\n",
      "  batch 210 loss: 3.8962747678160667\n",
      "  batch 211 loss: 3.8961044400930405\n",
      "  batch 212 loss: 3.896152190864086\n",
      "  batch 213 loss: 3.896115466952324\n",
      "  batch 214 loss: 3.896092750132084\n",
      "  batch 215 loss: 3.8962962552905083\n",
      "  batch 216 loss: 3.896113909780979\n",
      "  batch 217 loss: 3.896308310329914\n",
      "  batch 218 loss: 3.8962744176387787\n",
      "  batch 219 loss: 3.8960622623562813\n",
      "  batch 220 loss: 3.8960918933153152\n",
      "  batch 221 loss: 3.896108664572239\n",
      "  batch 222 loss: 3.896146647632122\n",
      "  batch 223 loss: 3.89602317661047\n",
      "  batch 224 loss: 3.8960414826869965\n",
      "  batch 225 loss: 3.8961585089564323\n",
      "  batch 226 loss: 3.8961491733789444\n",
      "  batch 227 loss: 3.896153151988983\n",
      "  batch 228 loss: 3.8960694074630737\n",
      "  batch 229 loss: 3.8962343484163284\n",
      "  batch 230 loss: 3.8964353650808334\n",
      "  batch 231 loss: 3.8961477130651474\n",
      "  batch 232 loss: 3.8961529210209846\n",
      "  batch 233 loss: 3.8962258100509644\n",
      "  batch 234 loss: 3.8962304294109344\n",
      "  batch 235 loss: 3.8960889726877213\n",
      "  batch 236 loss: 3.8961475640535355\n",
      "  batch 237 loss: 3.896193340420723\n",
      "  batch 238 loss: 3.8962768986821175\n",
      "  batch 239 loss: 3.8961508572101593\n",
      "  batch 240 loss: 3.8962155655026436\n",
      "  batch 241 loss: 3.8961177989840508\n",
      "  batch 242 loss: 3.8961493521928787\n",
      "  batch 243 loss: 3.8962425217032433\n",
      "  batch 244 loss: 3.896190047264099\n",
      "  batch 245 loss: 3.8962868452072144\n",
      "  batch 246 loss: 3.8961318656802177\n",
      "  batch 247 loss: 3.8960497826337814\n",
      "  batch 248 loss: 3.896095097064972\n",
      "  batch 249 loss: 3.8961445540189743\n",
      "  batch 250 loss: 3.89611154794693\n",
      "  batch 251 loss: 3.896123059093952\n",
      "  batch 252 loss: 3.8960608914494514\n",
      "  batch 253 loss: 3.896218679845333\n",
      "  batch 254 loss: 3.8961494117975235\n",
      "  batch 255 loss: 3.8962506726384163\n",
      "  batch 256 loss: 3.896098494529724\n",
      "  batch 257 loss: 3.8962095603346825\n",
      "  batch 258 loss: 3.89613139629364\n",
      "  batch 259 loss: 3.896052323281765\n",
      "  batch 260 loss: 3.8961416110396385\n",
      "  batch 261 loss: 3.896120510995388\n",
      "  batch 262 loss: 3.896156281232834\n",
      "  batch 263 loss: 3.896234877407551\n",
      "  batch 264 loss: 3.8961257115006447\n",
      "  batch 265 loss: 3.8962189331650734\n",
      "  batch 266 loss: 3.8961750343441963\n",
      "  batch 267 loss: 3.8960926830768585\n",
      "  batch 268 loss: 3.896196462213993\n",
      "  batch 269 loss: 3.8963182866573334\n",
      "  batch 270 loss: 3.8961362913250923\n",
      "  batch 271 loss: 3.89607509970665\n",
      "  batch 272 loss: 3.8960925713181496\n",
      "  batch 273 loss: 3.8961829468607903\n",
      "  batch 274 loss: 3.896052859723568\n",
      "  batch 275 loss: 3.8960671350359917\n",
      "  batch 276 loss: 3.8960553109645844\n",
      "  batch 277 loss: 3.896101735532284\n",
      "  batch 278 loss: 3.896215498447418\n",
      "  batch 279 loss: 3.8961307406425476\n",
      "  batch 280 loss: 3.8965051099658012\n",
      "  batch 281 loss: 3.896222524344921\n",
      "  batch 282 loss: 3.896109476685524\n",
      "  batch 283 loss: 3.8962271362543106\n",
      "  batch 284 loss: 3.896091014146805\n",
      "  batch 285 loss: 3.896168991923332\n",
      "  batch 286 loss: 3.8962191343307495\n",
      "  batch 287 loss: 3.8962847366929054\n",
      "  batch 288 loss: 3.8961389660835266\n",
      "  batch 289 loss: 3.896134614944458\n",
      "  batch 290 loss: 3.8960393965244293\n",
      "  batch 291 loss: 3.896197237074375\n",
      "  batch 292 loss: 3.896126441657543\n",
      "  batch 293 loss: 3.89609158039093\n",
      "  batch 294 loss: 3.8961294144392014\n",
      "  batch 295 loss: 3.8960950076580048\n",
      "  batch 296 loss: 3.8960870653390884\n",
      "  batch 297 loss: 3.896376594901085\n",
      "  batch 298 loss: 3.896136023104191\n",
      "  batch 299 loss: 3.896077811717987\n",
      "  batch 300 loss: 3.896206423640251\n",
      "  batch 301 loss: 3.8962409496307373\n",
      "  batch 302 loss: 3.896304927766323\n",
      "  batch 303 loss: 3.8960695415735245\n",
      "  batch 304 loss: 3.896089546382427\n",
      "  batch 305 loss: 3.8960742726922035\n",
      "  batch 306 loss: 3.896070346236229\n",
      "  batch 307 loss: 3.8961624205112457\n",
      "  batch 308 loss: 3.896152287721634\n",
      "  batch 309 loss: 3.896073453128338\n",
      "  batch 310 loss: 3.8961983174085617\n",
      "  batch 311 loss: 3.8961280286312103\n",
      "  batch 312 loss: 3.896257497370243\n",
      "  batch 313 loss: 3.8960764184594154\n",
      "  batch 314 loss: 3.8960114270448685\n",
      "  batch 315 loss: 3.8961738869547844\n",
      "  batch 316 loss: 3.8961610347032547\n",
      "  batch 317 loss: 3.8961960524320602\n",
      "  batch 318 loss: 3.896163523197174\n",
      "  batch 319 loss: 3.8960684314370155\n",
      "  batch 320 loss: 3.8960447907447815\n",
      "  batch 321 loss: 3.896149605512619\n",
      "  batch 322 loss: 3.896198444068432\n",
      "  batch 323 loss: 3.8961963802576065\n",
      "  batch 324 loss: 3.8961909860372543\n",
      "  batch 325 loss: 3.8962179869413376\n",
      "  batch 326 loss: 3.89605849981308\n",
      "  batch 327 loss: 3.896264038980007\n",
      "  batch 328 loss: 3.896231807768345\n",
      "  batch 329 loss: 3.896269753575325\n",
      "  batch 330 loss: 3.896103747189045\n",
      "  batch 331 loss: 3.896069437265396\n",
      "  batch 332 loss: 3.8963077440857887\n",
      "  batch 333 loss: 3.896239012479782\n",
      "  batch 334 loss: 3.8960895612835884\n",
      "  batch 335 loss: 3.896137334406376\n",
      "  batch 336 loss: 3.8960697799921036\n",
      "  batch 337 loss: 3.8961066007614136\n",
      "  batch 338 loss: 3.896147057414055\n",
      "  batch 339 loss: 3.8963856026530266\n",
      "  batch 340 loss: 3.89616259932518\n",
      "  batch 341 loss: 3.8960777893662453\n",
      "  batch 342 loss: 3.896192394196987\n",
      "  batch 343 loss: 3.896044433116913\n",
      "  batch 344 loss: 3.8962552547454834\n",
      "  batch 345 loss: 3.896079123020172\n",
      "  batch 346 loss: 3.896202377974987\n",
      "  batch 347 loss: 3.896129958331585\n",
      "  batch 348 loss: 3.8965078368782997\n",
      "  batch 349 loss: 3.8963181525468826\n",
      "  batch 350 loss: 3.8960571214556694\n",
      "  batch 351 loss: 3.896154060959816\n",
      "  batch 352 loss: 3.896216541528702\n",
      "  batch 353 loss: 3.8961657732725143\n",
      "  batch 354 loss: 3.8961695060133934\n",
      "  batch 355 loss: 3.8961015716195107\n",
      "  batch 356 loss: 3.896335318684578\n",
      "  batch 357 loss: 3.896227791905403\n",
      "  batch 358 loss: 3.89616060256958\n",
      "  batch 359 loss: 3.895993694663048\n",
      "  batch 360 loss: 3.8960887640714645\n",
      "  batch 361 loss: 3.8962497189641\n",
      "  batch 362 loss: 3.896203890442848\n",
      "  batch 363 loss: 3.896231785416603\n",
      "  batch 364 loss: 3.896096169948578\n",
      "  batch 365 loss: 3.8963020741939545\n",
      "  batch 366 loss: 3.896116279065609\n",
      "  batch 367 loss: 3.896199271082878\n",
      "  batch 368 loss: 3.8960549160838127\n",
      "  batch 369 loss: 3.8961742222309113\n",
      "  batch 370 loss: 3.8960463255643845\n",
      "  batch 371 loss: 3.8961205407977104\n",
      "  batch 372 loss: 3.8960567340254784\n",
      "  batch 373 loss: 3.896237187087536\n",
      "  batch 374 loss: 3.8961083441972733\n",
      "  batch 375 loss: 3.8961033448576927\n",
      "  batch 376 loss: 3.896089568734169\n",
      "  batch 377 loss: 3.896245077252388\n",
      "  batch 378 loss: 3.8960787430405617\n",
      "  batch 379 loss: 3.8961294293403625\n",
      "  batch 380 loss: 3.896251581609249\n",
      "  batch 381 loss: 3.896131381392479\n",
      "  batch 382 loss: 3.8960644528269768\n",
      "  batch 383 loss: 3.8961387053132057\n",
      "  batch 384 loss: 3.896275132894516\n",
      "  batch 385 loss: 3.8960500583052635\n",
      "  batch 386 loss: 3.8961886689066887\n",
      "  batch 387 loss: 3.8961012065410614\n",
      "  batch 388 loss: 3.896062456071377\n",
      "  batch 389 loss: 3.896236926317215\n",
      "  batch 390 loss: 3.8962485417723656\n",
      "  batch 391 loss: 3.896147057414055\n",
      "  batch 392 loss: 3.896178498864174\n",
      "  batch 393 loss: 3.8961804285645485\n",
      "  batch 394 loss: 3.8961225152015686\n",
      "  batch 395 loss: 3.896214857697487\n",
      "  batch 396 loss: 3.8962196186184883\n",
      "  batch 397 loss: 3.896098993718624\n",
      "  batch 398 loss: 3.8960629254579544\n",
      "  batch 399 loss: 3.8963759690523148\n",
      "  batch 400 loss: 3.8963711634278297\n",
      "  batch 401 loss: 3.896134629845619\n",
      "  batch 402 loss: 3.8961802050471306\n",
      "  batch 403 loss: 3.896125666797161\n",
      "  batch 404 loss: 3.8960157856345177\n",
      "  batch 405 loss: 3.89608683437109\n",
      "  batch 406 loss: 3.8961174860596657\n",
      "  batch 407 loss: 3.8963074311614037\n",
      "  batch 408 loss: 3.8960397243499756\n",
      "  batch 409 loss: 3.8962811902165413\n",
      "  batch 410 loss: 3.896441526710987\n",
      "  batch 411 loss: 3.8960665315389633\n",
      "  batch 412 loss: 3.896235905587673\n",
      "  batch 413 loss: 3.8961115330457687\n",
      "  batch 414 loss: 3.896151103079319\n",
      "  batch 415 loss: 3.8962773755192757\n",
      "  batch 416 loss: 3.8960794657468796\n",
      "  batch 417 loss: 3.896071180701256\n",
      "  batch 418 loss: 3.8962017744779587\n",
      "  batch 419 loss: 3.8962776958942413\n",
      "  batch 420 loss: 3.8960638120770454\n",
      "  batch 421 loss: 3.896131068468094\n",
      "  batch 422 loss: 3.896126687526703\n",
      "  batch 423 loss: 3.8961021453142166\n",
      "  batch 424 loss: 3.8963098004460335\n",
      "LOSS train 3.8963098004460335 valid 3.8959663144295225\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.896090514957905\n",
      "  batch 2 loss: 3.8961407393217087\n",
      "  batch 3 loss: 3.896183028817177\n",
      "  batch 4 loss: 3.8960795179009438\n",
      "  batch 5 loss: 3.89614537358284\n",
      "  batch 6 loss: 3.8959975093603134\n",
      "  batch 7 loss: 3.896013982594013\n",
      "  batch 8 loss: 3.896019570529461\n",
      "  batch 9 loss: 3.8960763216018677\n",
      "  batch 10 loss: 3.896243542432785\n",
      "  batch 11 loss: 3.8962701708078384\n",
      "  batch 12 loss: 3.896062783896923\n",
      "  batch 13 loss: 3.896383062005043\n",
      "  batch 14 loss: 3.8961317390203476\n",
      "  batch 15 loss: 3.8961378559470177\n",
      "  batch 16 loss: 3.89620054513216\n",
      "  batch 17 loss: 3.8962362930178642\n",
      "  batch 18 loss: 3.8961325138807297\n",
      "  batch 19 loss: 3.896043010056019\n",
      "  batch 20 loss: 3.896108500659466\n",
      "  batch 21 loss: 3.896020695567131\n",
      "  batch 22 loss: 3.896124243736267\n",
      "  batch 23 loss: 3.8961519449949265\n",
      "  batch 24 loss: 3.8960928693413734\n",
      "  batch 25 loss: 3.8960471898317337\n",
      "  batch 26 loss: 3.8962948247790337\n",
      "  batch 27 loss: 3.8960538432002068\n",
      "  batch 28 loss: 3.896226227283478\n",
      "  batch 29 loss: 3.8960325717926025\n",
      "  batch 30 loss: 3.8961947187781334\n",
      "  batch 31 loss: 3.8961289674043655\n",
      "  batch 32 loss: 3.8960344567894936\n",
      "  batch 33 loss: 3.8960574865341187\n",
      "  batch 34 loss: 3.8960625529289246\n",
      "  batch 35 loss: 3.896270766854286\n",
      "  batch 36 loss: 3.896117366850376\n",
      "  batch 37 loss: 3.8960530161857605\n",
      "  batch 38 loss: 3.8960426598787308\n",
      "  batch 39 loss: 3.8962835371494293\n",
      "  batch 40 loss: 3.8959932699799538\n",
      "  batch 41 loss: 3.8960970491170883\n",
      "  batch 42 loss: 3.896129831671715\n",
      "  batch 43 loss: 3.8961600214242935\n",
      "  batch 44 loss: 3.8961274847388268\n",
      "  batch 45 loss: 3.8960871174931526\n",
      "  batch 46 loss: 3.8960902243852615\n",
      "  batch 47 loss: 3.8960492685437202\n",
      "  batch 48 loss: 3.896157145500183\n",
      "  batch 49 loss: 3.896100178360939\n",
      "  batch 50 loss: 3.896062858402729\n",
      "  batch 51 loss: 3.896165043115616\n",
      "  batch 52 loss: 3.896154411137104\n",
      "  batch 53 loss: 3.896112598478794\n",
      "  batch 54 loss: 3.896025598049164\n",
      "  batch 55 loss: 3.8960658609867096\n",
      "  batch 56 loss: 3.896257556974888\n",
      "  batch 57 loss: 3.896085537970066\n",
      "  batch 58 loss: 3.8960446640849113\n",
      "  batch 59 loss: 3.896416775882244\n",
      "  batch 60 loss: 3.8961517587304115\n",
      "  batch 61 loss: 3.8960892483592033\n",
      "  batch 62 loss: 3.8960495740175247\n",
      "  batch 63 loss: 3.896149694919586\n",
      "  batch 64 loss: 3.8960872516036034\n",
      "  batch 65 loss: 3.8960254564881325\n",
      "  batch 66 loss: 3.8961392268538475\n",
      "  batch 67 loss: 3.896042488515377\n",
      "  batch 68 loss: 3.896012745797634\n",
      "  batch 69 loss: 3.896194949746132\n",
      "  batch 70 loss: 3.8960866183042526\n",
      "  batch 71 loss: 3.8960377275943756\n",
      "  batch 72 loss: 3.896274209022522\n",
      "  batch 73 loss: 3.896032564342022\n",
      "  batch 74 loss: 3.8961006551980972\n",
      "  batch 75 loss: 3.896087445318699\n",
      "  batch 76 loss: 3.89604739099741\n",
      "  batch 77 loss: 3.8960352763533592\n",
      "  batch 78 loss: 3.89610256254673\n",
      "  batch 79 loss: 3.896328955888748\n",
      "  batch 80 loss: 3.8960263654589653\n",
      "  batch 81 loss: 3.8961369395256042\n",
      "  batch 82 loss: 3.8960017934441566\n",
      "  batch 83 loss: 3.8961421251296997\n",
      "  batch 84 loss: 3.895995058119297\n",
      "  batch 85 loss: 3.8960621282458305\n",
      "  batch 86 loss: 3.896230399608612\n",
      "  batch 87 loss: 3.8961076959967613\n",
      "  batch 88 loss: 3.896013841032982\n",
      "  batch 89 loss: 3.8959623277187347\n",
      "  batch 90 loss: 3.8960111141204834\n",
      "  batch 91 loss: 3.8961179181933403\n",
      "  batch 92 loss: 3.896103911101818\n",
      "  batch 93 loss: 3.8960089087486267\n",
      "  batch 94 loss: 3.896103225648403\n",
      "  batch 95 loss: 3.8961172848939896\n",
      "  batch 96 loss: 3.8960328102111816\n",
      "  batch 97 loss: 3.896066337823868\n",
      "  batch 98 loss: 3.8961433470249176\n",
      "  batch 99 loss: 3.896050713956356\n",
      "  batch 100 loss: 3.896045356988907\n",
      "  batch 101 loss: 3.8960185274481773\n",
      "  batch 102 loss: 3.8962874934077263\n",
      "  batch 103 loss: 3.8962334394454956\n",
      "  batch 104 loss: 3.896125391125679\n",
      "  batch 105 loss: 3.896158918738365\n",
      "  batch 106 loss: 3.8961906731128693\n",
      "  batch 107 loss: 3.896133467555046\n",
      "  batch 108 loss: 3.8961163386702538\n",
      "  batch 109 loss: 3.895974174141884\n",
      "  batch 110 loss: 3.896208554506302\n",
      "  batch 111 loss: 3.896063856780529\n",
      "  batch 112 loss: 3.896061986684799\n",
      "  batch 113 loss: 3.896048903465271\n",
      "  batch 114 loss: 3.896048240363598\n",
      "  batch 115 loss: 3.8961313292384148\n",
      "  batch 116 loss: 3.8962940499186516\n",
      "  batch 117 loss: 3.896068125963211\n",
      "  batch 118 loss: 3.896241918206215\n",
      "  batch 119 loss: 3.8960782289505005\n",
      "  batch 120 loss: 3.8960302844643593\n",
      "  batch 121 loss: 3.8961423188447952\n",
      "  batch 122 loss: 3.8961385414004326\n",
      "  batch 123 loss: 3.896103836596012\n",
      "  batch 124 loss: 3.8960266560316086\n",
      "  batch 125 loss: 3.896031729876995\n",
      "  batch 126 loss: 3.896023139357567\n",
      "  batch 127 loss: 3.896083801984787\n",
      "  batch 128 loss: 3.896028622984886\n",
      "  batch 129 loss: 3.8961502388119698\n",
      "  batch 130 loss: 3.896057978272438\n",
      "  batch 131 loss: 3.896036870777607\n",
      "  batch 132 loss: 3.8961545303463936\n",
      "  batch 133 loss: 3.89667846262455\n",
      "  batch 134 loss: 3.896104469895363\n",
      "  batch 135 loss: 3.8961693570017815\n",
      "  batch 136 loss: 3.8960091546177864\n",
      "  batch 137 loss: 3.896022416651249\n",
      "  batch 138 loss: 3.8960791006684303\n",
      "  batch 139 loss: 3.8960957527160645\n",
      "  batch 140 loss: 3.896024040877819\n",
      "  batch 141 loss: 3.896023280918598\n",
      "  batch 142 loss: 3.896049626171589\n",
      "  batch 143 loss: 3.896189756691456\n",
      "  batch 144 loss: 3.8960819095373154\n",
      "  batch 145 loss: 3.8960531651973724\n",
      "  batch 146 loss: 3.8961354419589043\n",
      "  batch 147 loss: 3.89603254199028\n",
      "  batch 148 loss: 3.896027073264122\n",
      "  batch 149 loss: 3.896113447844982\n",
      "  batch 150 loss: 3.8960388004779816\n",
      "  batch 151 loss: 3.8961549028754234\n",
      "  batch 152 loss: 3.896086223423481\n",
      "  batch 153 loss: 3.896135352551937\n",
      "  batch 154 loss: 3.8960723727941513\n",
      "  batch 155 loss: 3.8960592597723007\n",
      "  batch 156 loss: 3.896003894507885\n",
      "  batch 157 loss: 3.896299220621586\n",
      "  batch 158 loss: 3.896032825112343\n",
      "  batch 159 loss: 3.8961168080568314\n",
      "  batch 160 loss: 3.8963821083307266\n",
      "  batch 161 loss: 3.896122470498085\n",
      "  batch 162 loss: 3.8960848227143288\n",
      "  batch 163 loss: 3.8960844352841377\n",
      "  batch 164 loss: 3.8961145132780075\n",
      "  batch 165 loss: 3.896040439605713\n",
      "  batch 166 loss: 3.895963042974472\n",
      "  batch 167 loss: 3.8965276330709457\n",
      "  batch 168 loss: 3.8961047679185867\n",
      "  batch 169 loss: 3.8960777670145035\n",
      "  batch 170 loss: 3.896019607782364\n",
      "  batch 171 loss: 3.8960536494851112\n",
      "  batch 172 loss: 3.896114446222782\n",
      "  batch 173 loss: 3.8964523896574974\n",
      "  batch 174 loss: 3.8959826081991196\n",
      "  batch 175 loss: 3.8961515352129936\n",
      "  batch 176 loss: 3.895975761115551\n",
      "  batch 177 loss: 3.896053247153759\n",
      "  batch 178 loss: 3.8962476551532745\n",
      "  batch 179 loss: 3.89597225189209\n",
      "  batch 180 loss: 3.8966012820601463\n",
      "  batch 181 loss: 3.896308146417141\n",
      "  batch 182 loss: 3.896081455051899\n",
      "  batch 183 loss: 3.896045319736004\n",
      "  batch 184 loss: 3.895982749760151\n",
      "  batch 185 loss: 3.896014876663685\n",
      "  batch 186 loss: 3.895992435514927\n",
      "  batch 187 loss: 3.895999990403652\n",
      "  batch 188 loss: 3.896154746413231\n",
      "  batch 189 loss: 3.8961694091558456\n",
      "  batch 190 loss: 3.8960123509168625\n",
      "  batch 191 loss: 3.8960612192749977\n",
      "  batch 192 loss: 3.896066300570965\n",
      "  batch 193 loss: 3.8960844352841377\n",
      "  batch 194 loss: 3.8960630521178246\n",
      "  batch 195 loss: 3.8960647359490395\n",
      "  batch 196 loss: 3.896144986152649\n",
      "  batch 197 loss: 3.8961223140358925\n",
      "  batch 198 loss: 3.8960075974464417\n",
      "  batch 199 loss: 3.8960009664297104\n",
      "  batch 200 loss: 3.896036744117737\n",
      "  batch 201 loss: 3.896032936871052\n",
      "  batch 202 loss: 3.895990066230297\n",
      "  batch 203 loss: 3.89604701846838\n",
      "  batch 204 loss: 3.896071881055832\n",
      "  batch 205 loss: 3.8959727734327316\n",
      "  batch 206 loss: 3.8960953801870346\n",
      "  batch 207 loss: 3.8960569724440575\n",
      "  batch 208 loss: 3.896052062511444\n",
      "  batch 209 loss: 3.896020755171776\n",
      "  batch 210 loss: 3.896628677845001\n",
      "  batch 211 loss: 3.895999677479267\n",
      "  batch 212 loss: 3.895968660712242\n",
      "  batch 213 loss: 3.8960134014487267\n",
      "  batch 214 loss: 3.896026559174061\n",
      "  batch 215 loss: 3.8960453644394875\n",
      "  batch 216 loss: 3.896145299077034\n",
      "  batch 217 loss: 3.8961265832185745\n",
      "  batch 218 loss: 3.8962021842598915\n",
      "  batch 219 loss: 3.8961063623428345\n",
      "  batch 220 loss: 3.8959872722625732\n",
      "  batch 221 loss: 3.8961451798677444\n",
      "  batch 222 loss: 3.896224595606327\n",
      "  batch 223 loss: 3.8960734829306602\n",
      "  batch 224 loss: 3.896065227687359\n",
      "  batch 225 loss: 3.8963855654001236\n",
      "  batch 226 loss: 3.8960040360689163\n",
      "  batch 227 loss: 3.896156258881092\n",
      "  batch 228 loss: 3.895991913974285\n",
      "  batch 229 loss: 3.896112099289894\n",
      "  batch 230 loss: 3.895961306989193\n",
      "  batch 231 loss: 3.89601219445467\n",
      "  batch 232 loss: 3.896047882735729\n",
      "  batch 233 loss: 3.8960381895303726\n",
      "  batch 234 loss: 3.8960112407803535\n",
      "  batch 235 loss: 3.8961019590497017\n",
      "  batch 236 loss: 3.8961406126618385\n",
      "  batch 237 loss: 3.896116092801094\n",
      "  batch 238 loss: 3.895984023809433\n",
      "  batch 239 loss: 3.8960603401064873\n",
      "  batch 240 loss: 3.8960397094488144\n",
      "  batch 241 loss: 3.8961436077952385\n",
      "  batch 242 loss: 3.8961634188890457\n",
      "  batch 243 loss: 3.896015077829361\n",
      "  batch 244 loss: 3.8959773257374763\n",
      "  batch 245 loss: 3.895981080830097\n",
      "  batch 246 loss: 3.89602367579937\n",
      "  batch 247 loss: 3.8959830328822136\n",
      "  batch 248 loss: 3.895980544388294\n",
      "  batch 249 loss: 3.896041788160801\n",
      "  batch 250 loss: 3.8962230905890465\n",
      "  batch 251 loss: 3.896040603518486\n",
      "  batch 252 loss: 3.896028272807598\n",
      "  batch 253 loss: 3.8960694447159767\n",
      "  batch 254 loss: 3.8960314095020294\n",
      "  batch 255 loss: 3.895983912050724\n",
      "  batch 256 loss: 3.8959958404302597\n",
      "  batch 257 loss: 3.896046742796898\n",
      "  batch 258 loss: 3.8961181566119194\n",
      "  batch 259 loss: 3.8960582241415977\n",
      "  batch 260 loss: 3.8960345163941383\n",
      "  batch 261 loss: 3.896087072789669\n",
      "  batch 262 loss: 3.896027147769928\n",
      "  batch 263 loss: 3.896118603646755\n",
      "  batch 264 loss: 3.8959840536117554\n",
      "  batch 265 loss: 3.8960500732064247\n",
      "  batch 266 loss: 3.896010972559452\n",
      "  batch 267 loss: 3.8961202055215836\n",
      "  batch 268 loss: 3.8960021063685417\n",
      "  batch 269 loss: 3.89603129029274\n",
      "  batch 270 loss: 3.8961742743849754\n",
      "  batch 271 loss: 3.8960235342383385\n",
      "  batch 272 loss: 3.896021232008934\n",
      "  batch 273 loss: 3.8960497081279755\n",
      "  batch 274 loss: 3.8961963057518005\n",
      "  batch 275 loss: 3.896123446524143\n",
      "  batch 276 loss: 3.8960664868354797\n",
      "  batch 277 loss: 3.89602367579937\n",
      "  batch 278 loss: 3.8960739970207214\n",
      "  batch 279 loss: 3.895955167710781\n",
      "  batch 280 loss: 3.895968459546566\n",
      "  batch 281 loss: 3.8960735201835632\n",
      "  batch 282 loss: 3.896042764186859\n",
      "  batch 283 loss: 3.8959799110889435\n",
      "  batch 284 loss: 3.8959993571043015\n",
      "  batch 285 loss: 3.8960692808032036\n",
      "  batch 286 loss: 3.8959232941269875\n",
      "  batch 287 loss: 3.895985059440136\n",
      "  batch 288 loss: 3.8960342779755592\n",
      "  batch 289 loss: 3.8959859162569046\n",
      "  batch 290 loss: 3.896007649600506\n",
      "  batch 291 loss: 3.896223194897175\n",
      "  batch 292 loss: 3.896033339202404\n",
      "  batch 293 loss: 3.89607460051775\n",
      "  batch 294 loss: 3.896077871322632\n",
      "  batch 295 loss: 3.8959302753210068\n",
      "  batch 296 loss: 3.8961402475833893\n",
      "  batch 297 loss: 3.8959743455052376\n",
      "  batch 298 loss: 3.896005727350712\n",
      "  batch 299 loss: 3.8960337936878204\n",
      "  batch 300 loss: 3.8960654735565186\n",
      "  batch 301 loss: 3.89608533680439\n",
      "  batch 302 loss: 3.896034114062786\n",
      "  batch 303 loss: 3.8960558623075485\n",
      "  batch 304 loss: 3.8960956260561943\n",
      "  batch 305 loss: 3.8959896937012672\n",
      "  batch 306 loss: 3.8960072323679924\n",
      "  batch 307 loss: 3.896140918135643\n",
      "  batch 308 loss: 3.896006479859352\n",
      "  batch 309 loss: 3.8960141241550446\n",
      "  batch 310 loss: 3.8960716128349304\n",
      "  batch 311 loss: 3.8960850909352303\n",
      "  batch 312 loss: 3.896128796041012\n",
      "  batch 313 loss: 3.8959682807326317\n",
      "  batch 314 loss: 3.8960612043738365\n",
      "  batch 315 loss: 3.896033897995949\n",
      "  batch 316 loss: 3.8960980102419853\n",
      "  batch 317 loss: 3.896051824092865\n",
      "  batch 318 loss: 3.8960529565811157\n",
      "  batch 319 loss: 3.8959971591830254\n",
      "  batch 320 loss: 3.8960091322660446\n",
      "  batch 321 loss: 3.8959919586777687\n",
      "  batch 322 loss: 3.895955391228199\n",
      "  batch 323 loss: 3.896128296852112\n",
      "  batch 324 loss: 3.896015077829361\n",
      "  batch 325 loss: 3.8960746452212334\n",
      "  batch 326 loss: 3.8959738984704018\n",
      "  batch 327 loss: 3.8960003331303596\n",
      "  batch 328 loss: 3.8960615694522858\n",
      "  batch 329 loss: 3.8960795029997826\n",
      "  batch 330 loss: 3.895990416407585\n",
      "  batch 331 loss: 3.8961535170674324\n",
      "  batch 332 loss: 3.8959896117448807\n",
      "  batch 333 loss: 3.895997941493988\n",
      "  batch 334 loss: 3.895987704396248\n",
      "  batch 335 loss: 3.896062970161438\n",
      "  batch 336 loss: 3.896121732890606\n",
      "  batch 337 loss: 3.8960509821772575\n",
      "  batch 338 loss: 3.8960089832544327\n",
      "  batch 339 loss: 3.8960598334670067\n",
      "  batch 340 loss: 3.8963556811213493\n",
      "  batch 341 loss: 3.8959861993789673\n",
      "  batch 342 loss: 3.895993433892727\n",
      "  batch 343 loss: 3.8961097300052643\n",
      "  batch 344 loss: 3.896029733121395\n",
      "  batch 345 loss: 3.8959782421588898\n",
      "  batch 346 loss: 3.8960512280464172\n",
      "  batch 347 loss: 3.896039456129074\n",
      "  batch 348 loss: 3.896118961274624\n",
      "  batch 349 loss: 3.895991876721382\n",
      "  batch 350 loss: 3.896039053797722\n",
      "  batch 351 loss: 3.896101012825966\n",
      "  batch 352 loss: 3.895930863916874\n",
      "  batch 353 loss: 3.896007716655731\n",
      "  batch 354 loss: 3.8961468786001205\n",
      "  batch 355 loss: 3.89605500549078\n",
      "  batch 356 loss: 3.8960101306438446\n",
      "  batch 357 loss: 3.895950600504875\n",
      "  batch 358 loss: 3.896234817802906\n",
      "  batch 359 loss: 3.8959716334939003\n",
      "  batch 360 loss: 3.89595165848732\n",
      "  batch 361 loss: 3.896049253642559\n",
      "  batch 362 loss: 3.8959988802671432\n",
      "  batch 363 loss: 3.89592607319355\n",
      "  batch 364 loss: 3.896073378622532\n",
      "  batch 365 loss: 3.896061211824417\n",
      "  batch 366 loss: 3.895983397960663\n",
      "  batch 367 loss: 3.8960364311933517\n",
      "  batch 368 loss: 3.8960354328155518\n",
      "  batch 369 loss: 3.8960097432136536\n",
      "  batch 370 loss: 3.8961379677057266\n",
      "  batch 371 loss: 3.895949460566044\n",
      "  batch 372 loss: 3.8960320502519608\n",
      "  batch 373 loss: 3.896102763712406\n",
      "  batch 374 loss: 3.8960206732153893\n",
      "  batch 375 loss: 3.895974412560463\n",
      "  batch 376 loss: 3.8960521072149277\n",
      "  batch 377 loss: 3.89601918309927\n",
      "  batch 378 loss: 3.896110877394676\n",
      "  batch 379 loss: 3.895947553217411\n",
      "  batch 380 loss: 3.8960244804620743\n",
      "  batch 381 loss: 3.8959898948669434\n",
      "  batch 382 loss: 3.896026521921158\n",
      "  batch 383 loss: 3.895973764359951\n",
      "  batch 384 loss: 3.896035134792328\n",
      "  batch 385 loss: 3.895975112915039\n",
      "  batch 386 loss: 3.8961803317070007\n",
      "  batch 387 loss: 3.896039165556431\n",
      "  batch 388 loss: 3.8959904313087463\n",
      "  batch 389 loss: 3.8959364145994186\n",
      "  batch 390 loss: 3.8960681036114693\n",
      "  batch 391 loss: 3.896143712103367\n",
      "  batch 392 loss: 3.895927518606186\n",
      "  batch 393 loss: 3.895998954772949\n",
      "  batch 394 loss: 3.8959497287869453\n",
      "  batch 395 loss: 3.895981118083\n",
      "  batch 396 loss: 3.896093986928463\n",
      "  batch 397 loss: 3.896032080054283\n",
      "  batch 398 loss: 3.896013617515564\n",
      "  batch 399 loss: 3.8961241245269775\n",
      "  batch 400 loss: 3.896062061190605\n",
      "  batch 401 loss: 3.895984895527363\n",
      "  batch 402 loss: 3.8960783258080482\n",
      "  batch 403 loss: 3.8960173204541206\n",
      "  batch 404 loss: 3.8959630504250526\n",
      "  batch 405 loss: 3.896055169403553\n",
      "  batch 406 loss: 3.8960268795490265\n",
      "  batch 407 loss: 3.895938143134117\n",
      "  batch 408 loss: 3.8959959000349045\n",
      "  batch 409 loss: 3.896000497043133\n",
      "  batch 410 loss: 3.895968236029148\n",
      "  batch 411 loss: 3.8960700556635857\n",
      "  batch 412 loss: 3.895990774035454\n",
      "  batch 413 loss: 3.895953729748726\n",
      "  batch 414 loss: 3.895975925028324\n",
      "  batch 415 loss: 3.8960428684949875\n",
      "  batch 416 loss: 3.896027885377407\n",
      "  batch 417 loss: 3.8958966434001923\n",
      "  batch 418 loss: 3.8959886133670807\n",
      "  batch 419 loss: 3.896028511226177\n",
      "  batch 420 loss: 3.8959643095731735\n",
      "  batch 421 loss: 3.8961330354213715\n",
      "  batch 422 loss: 3.8960147500038147\n",
      "  batch 423 loss: 3.8959444761276245\n",
      "  batch 424 loss: 3.89601594209671\n",
      "LOSS train 3.89601594209671 valid 3.895899605656148\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.8960029259324074\n",
      "  batch 2 loss: 3.896142452955246\n",
      "  batch 3 loss: 3.895959697663784\n",
      "  batch 4 loss: 3.8959790468215942\n",
      "  batch 5 loss: 3.896072156727314\n",
      "  batch 6 loss: 3.8961028158664703\n",
      "  batch 7 loss: 3.8960069194436073\n",
      "  batch 8 loss: 3.8959803208708763\n",
      "  batch 9 loss: 3.895962655544281\n",
      "  batch 10 loss: 3.8960356041789055\n",
      "  batch 11 loss: 3.8960244432091713\n",
      "  batch 12 loss: 3.8959411084651947\n",
      "  batch 13 loss: 3.896015726029873\n",
      "  batch 14 loss: 3.895984008908272\n",
      "  batch 15 loss: 3.8960020840168\n",
      "  batch 16 loss: 3.8959136679768562\n",
      "  batch 17 loss: 3.8959973007440567\n",
      "  batch 18 loss: 3.8959896862506866\n",
      "  batch 19 loss: 3.8964798822999\n",
      "  batch 20 loss: 3.895962178707123\n",
      "  batch 21 loss: 3.896046608686447\n",
      "  batch 22 loss: 3.8960152715444565\n",
      "  batch 23 loss: 3.8959376737475395\n",
      "  batch 24 loss: 3.8960035666823387\n",
      "  batch 25 loss: 3.896073132753372\n",
      "  batch 26 loss: 3.8959818705916405\n",
      "  batch 27 loss: 3.8959795981645584\n",
      "  batch 28 loss: 3.8960285410284996\n",
      "  batch 29 loss: 3.8959590792655945\n",
      "  batch 30 loss: 3.8960113301873207\n",
      "  batch 31 loss: 3.895967923104763\n",
      "  batch 32 loss: 3.8960191756486893\n",
      "  batch 33 loss: 3.89598685503006\n",
      "  batch 34 loss: 3.8959493562579155\n",
      "  batch 35 loss: 3.8961413875222206\n",
      "  batch 36 loss: 3.896012619137764\n",
      "  batch 37 loss: 3.896094113588333\n",
      "  batch 38 loss: 3.89607934653759\n",
      "  batch 39 loss: 3.8959356769919395\n",
      "  batch 40 loss: 3.8959862664341927\n",
      "  batch 41 loss: 3.896005854010582\n",
      "  batch 42 loss: 3.8960111141204834\n",
      "  batch 43 loss: 3.896010458469391\n",
      "  batch 44 loss: 3.896013416349888\n",
      "  batch 45 loss: 3.896041564643383\n",
      "  batch 46 loss: 3.896038144826889\n",
      "  batch 47 loss: 3.8959810584783554\n",
      "  batch 48 loss: 3.895988091826439\n",
      "  batch 49 loss: 3.8960608541965485\n",
      "  batch 50 loss: 3.896000884473324\n",
      "  batch 51 loss: 3.8960505351424217\n",
      "  batch 52 loss: 3.8960214033722878\n",
      "  batch 53 loss: 3.8959577307105064\n",
      "  batch 54 loss: 3.8959332704544067\n",
      "  batch 55 loss: 3.895934507250786\n",
      "  batch 56 loss: 3.8959627971053123\n",
      "  batch 57 loss: 3.8982274755835533\n",
      "  batch 58 loss: 3.8961435928940773\n",
      "  batch 59 loss: 3.8959906548261642\n",
      "  batch 60 loss: 3.8959631100296974\n",
      "  batch 61 loss: 3.896017588675022\n",
      "  batch 62 loss: 3.896096356213093\n",
      "  batch 63 loss: 3.8959739208221436\n",
      "  batch 64 loss: 3.8959828838706017\n",
      "  batch 65 loss: 3.8959338068962097\n",
      "  batch 66 loss: 3.895920090377331\n",
      "  batch 67 loss: 3.8959400057792664\n",
      "  batch 68 loss: 3.8960601836442947\n",
      "  batch 69 loss: 3.896064631640911\n",
      "  batch 70 loss: 3.8961209058761597\n",
      "  batch 71 loss: 3.896225556731224\n",
      "  batch 72 loss: 3.895985893905163\n",
      "  batch 73 loss: 3.896050252020359\n",
      "  batch 74 loss: 3.8960461616516113\n",
      "  batch 75 loss: 3.896004095673561\n",
      "  batch 76 loss: 3.896011173725128\n",
      "  batch 77 loss: 3.8960215598344803\n",
      "  batch 78 loss: 3.8960175961256027\n",
      "  batch 79 loss: 3.8959429636597633\n",
      "  batch 80 loss: 3.8959716632962227\n",
      "  batch 81 loss: 3.895991340279579\n",
      "  batch 82 loss: 3.8959764018654823\n",
      "  batch 83 loss: 3.8960183560848236\n",
      "  batch 84 loss: 3.89601544290781\n",
      "  batch 85 loss: 3.896026127040386\n",
      "  batch 86 loss: 3.895966984331608\n",
      "  batch 87 loss: 3.8959291875362396\n",
      "  batch 88 loss: 3.8959857895970345\n",
      "  batch 89 loss: 3.8960730135440826\n",
      "  batch 90 loss: 3.895944133400917\n",
      "  batch 91 loss: 3.8960117772221565\n",
      "  batch 92 loss: 3.8962431848049164\n",
      "  batch 93 loss: 3.8960021510720253\n",
      "  batch 94 loss: 3.896001137793064\n",
      "  batch 95 loss: 3.895982474088669\n",
      "  batch 96 loss: 3.8959828689694405\n",
      "  batch 97 loss: 3.8960134014487267\n",
      "  batch 98 loss: 3.8959237933158875\n",
      "  batch 99 loss: 3.8960006311535835\n",
      "  batch 100 loss: 3.895992159843445\n",
      "  batch 101 loss: 3.896153338253498\n",
      "  batch 102 loss: 3.895995579659939\n",
      "  batch 103 loss: 3.895959012210369\n",
      "  batch 104 loss: 3.896036371588707\n",
      "  batch 105 loss: 3.8960225135087967\n",
      "  batch 106 loss: 3.895967908203602\n",
      "  batch 107 loss: 3.8960036039352417\n",
      "  batch 108 loss: 3.8961512595415115\n",
      "  batch 109 loss: 3.8960166946053505\n",
      "  batch 110 loss: 3.895990878343582\n",
      "  batch 111 loss: 3.8959464207291603\n",
      "  batch 112 loss: 3.895889572799206\n",
      "  batch 113 loss: 3.896011620759964\n",
      "  batch 114 loss: 3.8959698602557182\n",
      "  batch 115 loss: 3.896078035235405\n",
      "  batch 116 loss: 3.8959618359804153\n",
      "  batch 117 loss: 3.8959468826651573\n",
      "  batch 118 loss: 3.896251007914543\n",
      "  batch 119 loss: 3.896030753850937\n",
      "  batch 120 loss: 3.895971715450287\n",
      "  batch 121 loss: 3.8959610760211945\n",
      "  batch 122 loss: 3.8959782421588898\n",
      "  batch 123 loss: 3.8961771354079247\n",
      "  batch 124 loss: 3.8962221518158913\n",
      "  batch 125 loss: 3.8960568979382515\n",
      "  batch 126 loss: 3.8960254788398743\n",
      "  batch 127 loss: 3.896025002002716\n",
      "  batch 128 loss: 3.896028906106949\n",
      "  batch 129 loss: 3.896003194153309\n",
      "  batch 130 loss: 3.8959463760256767\n",
      "  batch 131 loss: 3.8960127383470535\n",
      "  batch 132 loss: 3.89613326638937\n",
      "  batch 133 loss: 3.8960882276296616\n",
      "  batch 134 loss: 3.8959629014134407\n",
      "  batch 135 loss: 3.89595302939415\n",
      "  batch 136 loss: 3.8959877118468285\n",
      "  batch 137 loss: 3.8959961980581284\n",
      "  batch 138 loss: 3.8959407284855843\n",
      "  batch 139 loss: 3.8960183933377266\n",
      "  batch 140 loss: 3.89596775919199\n",
      "  batch 141 loss: 3.8959806337952614\n",
      "  batch 142 loss: 3.8960065841674805\n",
      "  batch 143 loss: 3.895953379571438\n",
      "  batch 144 loss: 3.895935572683811\n",
      "  batch 145 loss: 3.8960912078619003\n",
      "  batch 146 loss: 3.8959373086690903\n",
      "  batch 147 loss: 3.895903244614601\n",
      "  batch 148 loss: 3.8959935382008553\n",
      "  batch 149 loss: 3.8961340710520744\n",
      "  batch 150 loss: 3.8959400579333305\n",
      "  batch 151 loss: 3.896016351878643\n",
      "  batch 152 loss: 3.896001748740673\n",
      "  batch 153 loss: 3.896076425909996\n",
      "  batch 154 loss: 3.8959395065903664\n",
      "  batch 155 loss: 3.8960089832544327\n",
      "  batch 156 loss: 3.89602217823267\n",
      "  batch 157 loss: 3.895936906337738\n",
      "  batch 158 loss: 3.895994186401367\n",
      "  batch 159 loss: 3.8959547355771065\n",
      "  batch 160 loss: 3.896170623600483\n",
      "  batch 161 loss: 3.896069087088108\n",
      "  batch 162 loss: 3.89598548412323\n",
      "  batch 163 loss: 3.8959749191999435\n",
      "  batch 164 loss: 3.895960785448551\n",
      "  batch 165 loss: 3.8960363268852234\n",
      "  batch 166 loss: 3.8959889709949493\n",
      "  batch 167 loss: 3.8962762504816055\n",
      "  batch 168 loss: 3.8960365504026413\n",
      "  batch 169 loss: 3.8959936052560806\n",
      "  batch 170 loss: 3.8959765136241913\n",
      "  batch 171 loss: 3.895955204963684\n",
      "  batch 172 loss: 3.8961210772395134\n",
      "  batch 173 loss: 3.8959884271025658\n",
      "  batch 174 loss: 3.8959587812423706\n",
      "  batch 175 loss: 3.8959937542676926\n",
      "  batch 176 loss: 3.89594180136919\n",
      "  batch 177 loss: 3.8959963992238045\n",
      "  batch 178 loss: 3.8960776701569557\n",
      "  batch 179 loss: 3.896321751177311\n",
      "  batch 180 loss: 3.895981051027775\n",
      "  batch 181 loss: 3.8959443122148514\n",
      "  batch 182 loss: 3.8960425555706024\n",
      "  batch 183 loss: 3.8959807232022285\n",
      "  batch 184 loss: 3.8960699662566185\n",
      "  batch 185 loss: 3.895897798240185\n",
      "  batch 186 loss: 3.896097533404827\n",
      "  batch 187 loss: 3.8959293588995934\n",
      "  batch 188 loss: 3.8960198014974594\n",
      "  batch 189 loss: 3.8959529995918274\n",
      "  batch 190 loss: 3.8959314450621605\n",
      "  batch 191 loss: 3.8959998711943626\n",
      "  batch 192 loss: 3.8959350287914276\n",
      "  batch 193 loss: 3.895985096693039\n",
      "  batch 194 loss: 3.8959927931427956\n",
      "  batch 195 loss: 3.895984761416912\n",
      "  batch 196 loss: 3.8959361761808395\n",
      "  batch 197 loss: 3.8959743306040764\n",
      "  batch 198 loss: 3.8959296196699142\n",
      "  batch 199 loss: 3.8960072472691536\n",
      "  batch 200 loss: 3.8961464017629623\n",
      "  batch 201 loss: 3.895983800292015\n",
      "  batch 202 loss: 3.8959061577916145\n",
      "  batch 203 loss: 3.895945616066456\n",
      "  batch 204 loss: 3.8959017768502235\n",
      "  batch 205 loss: 3.8960727900266647\n",
      "  batch 206 loss: 3.896001487970352\n",
      "  batch 207 loss: 3.896083302795887\n",
      "  batch 208 loss: 3.895978756248951\n",
      "  batch 209 loss: 3.89597799628973\n",
      "  batch 210 loss: 3.8959449753165245\n",
      "  batch 211 loss: 3.8959850147366524\n",
      "  batch 212 loss: 3.8960374370217323\n",
      "  batch 213 loss: 3.895959012210369\n",
      "  batch 214 loss: 3.895930901169777\n",
      "  batch 215 loss: 3.8960361033678055\n",
      "  batch 216 loss: 3.895904876291752\n",
      "  batch 217 loss: 3.895963378250599\n",
      "  batch 218 loss: 3.895926535129547\n",
      "  batch 219 loss: 3.8960413485765457\n",
      "  batch 220 loss: 3.8960253596305847\n",
      "  batch 221 loss: 3.8959664702415466\n",
      "  batch 222 loss: 3.895935110747814\n",
      "  batch 223 loss: 3.8959261402487755\n",
      "  batch 224 loss: 3.8959514424204826\n",
      "  batch 225 loss: 3.8959931060671806\n",
      "  batch 226 loss: 3.8959433287382126\n",
      "  batch 227 loss: 3.8960031792521477\n",
      "  batch 228 loss: 3.8960030004382133\n",
      "  batch 229 loss: 3.8959134221076965\n",
      "  batch 230 loss: 3.895929254591465\n",
      "  batch 231 loss: 3.895910993218422\n",
      "  batch 232 loss: 3.895964525640011\n",
      "  batch 233 loss: 3.895925246179104\n",
      "  batch 234 loss: 3.8959557339549065\n",
      "  batch 235 loss: 3.8959545120596886\n",
      "  batch 236 loss: 3.8960829377174377\n",
      "  batch 237 loss: 3.895925059914589\n",
      "  batch 238 loss: 3.8960645869374275\n",
      "  batch 239 loss: 3.896089978516102\n",
      "  batch 240 loss: 3.895912155508995\n",
      "  batch 241 loss: 3.8960437923669815\n",
      "  batch 242 loss: 3.8961892426013947\n",
      "  batch 243 loss: 3.895926207304001\n",
      "  batch 244 loss: 3.8961506113409996\n",
      "  batch 245 loss: 3.8959558308124542\n",
      "  batch 246 loss: 3.8959695920348167\n",
      "  batch 247 loss: 3.8958992063999176\n",
      "  batch 248 loss: 3.895913653075695\n",
      "  batch 249 loss: 3.8958907648921013\n",
      "  batch 250 loss: 3.8959727808833122\n",
      "  batch 251 loss: 3.8961131125688553\n",
      "  batch 252 loss: 3.895952120423317\n",
      "  batch 253 loss: 3.895914912223816\n",
      "  batch 254 loss: 3.895915985107422\n",
      "  batch 255 loss: 3.8959776759147644\n",
      "  batch 256 loss: 3.896024614572525\n",
      "  batch 257 loss: 3.896040618419647\n",
      "  batch 258 loss: 3.8959421291947365\n",
      "  batch 259 loss: 3.8959925398230553\n",
      "  batch 260 loss: 3.8959013670682907\n",
      "  batch 261 loss: 3.895927593111992\n",
      "  batch 262 loss: 3.895952083170414\n",
      "  batch 263 loss: 3.896021328866482\n",
      "  batch 264 loss: 3.8958909064531326\n",
      "  batch 265 loss: 3.8960089534521103\n",
      "  batch 266 loss: 3.895988419651985\n",
      "  batch 267 loss: 3.8959182873368263\n",
      "  batch 268 loss: 3.895936146378517\n",
      "  batch 269 loss: 3.8959846571087837\n",
      "  batch 270 loss: 3.8962694630026817\n",
      "  batch 271 loss: 3.8959984853863716\n",
      "  batch 272 loss: 3.8959182277321815\n",
      "  batch 273 loss: 3.895960196852684\n",
      "  batch 274 loss: 3.895960219204426\n",
      "  batch 275 loss: 3.8959628120064735\n",
      "  batch 276 loss: 3.8959683924913406\n",
      "  batch 277 loss: 3.8960316255688667\n",
      "  batch 278 loss: 3.895942948758602\n",
      "  batch 279 loss: 3.8960069119930267\n",
      "  batch 280 loss: 3.895934745669365\n",
      "  batch 281 loss: 3.8959533348679543\n",
      "  batch 282 loss: 3.8959477320313454\n",
      "  batch 283 loss: 3.8959114402532578\n",
      "  batch 284 loss: 3.8959202095866203\n",
      "  batch 285 loss: 3.8961370289325714\n",
      "  batch 286 loss: 3.895922265946865\n",
      "  batch 287 loss: 3.895913764834404\n",
      "  batch 288 loss: 3.8960444629192352\n",
      "  batch 289 loss: 3.895953953266144\n",
      "  batch 290 loss: 3.895963206887245\n",
      "  batch 291 loss: 3.895936571061611\n",
      "  batch 292 loss: 3.895968973636627\n",
      "  batch 293 loss: 3.896021619439125\n",
      "  batch 294 loss: 3.8959036991000175\n",
      "  batch 295 loss: 3.895935721695423\n",
      "  batch 296 loss: 3.896126940846443\n",
      "  batch 297 loss: 3.8960623368620872\n",
      "  batch 298 loss: 3.8959172442555428\n",
      "  batch 299 loss: 3.895994946360588\n",
      "  batch 300 loss: 3.895948089659214\n",
      "  batch 301 loss: 3.8960029259324074\n",
      "  batch 302 loss: 3.8959841430187225\n",
      "  batch 303 loss: 3.895980566740036\n",
      "  batch 304 loss: 3.8959015011787415\n",
      "  batch 305 loss: 3.8959585800766945\n",
      "  batch 306 loss: 3.8959433287382126\n",
      "  batch 307 loss: 3.895964153110981\n",
      "  batch 308 loss: 3.8959920406341553\n",
      "  batch 309 loss: 3.8961800932884216\n",
      "  batch 310 loss: 3.895923636853695\n",
      "  batch 311 loss: 3.895894967019558\n",
      "  batch 312 loss: 3.895936354994774\n",
      "  batch 313 loss: 3.895988404750824\n",
      "  batch 314 loss: 3.8958937749266624\n",
      "  batch 315 loss: 3.896017797291279\n",
      "  batch 316 loss: 3.895941138267517\n",
      "  batch 317 loss: 3.8959479331970215\n",
      "  batch 318 loss: 3.895897202193737\n",
      "  batch 319 loss: 3.8959766253829002\n",
      "  batch 320 loss: 3.895987167954445\n",
      "  batch 321 loss: 3.8959365636110306\n",
      "  batch 322 loss: 3.8960161954164505\n",
      "  batch 323 loss: 3.8959743678569794\n",
      "  batch 324 loss: 3.8959308490157127\n",
      "  batch 325 loss: 3.8959809988737106\n",
      "  batch 326 loss: 3.8959059044718742\n",
      "  batch 327 loss: 3.8959469199180603\n",
      "  batch 328 loss: 3.8960619643330574\n",
      "  batch 329 loss: 3.895920231938362\n",
      "  batch 330 loss: 3.89591683447361\n",
      "  batch 331 loss: 3.8959542214870453\n",
      "  batch 332 loss: 3.895927958190441\n",
      "  batch 333 loss: 3.8959329053759575\n",
      "  batch 334 loss: 3.895954929292202\n",
      "  batch 335 loss: 3.895917445421219\n",
      "  batch 336 loss: 3.895904593169689\n",
      "  batch 337 loss: 3.8959890976548195\n",
      "  batch 338 loss: 3.895927019417286\n",
      "  batch 339 loss: 3.8958838656544685\n",
      "  batch 340 loss: 3.8959916457533836\n",
      "  batch 341 loss: 3.89593193680048\n",
      "  batch 342 loss: 3.895992435514927\n",
      "  batch 343 loss: 3.8960461616516113\n",
      "  batch 344 loss: 3.8959018513560295\n",
      "  batch 345 loss: 3.8959305733442307\n",
      "  batch 346 loss: 3.895980902016163\n",
      "  batch 347 loss: 3.896085910499096\n",
      "  batch 348 loss: 3.8959278911352158\n",
      "  batch 349 loss: 3.89594204723835\n",
      "  batch 350 loss: 3.896016962826252\n",
      "  batch 351 loss: 3.8961585015058517\n",
      "  batch 352 loss: 3.8959415927529335\n",
      "  batch 353 loss: 3.895931400358677\n",
      "  batch 354 loss: 3.8959403336048126\n",
      "  batch 355 loss: 3.8961390927433968\n",
      "  batch 356 loss: 3.8963580429553986\n",
      "  batch 357 loss: 3.8959175422787666\n",
      "  batch 358 loss: 3.895964875817299\n",
      "  batch 359 loss: 3.895929165184498\n",
      "  batch 360 loss: 3.8959562554955482\n",
      "  batch 361 loss: 3.8960404470562935\n",
      "  batch 362 loss: 3.895896963775158\n",
      "  batch 363 loss: 3.8958883062005043\n",
      "  batch 364 loss: 3.8959101364016533\n",
      "  batch 365 loss: 3.895942471921444\n",
      "  batch 366 loss: 3.8959712386131287\n",
      "  batch 367 loss: 3.8959380984306335\n",
      "  batch 368 loss: 3.895899310708046\n",
      "  batch 369 loss: 3.8959371224045753\n",
      "  batch 370 loss: 3.8959240317344666\n",
      "  batch 371 loss: 3.89593642950058\n",
      "  batch 372 loss: 3.8959882333874702\n",
      "  batch 373 loss: 3.895937703549862\n",
      "  batch 374 loss: 3.8959195017814636\n",
      "  batch 375 loss: 3.8958874493837357\n",
      "  batch 376 loss: 3.8959700614213943\n",
      "  batch 377 loss: 3.8959022015333176\n",
      "  batch 378 loss: 3.895968832075596\n",
      "  batch 379 loss: 3.896005943417549\n",
      "  batch 380 loss: 3.8959615528583527\n",
      "  batch 381 loss: 3.896014116704464\n",
      "  batch 382 loss: 3.89590498059988\n",
      "  batch 383 loss: 3.896029770374298\n",
      "  batch 384 loss: 3.8959610536694527\n",
      "  batch 385 loss: 3.8959208503365517\n",
      "  batch 386 loss: 3.8959689885377884\n",
      "  batch 387 loss: 3.896023415029049\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 13\u001b[0m avg_loss \u001b[39m=\u001b[39m train_one_epoch(epoch_number, writer)\n\u001b[0;32m     15\u001b[0m running_vloss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[39m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# statistics for batch normalization.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m      7\u001b[0m output_logits, notes_gt \u001b[39m=\u001b[39m unpack_batch(batch)\n\u001b[0;32m      9\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output_logits, notes_gt)\n\u001b[1;32m---> 10\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \n\u001b[0;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     14\u001b[0m \u001b[39m# Gather data and report\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\agienv\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\agienv\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/composer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(train_loader):\n",
    "            voutputs, vgt = unpack_batch(vdata)\n",
    "            vloss = loss_fn(voutputs, vgt)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'checkpoints/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
