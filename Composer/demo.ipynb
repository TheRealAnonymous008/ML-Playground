{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import make_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Extract all the midi files \n",
    "\n",
    "midi_files = []\n",
    "\n",
    "for path, subdirs, files in os.walk(r'./data/'):\n",
    "    for name in files:\n",
    "        if (name.endswith('.mid')):\n",
    "            midi_files.append(os.path.join(path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files...: 100%|██████████| 15/15 [00:03<00:00,  4.49files/s, ./data/1910 Fruitgum Company\\Simon Says.1.mid]               \n"
     ]
    }
   ],
   "source": [
    "TEST_FILE = r'Ode to Joy from the 9th Symphony.mid'\n",
    "df = make_dataset(midi_files, file_name=\"datasets/midi-dataset.csv\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"midi-dataset-mini.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"midi-dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['notes'] = df.notes.apply(lambda x: [int(y) for y in str(x).removeprefix('[').removesuffix(']').split(' ') if y.isnumeric()])\n",
    "df['velocities'] = df.velocities.apply(lambda x: [int(y) for y in str(x).removeprefix('[').removesuffix(']').split(' ') if y.isnumeric()])\n",
    "df['times'] = df.times.apply(lambda x: [float(y) for y in str(x).removeprefix('[').removesuffix(']').split(' ') if y.replace('.', '').replace('e+', '').replace('e-','').isnumeric()])\n",
    "df['durations'] = df.durations.apply(lambda x: [float(y) for y in str(x).removeprefix('[').removesuffix(']').split(' ') if y.replace('.', '').replace('e+', '').replace('e-','').isnumeric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[11088])\n",
    "\n",
    "plt.plot(df['times'][11088])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_bins = [0 for i in range(0, 200)]\n",
    "velocity_bins = [0 for i in range(0, 128)]\n",
    "duration_means = []\n",
    "duration_std = []\n",
    "\n",
    "time_means = []\n",
    "time_std = []\n",
    "\n",
    "for i in range(0, len(df['notes'])):\n",
    "    for x in df['notes'][i]:\n",
    "        note_bins[x] += 1\n",
    "\n",
    "    for x in df['velocities'][i]:\n",
    "        velocity_bins[x] += 1\n",
    "    \n",
    "    duration_means.append(np.mean(df['durations'][i]))\n",
    "    duration_std.append(np.std(df['durations'][i]))\n",
    "\n",
    "    time_means.append(np.mean(df['times'][i]))\n",
    "    time_std.append(np.std(df['times'][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(note_bins)\n",
    "plt.xlim((0, 127))\n",
    "plt.ylim(0, max(note_bins[0:128]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = duration_means\n",
    "q = duration_std\n",
    "p[np.argmax(duration_means)] = 0\n",
    "q[np.argmax(duration_std)] = 0\n",
    "\n",
    "plt.scatter(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = time_means\n",
    "q = time_std\n",
    "p[np.argmax(time_means)] = 0\n",
    "p[np.argmax(time_means)] = 0\n",
    "\n",
    "q[np.argmax(time_std)] = 0\n",
    "q[np.argmax(time_std)] = 0\n",
    "\n",
    "plt.scatter(p,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(velocity_bins)\n",
    "plt.xlim((0, 127))\n",
    "plt.ylim(0, max(velocity_bins[0:128]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis specific to duration.\n",
    "# We want to determine if we can quantize it some Gaussian noise\n",
    "\n",
    "vals = []\n",
    "losses_mean = []\n",
    "losses_std = []\n",
    "D = 32\n",
    "\n",
    "for D in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "    L = []\n",
    "    for track in df['durations']:\n",
    "        p = np.array(track)\n",
    "        l = D * p - np.trunc(D * p) \n",
    "\n",
    "        L.append(np.mean(l))\n",
    "    L = np.array(L)\n",
    "    losses_mean.append(np.mean(L))\n",
    "    losses_std.append(np.std(L))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_mean)\n",
    "plt.plot(losses_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
