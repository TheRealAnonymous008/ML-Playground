{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from languages import LANGUAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df = pd.read_csv(\"English_Processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonetic Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126054\n"
     ]
    }
   ],
   "source": [
    "print(len(loaded_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the list of pronunciations\n",
    "pronunciation_list = []\n",
    "for p in loaded_df['pronunciation']:\n",
    "    _pronunciations = p.split(',')\n",
    "    pronunciation_list.extend(_pronunciations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126054\n"
     ]
    }
   ],
   "source": [
    "print(len(pronunciation_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a histogram of the different symbols\n",
    "symbols_hist = {}\n",
    "\n",
    "for P in pronunciation_list:\n",
    "    # Make use of only the narrow transcriptions.\n",
    "    for x in P:\n",
    "        if x in symbols_hist.keys():\n",
    "            symbols_hist[x] += 1\n",
    "        else:\n",
    "            symbols_hist[x] = 1  \n",
    "\n",
    "# Sort the symbols \n",
    "symbols = sorted(symbols_hist, key = lambda x: symbols_hist[x], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ 243998\n",
      "Àà 108659\n",
      "…ô 87382\n",
      "…™ 85400\n",
      "t 57628\n",
      "n 56618\n",
      ". 49403\n",
      "s 48654\n",
      "…π 47957\n",
      "l 44831\n",
      "k 41398\n",
      "d 37409\n",
      "Àê 34861\n",
      "i 33967\n",
      "m 28404\n",
      " ä 24906\n",
      "Àå 24905\n",
      "p 23985\n",
      "…õ 23029\n",
      "√¶ 21925\n",
      "b 20264\n",
      "a 17685\n",
      "…ë 16886\n",
      "e 15881\n",
      "f 14851\n",
      "z 13948\n",
      " É 13700\n",
      "…° 12024\n",
      "…î 11077\n",
      "u 10912\n",
      "…í 10577\n",
      "( 10408\n",
      ") 10407\n",
      "v 10266\n",
      "o 9905\n",
      "  9800\n",
      " å 9735\n",
      "Õ° 9692\n",
      "j 8758\n",
      " í 8510\n",
      "w 8090\n",
      "≈ã 7695\n",
      "h 6977\n",
      "…ö 6367\n",
      "[ 3986\n",
      "] 3986\n",
      "Œ∏ 3872\n",
      "- 3370\n",
      "…ú 3123\n",
      "Ã© 2746\n",
      "…ù 2029\n",
      "r 1733\n",
      "√∞ 1270\n",
      "…æ 1218\n",
      "ÃØ 1148\n",
      "…´ 827\n",
      " ∞ 799\n",
      "ÃÉ 434\n",
      "…ê 368\n",
      " î 344\n",
      " â 337\n",
      "…ò 262\n",
      "x 250\n",
      " ç 229\n",
      "Ã• 154\n",
      "‚Äø 135\n",
      "Ã† 134\n",
      "…® 132\n",
      "Ãö 126\n",
      "À¶ 113\n",
      "Ãà 103\n",
      "À® 100\n",
      "Ã™ 91\n",
      " ∑ 86\n",
      "√§ 74\n",
      "Àß 73\n",
      "…µ 73\n",
      "Ãû 61\n",
      "Ãä 50\n",
      " Å 49\n",
      "ÃÜ 46\n",
      "…¨ 41\n",
      "Àë 38\n",
      "À© 36\n",
      "…± 36\n",
      "…Ø 33\n",
      "Àû 31\n",
      "Ãü 30\n",
      "y 30\n",
      "~ 30\n",
      "À• 28\n",
      "√ß 27\n",
      "√∏ 25\n",
      " à 23\n",
      "…ª 22\n",
      "Àî 21\n",
      "Ã¨ 18\n",
      " ≤ 18\n",
      "Ãù 17\n",
      "…ñ 17\n",
      "≈ì 17\n",
      "ÀÄ 15\n",
      " ± 15\n",
      "…ï 15\n",
      "…≤ 15\n",
      "…∏ 15\n",
      " è 13\n",
      "·µä 13\n",
      " ã 13\n",
      "œá 12\n",
      "·∫Ω 11\n",
      "ƒ© 11\n",
      "‚Äñ 11\n",
      "√µ 10\n",
      "À§ 9\n",
      "≈© 9\n",
      "Ã∞ 9\n",
      "…¶ 9\n",
      " º 8\n",
      "c 8\n",
      "| 8\n",
      "√£ 8\n",
      "…£ 6\n",
      "«Ä 6\n",
      "‚Åø 5\n",
      "Ãç 5\n",
      "¬≤ 5\n",
      "…≥ 5\n",
      "Œ≤ 5\n",
      "≈è 5\n",
      "q 5\n",
      "…û 4\n",
      "À† 4\n",
      "…§ 4\n",
      "…ü 4\n",
      "·µà 4\n",
      "‚Åµ 4\n",
      "«É 3\n",
      "Õú 3\n",
      " ô 3\n",
      "ƒß 3\n",
      "ƒÉ 3\n",
      "…≠ 3\n",
      "ÃÄ 3\n",
      " Ä 3\n",
      " é 2\n",
      "‚Üì 2\n",
      "¬π 2\n",
      "Ãô 2\n",
      "√Ø 2\n",
      "·∑à 2\n",
      "ƒÅ 2\n",
      "√° 2\n",
      " ü 2\n",
      "' 2\n",
      "√∂ 2\n",
      " ò 2\n",
      "À° 2\n",
      "·µª 2\n",
      "«Å 2\n",
      "ƒ≠ 2\n",
      "≈≠ 2\n",
      "…• 2\n",
      "…¥ 2\n",
      "…ì 1\n",
      "Ãú 1\n",
      "Àó 1\n",
      "Õá 1\n",
      "√º 1\n",
      " ù 1\n",
      "¬≥ 1\n",
      "‚Å∞ 1\n",
      " ê 1\n",
      "·∏ø 1\n",
      "‚Üó 1\n",
      "Ô∏é 1\n",
      " ¨ 1\n",
      "‚Ä¶ 1\n",
      "ÃÅ 1\n",
      "√¨ 1\n",
      "√≥ 1\n",
      "Ãò 1\n",
      "≈ç 1\n",
      "* 1\n",
      " Ç 1\n",
      "‚Å¥ 1\n",
      "…Ω 1\n",
      "ÃΩ 1\n",
      "√† 1\n",
      "√≤ 1\n",
      " © 1\n",
      "êû™ 1\n"
     ]
    }
   ],
   "source": [
    "for x in symbols:\n",
    "    print(x, symbols_hist[x]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Àà\n",
      "…ô\n",
      "…™\n",
      "t\n",
      "n\n",
      ".\n",
      "s\n",
      "…π\n",
      "l\n",
      "k\n",
      "d\n",
      "Àê\n",
      "i\n",
      "m\n",
      " ä\n",
      "Àå\n",
      "p\n",
      "…õ\n",
      "√¶\n",
      "b\n",
      "a\n",
      "…ë\n",
      "e\n",
      "f\n",
      "z\n",
      " É\n",
      "…°\n",
      "…î\n",
      "u\n",
      "…í\n",
      "(\n",
      ")\n",
      "v\n",
      "o\n",
      " \n",
      " å\n",
      "Õ°\n",
      "j\n",
      " í\n",
      "w\n",
      "≈ã\n",
      "h\n",
      "…ö\n",
      "Œ∏\n",
      "-\n",
      "…ú\n",
      "Ã©\n",
      "…ù\n",
      "r\n",
      "√∞\n",
      "…æ\n",
      "ÃØ\n",
      "…´\n",
      " ∞\n",
      "ÃÉ\n",
      "…ê\n",
      " î\n",
      " â\n",
      "…ò\n",
      "x\n",
      " ç\n",
      "Ã•\n",
      "‚Äø\n",
      "Ã†\n",
      "…®\n",
      "Ãö\n",
      "À¶\n",
      "Ãà\n",
      "À®\n",
      "Ã™\n",
      " ∑\n",
      "√§\n",
      "Àß\n",
      "…µ\n",
      "Ãû\n",
      "Ãä\n",
      " Å\n",
      "ÃÜ\n",
      "…¨\n",
      "Àë\n",
      "À©\n",
      "…±\n",
      "…Ø\n",
      "Àû\n",
      "Ãü\n",
      "y\n",
      "~\n",
      "À•\n",
      "√ß\n",
      "√∏\n",
      " à\n",
      "…ª\n",
      "Àî\n",
      "Ã¨\n",
      " ≤\n",
      "Ãù\n",
      "…ñ\n",
      "≈ì\n",
      "ÀÄ\n",
      " ±\n",
      "…ï\n",
      "…≤\n",
      "…∏\n",
      " è\n",
      "·µä\n",
      " ã\n",
      "œá\n",
      "·∫Ω\n",
      "ƒ©\n",
      "‚Äñ\n",
      "√µ\n",
      "À§\n",
      "≈©\n",
      "Ã∞\n",
      "…¶\n",
      " º\n",
      "c\n",
      "|\n",
      "√£\n",
      "…£\n",
      "«Ä\n",
      "‚Åø\n",
      "Ãç\n",
      "¬≤\n",
      "…≥\n",
      "Œ≤\n",
      "≈è\n",
      "q\n",
      "…û\n",
      "À†\n",
      "…§\n",
      "…ü\n",
      "·µà\n",
      "‚Åµ\n",
      "«É\n",
      "Õú\n",
      " ô\n",
      "ƒß\n",
      "ƒÉ\n",
      "…≠\n",
      "ÃÄ\n",
      " Ä\n",
      " é\n",
      "‚Üì\n",
      "¬π\n",
      "Ãô\n",
      "√Ø\n",
      "·∑à\n",
      "ƒÅ\n",
      "√°\n",
      " ü\n",
      "'\n",
      "√∂\n",
      " ò\n",
      "À°\n",
      "·µª\n",
      "«Å\n",
      "ƒ≠\n",
      "≈≠\n",
      "…•\n",
      "…¥\n",
      "…ì\n",
      "Ãú\n",
      "Àó\n",
      "Õá\n",
      "√º\n",
      " ù\n",
      "¬≥\n",
      "‚Å∞\n",
      " ê\n",
      "·∏ø\n",
      "‚Üó\n",
      "Ô∏é\n",
      " ¨\n",
      "‚Ä¶\n",
      "ÃÅ\n",
      "√¨\n",
      "√≥\n",
      "Ãò\n",
      "≈ç\n",
      "*\n",
      " Ç\n",
      "‚Å¥\n",
      "…Ω\n",
      "ÃΩ\n",
      "√†\n",
      "√≤\n",
      " ©\n",
      "êû™\n"
     ]
    }
   ],
   "source": [
    "for s in symbols:\n",
    "    if s == \"/\" or s == \"[\" or s == \"]\":\n",
    "        continue \n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    }
   ],
   "source": [
    "print(len(symbols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, processors, decoders\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from languages import LANGUAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pronunciation</th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/p…ôÀàs…õnt/</td>\n",
       "      <td>%</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/p…öÀàs…õnt/</td>\n",
       "      <td>%</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/k ä≈ã/</td>\n",
       "      <td>!kung</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/«Ék ä≈ã/</td>\n",
       "      <td>!kung</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/√¶nd l…™t/</td>\n",
       "      <td>&amp;lit</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 pronunciation   word language\n",
       "0           0     /p…ôÀàs…õnt/      %  English\n",
       "1           1     /p…öÀàs…õnt/      %  English\n",
       "2           2         /k ä≈ã/  !kung  English\n",
       "3           3        /«Ék ä≈ã/  !kung  English\n",
       "4           4     /√¶nd l…™t/   &lit  English"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_df = pd.read_csv(\"English_Processed.csv\")\n",
    "symbols = np.load(\"symbols.npy\")\n",
    "loaded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution import CharacterTokenizer, PhoneticTransformer, generate_square_subsequent_mask, PhoneticsDataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda {'[CLS]': 0, '[SEP]': 1, '[BOS]': 2, '[MASK]': 3, '[PAD]': 4, '[RESERVED]': 5, '[UNK]': 6, '/': 7, '.': 8, 'Àà': 9, 't': 10, 'a': 11, 'i': 12, 's': 13, '[': 14, ']': 15, 'n': 16, 'k': 17, 'e': 18, 'o': 19, 'Àê': 20, 'l': 21, 'u': 22, 'm': 23, 'r': 24, 'p': 25, ' ': 26, '…ô': 27, 'd': 28, '…™': 29, ' ≤': 30, '…õ': 31, 'j': 32, 'Õ°': 33, '…ë': 34, '‚Åµ': 35, '…î': 36, '…ê': 37, 'ÃØ': 38, 'b': 39, '¬≤': 40, '…æ': 41, '¬≥': 42, '¬π': 43, 'v': 44, 'Àå': 45, 'Ãû': 46, 'Ã™': 47, ' Å': 48, '…°': 49, 'z': 50, '‚Å¥': 51, 'f': 52, ' É': 53, ' ä': 54, '≈ã': 55, '‚Åª': 56, 'ÃÉ': 57, ' ∞': 58, '…®': 59, '…ï': 60, 'Ã†': 61, 'w': 62, 'h': 63, 'Ãù': 64, ')': 65, '(': 66, '√§': 67, 'y': 68, ' Ç': 69, 'Àß': 70, ' í': 71, '√¶': 72, 'x': 73, '…´': 74, 'Ãö': 75, ' î': 76, ' ã': 77, 'À®': 78, 'À©': 79, '…≤': 80, '√∞': 81, 'Ãü': 82, 'À¶': 83, '·∫Ω': 84, ' ê': 85, '…í': 86, '…π': 87, 'Œ∏': 88, 'Ã©': 89, '√∏': 90, 'ƒ©': 91, 'À•': 92, ' à': 93, '…¶': 94, '…£': 95, '…Ø': 96, '‚ÅΩ': 97, '‚Åæ': 98, '·µù': 99, 'Œ≤': 100, '…§': 101, '√µ': 102, '√£': 103, 'À£': 104, ' é': 105, ' å': 106, '…µ': 107, 'œá': 108, 'À†': 109, ' ë': 110, ' ù': 111, '≈ì': 112, '…ª': 113, '√ß': 114, 'c': 115, 'Ã•': 116, 'ÀÄ': 117, '~': 118, ' è': 119, ' ∑': 120, '‚Äø': 121, ' â': 122, ' º': 123, 'Ãª': 124, '…•': 125, '-': 126, '≈©': 127, 'Ãπ': 128, '…ó': 129, '…ì': 130, '…¥': 131, '…ü': 132, 'ÃÅ': 133, ' ±': 134, '«é': 135, 'Àë': 136, '√≠': 137, 'Õà': 138, '…≠': 139, '√†': 140, '«í': 141, 'q': 142, '√°': 143, '…ö': 144, '«ê': 145, 'Ã∫': 146, 'ƒõ': 147, 'Ãä': 148, 'À§': 149, '«Ω': 150, 'ÃÄ': 151, '…ñ': 152, 'Ãç': 153, '…∞': 154, '√¢': 155, '…≥': 156, '…Ω': 157, 'ƒß': 158, '…ú': 159, '√™': 160, '…∏': 161, '√∫': 162, '«î': 163, '√¨': 164, '√©': 165, '…±': 166, '…¨': 167, 'Ã¨': 168, '√¥': 169, '*': 170, '√Æ': 171, '√≥': 172, ' Ä': 173, ' ï': 174, '…ù': 175, '√ª': 176, '√®': 177, '√≤': 178, 'ÃÑ': 179, '√Ø': 180, '…ò': 181, 'Ã∞': 182, '…Æ': 183, '√π': 184, '‚Üí': 185, 'ƒÅ': 186, '·µä': 187, 'À¢': 188, 'ÃÇ': 189, 'ÃÜ': 190, '≈ç': 191, '|': 192, '·µë': 193, '≈ô': 194, 'Ã§': 195, '‚Åø': 196, 'ƒ´': 197, '…¢': 198, '·µê': 199, '\\xa0': 200, 'g': 201, 'Ãú': 202, ' ç': 203, 'ƒì': 204, 'Ãô': 205, '…ß': 206, '«É': 207, '·µò': 208, 'À°': 209, '·∏≠': 210, '·πµ': 211, 'Ãà': 212, '≈´': 213, '…†': 214, '·ππ': 215, '»µ': 216, '·∏ø': 217, '«Ä': 218, 'Íúú': 219, '…∂': 220, '…û': 221, 'Õú': 222, 'ÃΩ': 223, '«Å': 224, '‚àÖ': 225, '·∏õ': 226, 'Ãå': 227, '·π≥': 228, '‚Å∞': 229, '·ªπ': 230, '·∫°': 231, '·∂Ø': 232, '·∂¢': 233, '\\u2009': 234, 'Àï': 235, '≈è': 236, '·∑à': 237, 'Õâ': 238, 'Àû': 239, \"'\": 240, '>': 241, 'Àî': 242, '√´': 243, '«£': 244, '·∏Å': 245, 'ƒÉ': 246, 'N': 247, '·∂¨': 248, '·∂Æ': 249, '‚Äñ': 250, 'Àó': 251, '√º': 252, '‚Ä¶': 253, '≈≠': 254, 'Ãò': 255, 'ƒï': 256, '‚óå': 257, 'Àñ': 258, 'C': 259, 'Ô∏é': 260, '‚Üó': 261, '\\u200c': 262, 'Õé': 263, '·µè': 264, '√∂': 265, ' ô': 266, 'ƒ≠': 267, '√•': 268, 'ùÜë': 269, ' ¥': 270, '≈∑': 271, '‰∏Ä': 272, '≈Ñ': 273, '·¥ú': 274, '\\u200b': 275, 'Q': 276, '√Ω': 277, '≈ï': 278, 'V': 279, ' ü': 280, '·µà': 281, 'Ã£': 282, 'Õç': 283, '√±': 284, ' ≥': 285, '«π': 286, '·Åä': 287, 'Õá': 288, '‚Üò': 289, '·∂£': 290, '·Ä¨': 291, '·Ä±': 292, '·Ä∫': 293, '1': 294, '‚Üì': 295, ' ò': 296, '·µª': 297, ' Ñ': 298, '·ª≥': 299, 'Ãã': 300, '·∫π': 301, '≈à': 302, ' ú': 303, '·µá': 304, '·™Ω': 305, '·Äõ': 306, '·ÄÅ': 307, '·Äª': 308, '·ÄÑ': 309, ';': 310, '·Äò': 311, '·Ä∏': 312, '+': 313, '‚Äô': 314, '·Äô': 315, '·Äï': 316, '·ÄÄ': 317, '‚Åì': 318, '‚Åπ': 319, '·∫ë': 320, '&': 321, '‰∏ç': 322, ' ¨': 323, ' ©': 324, 'êû™': 325, '·πΩ': 326, 'Íúî': 327, '%': 328, '·∑á': 329, '…∫': 330, '·∑Ü': 331, '»Å': 332, 'Õè': 333, '√ø': 334, 'Àá': 335, '·ªç': 336, '·ªµ': 337, '·µÑ': 338, '«ø': 339, ':': 340}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joaquin\\miniconda3\\envs\\agi\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pronunciation_tokenizer = CharacterTokenizer(symbols, 100)\n",
    "vocab = pronunciation_tokenizer.get_vocab()\n",
    "phonetics_model = PhoneticTransformer(len(vocab))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device, vocab)\n",
    "\n",
    "phonetics_model = phonetics_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in range(0, len(loaded_df)):\n",
    "    x = loaded_df.iloc[i]\n",
    "    tok = pronunciation_tokenizer.encode(x[\"pronunciation\"])\n",
    "    row = []\n",
    "    for t in tok: \n",
    "        row.append(t)\n",
    "        # dataset.append(row.copy())      \n",
    "    dataset.append(tok)\n",
    "    \n",
    "phonetics_dataset = PhoneticsDataset(dataset, vocab, 100)\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "dataloader = DataLoader(phonetics_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:45<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.350490323958858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:44<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.2967006218048835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:45<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.253999258241346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:44<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 2.2160379982763723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:45<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.1829415713587115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:44<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 2.152017826034177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:45<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 2.124942881445731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:45<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 2.0978832167963826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:45<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 2.071246110623883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:44<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 2.048026729014612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:44<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss: 2.0237581287660906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:45<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss: 1.999888026906598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:44<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss: 1.9771471811879067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:45<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss: 1.9541971241274187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:44<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss: 1.9304241413070309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:44<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss: 1.9077423916709038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:44<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss: 1.8837462855923561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:44<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss: 1.8604084003356196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:44<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss: 1.8351659409461483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:45<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss: 1.8091837004307778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(phonetics_model.parameters(), lr = 0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.get(\"[PAD]\"))\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    phonetics_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (input_ids, target_ids) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        tgt_mask = generate_square_subsequent_mask(input_ids.size(1)).to(device)\n",
    "\n",
    "        output = phonetics_model(input_ids, tgt_mask = tgt_mask)\n",
    "        loss = criterion(output.view(-1, len(vocab)), target_ids.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, max_length = 100, start_token = \"[CLS]\", start_seq = [], end_token = \"[EOS]\", temperature = 2.0, p = 0.9):\n",
    "    generated_sequence = [start_token]\n",
    "    generated_sequence.extend(start_seq)\n",
    "    for _ in range(max_length - len(generated_sequence)):\n",
    "        input_ids = torch.tensor([generated_sequence], dtype=torch.long).to(device)\n",
    "        tgt_mask = generate_square_subsequent_mask(input_ids.size(1)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, tgt_mask=tgt_mask)\n",
    "\n",
    "        next_token_logits = output[0, len(generated_sequence ) - 1, :] / temperature\n",
    "        probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cum_prob = torch.cumsum(sorted_probs, dim=-1)\n",
    "        cutoff_index = torch.searchsorted(cum_prob, p)\n",
    "\n",
    "        sorted_probs = sorted_probs[:cutoff_index + 1]\n",
    "        sorted_indices = sorted_indices[:cutoff_index + 1]\n",
    "\n",
    "        next_token_id = sorted_indices[torch.multinomial(sorted_probs, num_samples=1)].item()\n",
    "        generated_sequence.append(next_token_id)\n",
    "\n",
    "        if next_token_id == end_token:\n",
    "            break \n",
    "\n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Àê(ÀêÀåÀà.i.iuÀê.buÀêÀàn.…ô..…ô ä…ô.Ààs..…ôÀàd…ôÀåÀà.kÀàb…õ.p.…ô.d.buÀê.l..n.n.m…õÀàd å…ô.b…π…ôuÀê.Àê..n.…π..pl å…ôb.buÀê.m)Ààb…îb…ëÀê.\n",
      "…™ÀàÀàÀà…ôn…ísnnÀå…™…™Ààn.…™Ààs…ô.Ààt.k.t( än.…ô(…™nÀàn.n(…™s…ôsd…ôÀên…™Ààp.…ô(…π…™.…™m…ô.…ô.Ààsn…™.Ààn.Àà..s.t.…™n)s.…ô äÀàm Ààn.ma…™…ôÀà\n",
      "\n",
      "…ô…πtÀêd…πÀêbÀàk…π…™…ô…ô(…πiÀêÀàÀå…πiÀêÀàdj…π) ä( äÀê…π…îÀê…πÀê…π)…ô ä…ô)…πÀê(…π)Àà\n",
      "…õ≈ã≈ã…õ ås…õ.t…õstt…™st…™ssst…™d.da…™nt√¶nss…™.tt…™t äte…™snnsÀàtÀàm…™kt…™sp.sÀàpdÕ°sÀåts.t…õnt.t…ô.s…™nt ÀàdssÀàsÀàn …ôt…™ntppÀå\n",
      "\n",
      "\n",
      "≈ã. å≈ã.…ô.s..…ô.n.…ô..Àà…™d..d....n.s.k…ëÀê.…ôÀàd…™Ààm.…ô.…ô.Ààm√¶s.m.s.n…ô.Àà.t.n.…ôÀàÀàp..dÕ° É…ôn.st…™…ô.Ààb(√¶.m å…ôÀàb.t.…ô.d…™\n"
     ]
    }
   ],
   "source": [
    "for _ in range(0, 10):\n",
    "    test = generate_text(phonetics_model, start_token=0, end_token=vocab[\"[SEP]\"], temperature = 1.2, p = 0.8)\n",
    "    print(pronunciation_tokenizer.decode(test[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
